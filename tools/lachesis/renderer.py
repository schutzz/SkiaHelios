import json
import re
import polars as pl
import os
from datetime import datetime, timedelta
from pathlib import Path
from tools.lachesis.intel import TEXT_RES

from tools.lachesis.narrator import NarrativeGenerator
from tools.lachesis.user_reporter import UserActivityReporter

# [Display Decoupling] Tags to hide from the visual report
HIDDEN_TAGS = {"VOID", "NOISE", "CHECKED", "SYSTEM_NOISE", "BENIGN", "IGNORE"}

# [v2.0] WebShell Allowlist - ã‚·ã‚¹ãƒ†ãƒ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ WEBSHELL ã‚«ãƒ†ã‚´ãƒªã‹ã‚‰é™¤å¤–
WEBSHELL_ALLOWLIST_PATTERNS = [
    r"\.cdf-ms$",                          # SxS ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚¹ãƒˆã‚¢
    r"\.(mum|cat|manifest)$",              # ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«
    r"^\{[0-9a-fA-F\-]{8,}\}",              # GUID ãƒ•ã‚¡ã‚¤ãƒ«å (å‰æ–¹ä¸€è‡´)
    r"\.\{[0-9a-fA-F\-]+\}$",              # GUID ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹
    r"^[0-9a-f]{8,}_.*31bf3856ad364e35",   # Windows ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆç½²å
    r"(?i)^bootmgr\.exe\.mui",             # Boot Manager UI
    r"\.pf$",                              # Prefetch ãƒ•ã‚¡ã‚¤ãƒ«
    r"(?i)[\\/]winsxs[\\/]",               # WinSxS ãƒ‘ã‚¹
    r"(?i)[\\/]servicing[\\/]",            # Servicing ãƒ‘ã‚¹
]

# [NEW] Jinja2 Integration
try:
    from jinja2 import Environment, FileSystemLoader
except ImportError:
    print("    [!] Jinja2 not found. Lachesis Renderer requires jinja2.")
    Environment = None

class LachesisRenderer:
    def __init__(self, output_path, lang="jp"):
        self.output_path = output_path
        self.lang = lang if lang in TEXT_RES else "jp"
        self.txt = TEXT_RES[self.lang]
        self.hostname = "Unknown"
        self.template_env = None
        
        # Setup Jinja2
        if Environment:
            try:
                # Use resolve() to get absolute path regardless of execution context
                base_dir = Path(__file__).resolve().parent
                template_dir = base_dir / "templates"
                if not template_dir.exists():
                    print(f"    [!] Template directory not found: {template_dir}")
                else:
                    self.template_env = Environment(loader=FileSystemLoader(str(template_dir)))
            except Exception as e:
                print(f"    [!] Jinja2 Setup Failed: {e}")

        # Setup Narrative Engine
        try:
            # Path: tools/lachesis/renderer.py -> tools/lachesis/ -> SkiaHelios/rules/narrative_templates.yaml
            root_dir = Path(__file__).resolve().parent.parent.parent
            narrative_path = root_dir / "rules" / "narrative_templates.yaml"
            self.narrator = NarrativeGenerator(str(narrative_path))
        except Exception as e:
            print(f"    [!] Narrative Engine Setup Failed: {e}")
            self.narrator = None

    # ... (render_report method remains same until _generate_tech_narrative)

    def _generate_tech_narrative(self, ioc, all_iocs):
        """
        [Narrator] Generates human-readable explanation using Hybrid approach:
        1. NarrativeGenerator (YAML Templates) for static definitions.
        2. Dynamic logic for context correlation (e.g. PowerShell ISE).
        
        [PERF v10.0] Added caching for repeated tags/types
        """
        # [PERF v10.0] Cache key based on Tag + Value pattern
        tag = str(ioc.get("Tag", ""))
        val = str(ioc.get("Value", "") or ioc.get("Target_Path", "") or ioc.get("FileName", ""))
        cache_key = f"{tag}|{val[:50]}"  # Truncate value for cache key
        
        # Check cache first
        if not hasattr(self, '_narrative_cache'):
            self._narrative_cache = {}
        
        if cache_key in self._narrative_cache:
            return self._narrative_cache[cache_key]
        
        narrative = ""
        
        # 1. Try Template Engine First
        if self.narrator:
            narrative = self.narrator.resolve(ioc)
        
        # 2. Dynamic Correlation Logic (Enhancements)
        # Fallback if Template didn't match ADS but we know it is ADS (Safety Net / Hybrid)
        if not narrative and ("ADS" in tag or "MASQUERADE" in tag):
             # Basic Manual Fallback (if YAML missing)
             narrative = f"### ğŸ›¡ï¸ éš è”½å·¥ä½œ (Defense Evasion: ADS)\nDetected ADS Masquerading: `{val}`"

        # ADS Correlation: PowerShell ISE
        if "ADS" in tag or "MASQUERADE" in tag:
            has_ise = any("PowerShell_ISE" in str(i.get("Value", "")) or "PowerShell_ISE" in str(i.get("Target_Path", "")) for i in all_iocs)
            if has_ise:
                ise_note = "\n\nâš ï¸ **Context**: ç›´è¿‘ã§ `PowerShell_ISE.exe` ã®å®Ÿè¡Œç—•è·¡ãŒç¢ºèªã•ã‚Œã¦ãŠã‚Šã€ã“ã®ãƒ„ãƒ¼ãƒ«ã‚’ç”¨ã„ã¦ADSãŒä½œæˆã•ã‚ŒãŸå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
                if narrative: narrative += ise_note

        # Store in cache
        self._narrative_cache[cache_key] = narrative
        
        return narrative

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # [NEW] Display Data Beautification
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def _clean_display_data(self, iocs):
        """
        [Display Decoupling] 
        Reportè¡¨ç¤ºç”¨ã«ã€ãƒ‘ã‚¹ã€å€¤ã€ã‚µãƒãƒªãƒ¼ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ã€ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆå´©ã‚Œã‚’é˜²ãã€‚
        """
        cleaned = []
        for ioc in iocs:
            new_ioc = ioc.copy()
            
            # 1. Clean Tags
            raw_tags = str(new_ioc.get('Tag', '')).replace(' ', '').split(',')
            visible_tags = sorted(list(set([t for t in raw_tags if t and t not in HIDDEN_TAGS and len(t) > 1])))
            new_ioc['Tag'] = ", ".join(visible_tags[:3]) if visible_tags else "-"
            
            # 2. Smart Truncate Logic
            # Value ã¨ Summary ä¸¡æ–¹ã«é©ç”¨
            for field in ['Value', 'Summary']:
                if field not in new_ioc: continue
                val = str(new_ioc.get(field, ''))
                if not val: continue

                # [Display Decoupling Guard] JSON/ã‚¹ã‚¯ãƒªãƒ—ãƒˆåˆ¤å®š
                is_code_or_json = "{" in val or "}" in val or "\n" in val or "ScriptBlock" in val or "EventData" in val
                
                if is_code_or_json:
                    # ç„¡æ¡ä»¶ã«æœ«å°¾ã®ã‚´ãƒŸã‚’å‰Šã‚‹
                    val = re.sub(r'[\"\'\}\] ]+$', '', val).strip()
                    if len(val) > 40:
                        new_ioc[field] = f"{val[:37]}..."
                    else:
                        new_ioc[field] = val
                    continue
                
                # é€šå¸¸ã®é•·ã„ãƒ‘ã‚¹ç­‰ã®å‡¦ç† (Valueãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ã¿å¼·åŠ›ã«é©ç”¨)
                if field == 'Value' and len(val) > 60:
                    parts = val.replace('/', '\\').split('\\')
                    if len(parts) > 3:
                        new_ioc[field] = f"...\\{parts[-2]}\\{parts[-1]}"
                    else:
                        new_ioc[field] = val[:30] + "..." + val[-25:]
            
            # 3. Aggressive Global Guard (Check all fields)
            for k, v in new_ioc.items():
                if k in ['Value', 'Summary', 'Payload', 'CommandLine', 'Note']:
                    v_str = str(v)
                    if ("{" in v_str and "}" in v_str) or ("ScriptBlock" in v_str):
                        if len(v_str) > 40:
                             # Scrub and truncate anything that leaks as JSON
                             v_scrubbed = re.sub(r'[\\\"\\\'\\}\\] ]+$', '', v_str).strip()
                             new_ioc[k] = f"{v_scrubbed[:37]}..."
            
            cleaned.append(new_ioc)
        return cleaned

    def render_report(self, analysis_data, analyzer, enricher, origin_stories, dfs_for_ioc, metadata):
        """
        Render report using Jinja2 template.
        Prepares all necessary context variables and passes them to the template.
        """
        self.hostname = metadata.get("hostname", "Unknown")
        out_file = self.output_path
        
        # Debug Log File
        log_file = Path(out_file).parent / "renderer_debug_log.txt"
        with open(log_file, "a", encoding="utf-8") as log:
            log.write(f"\n[{datetime.now()}] Starting render_report for {out_file}\n")
        
        print(f"[*] Lachesis v6.5 (Grimoire Engine) is weaving the Grimoire into {out_file}...")

        if not self.template_env:
            msg = "    [!] Critical: Jinja2 environment not initialized."
            print(msg)
            with open(log_file, "a", encoding="utf-8") as log: log.write(msg + "\n")
            return

        # 1. Prepare Context Data
        try:
            # [User Request] Apply USN Condenser to Timeline Phases (Section 3)
            # This fixes the "USN Storm" in the Detailed Timeline table.
            if analysis_data and 'phases' in analysis_data:
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                # Context-Aware Scoring (Root Cause Fix) - Phase Timeline
                # Uses unified adjust_score() for FN/FP balanced filtering
                # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                from tools.lachesis.sh_analyzer import LachesisAnalyzer
                
                PHASE_DISPLAY_THRESHOLD = 50  # Same as IOC section
                
                filtered_phases = []
                for phase in analysis_data['phases']:
                    filtered_phase = []
                    for ev in phase:
                        summary = str(ev.get('Summary', ''))
                        # [FIX] Populate Path from multiple possible sources
                        path = str(ev.get('Path', '')) or str(ev.get('Full_Path', '')) or str(ev.get('Value', '')) or summary
                        ev['Path'] = path  # Ensure Path is set on the event for template
                        original_score = int(ev.get('Score', 0)) if ev.get('Score') else 0
                        
                        # [Grimoire v6.3/v6.4] Image Hygiene for Timeline
                        path_lower = path.lower().replace("/", "\\")
                        fname = path_lower.split("\\")[-1]
                        tags = str(ev.get('Tag', '')).upper()
                        
                        # Extended noise extensions
                        noise_exts = [".mui", ".nls", ".dll", ".sys", ".jpg", ".png", ".gif", ".ico", ".xml", ".dat"]
                        
                        # Extended system paths
                        system_resource_paths = [
                            "windows\\system32", "windows\\syswow64", "windows\\web\\",
                            "windows\\branding\\", "program files\\windowsapps",
                        ]
                        browser_cache_paths = [
                            "appdata\\local\\microsoft\\windows\\inetcache",
                            "temporary internet files", "content.ie5",
                        ]
                        
                        # Evidence Shield keywords
                        RECON_KEYWORDS = ["xampp", "phpmyadmin", "admin", "dashboard", "kibana", "phishing", "c2", "login"]
                        
                        if any(fname.endswith(ext) for ext in noise_exts):
                            # Recon keywords protect images
                            if any(fname.endswith(ie) for ie in [".png", ".jpg", ".gif", ".ico"]):
                                if any(kw in path_lower for kw in RECON_KEYWORDS):
                                    pass  # Keep as evidence
                                elif any(sp in path_lower for sp in system_resource_paths):
                                    continue  # Drop
                                elif any(bp in path_lower for bp in browser_cache_paths):
                                    continue  # Drop
                            # MUI/NLS/DLL/SYS files in system paths
                            elif any(sp in path_lower for sp in system_resource_paths) and "RECON" not in tags:
                                continue  # Skip this event
                        
                        # Apply Context-Aware Score Adjustment
                        command_line = str(ev.get('CommandLine', '')) or str(ev.get('cmdline', ''))
                        adjusted_score, new_tags = analyzer.adjust_score(path, original_score, command_line=command_line)
                        
                        # Update event with adjusted score
                        ev['Score'] = adjusted_score
                        
                        # Keep if above threshold
                        if adjusted_score >= PHASE_DISPLAY_THRESHOLD:
                            filtered_phase.append(ev)
                    
                    # Only add non-empty phases
                    if filtered_phase:
                        # [Grimoire v9.1] Apply Global Aggregation/De-dupe to Timeline Phases
                        # This eliminates spams like pwin10 and repeating hash_suite logs in the timeline.
                        # We pass a flag to keep timeline thresholds inclusive if needed, 
                        # but for now _group_all_iocs should work.
                        condensed = self._group_all_iocs(filtered_phase, analyzer, threshold=PHASE_DISPLAY_THRESHOLD)
                        filtered_phases.append(condensed)
                
                analysis_data['phases'] = filtered_phases


            # [Optimization] Global Noise Reduction ( The Reaper & Kaishaku )
            # Applies Score Adjustment, Tagging, and Noise Filtering BEFORE Grouping.
            # This ensures Section 4 (Tables), Section 7 (IOCs), and Timeline are consistent.
            from tools.lachesis.sh_analyzer import LachesisAnalyzer
            DISPLAY_THRESHOLD = 50
            
            cleaned_iocs = []
            print(f"[DEBUG-FLOW] Starting Global Cleaning. Input IOCs: {len(analyzer.visual_iocs)}")

            for ioc in analyzer.visual_iocs:
                # 1. Path Calculation
                path = str(ioc.get("Path", ""))
                value = str(ioc.get("Value", ""))
                target_path = path if path else value
                
                # 2. Score Adjustment & Tagging
                original_score = int(ioc.get("Score", 0) or 0)
                command_line = str(ioc.get("CommandLine", "")) or str(ioc.get('cmdline', ''))
                adjusted_score, new_tags = analyzer.adjust_score(target_path, original_score, analyzer.path_penalties, command_line=command_line)
                
                ioc["Score"] = adjusted_score
                ioc["Original_Score"] = original_score
                
                if new_tags:
                    existing = str(ioc.get("Tag", ""))
                    ioc["Tag"] = existing + "," + ",".join(new_tags) if existing else ",".join(new_tags)

                # 3. [The Reaper] Final Noise Filter
                if "SYSTEM_NOISE" in str(ioc.get("Tag", "")) and adjusted_score < 400:
                    continue 

                # 4. [Nuclear Option] TIMESTOMP Score Threshold
                # User Order: "Forget old logic. Score < 500 = Drop."
                ioc_type = str(ioc.get("Type", "")).upper()
                ioc_cat = str(ioc.get("Category", "")).upper()
                if "TIMESTOMP" in ioc_type or "TIMESTOMP" in ioc_cat:
                    if adjusted_score < 500:
                        continue

                # 5. Threshold Check
                if adjusted_score >= DISPLAY_THRESHOLD:
                    cleaned_iocs.append(ioc)

            # 6. Grouping and Preparation
            refined_iocs = self._group_all_iocs(cleaned_iocs, analyzer)
            
            # [Display Decoupling] å…¨ã¦ã®è¡¨ç¤ºãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ‹¬ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            refined_iocs = self._clean_display_data(refined_iocs)

            # [Fix] Global Clean of VOID bug
            for ioc in refined_iocs:
                t_str = str(ioc.get('Time', ''))
                if "VOID_VISUALIZA" in t_str: 
                     ioc['Time'] = "-"

            # [Fix] Pre-calculate technical findings using CLEANED IOCs
            tech_findings = self._prepare_technical_findings_from_list(refined_iocs, analyzer, origin_stories)
            init_access = tech_findings.get("INITIAL ACCESS", [])
            high_lnks = [i for i in init_access if i.get("Insight")]
            gen_lnks = [i for i in init_access if not i.get("Insight")]
            
            # Since filtering is already done, scored_iocs is just refined_iocs
            scored_iocs = refined_iocs

            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # [Grimoire v9.3] Priority Sorting & Appendix Strategy
            # Ensure highest scores are ALWAYS in Section 7.1
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            refined_iocs.sort(key=lambda x: int(x.get('Score', 0)), reverse=True)
            
            context = {
                "txt": self.txt,
                "hostname": self.hostname,
                "metadata": metadata,
                "analysis_data": analysis_data,
                "now": datetime.now().strftime('%Y-%m-%d'),
                "dynamic_verdict": analyzer.get_verdict_for_report(analysis_data.get("verdict_flags")),
                "attack_methods": self._get_attack_methods(analyzer),
                
                # [CHANGE] Replace the old vertical flowchart with the new Sequence Diagram
                # Old: self._render_mermaid_vertical_clustered(refined_iocs)
                # New: self._render_attack_chain_mermaid(refined_iocs)
                "mermaid_timeline": self._render_attack_chain_mermaid(refined_iocs),
                
                "key_indicators": self._prepare_key_indicators(refined_iocs),
                "phishing_lnks": self._prepare_origin_seeds(analyzer.pivot_seeds, "PHISHING", origin_stories),
                "drop_items": self._prepare_origin_seeds(analyzer.pivot_seeds, "DROP", origin_stories, exclude="PHISHING"),
                "anti_forensics_tools": self._prepare_anti_forensics(refined_iocs, dfs_for_ioc),
                "technical_findings": tech_findings,
                "high_interest_lnks": high_lnks[:5], # Rule 3: Top 5 restrict
                "generic_lnks": gen_lnks[:5],        # Rule 3: Top 5 restrict
                "appendix_lnks": high_lnks[5:] + gen_lnks[5:], # Move to Appendix
                "attack_chain_mermaid": "",  # [FIX] Removed duplicate - mermaid_timeline already shows in Executive Summary
                "plutos_section": self._render_plutos_section_text(dfs_for_ioc, analyzer),
                "stats": self._prepare_stats(analyzer, analysis_data, dfs_for_ioc, refined_iocs),
                "recommendations": self._prepare_recommendations(analyzer),
                # [v6.7.1] Ensure Time is sortable in Jinja2 (None => empty string)
                "all_iocs": [{**ioc, 'Time': ioc.get('Time') or ''} for ioc in refined_iocs],
                
                "iocs_section_7_1": self._clean_display_data(refined_iocs[:5]), 
                "iocs_section_7_2": self._clean_display_data(refined_iocs[5:15]),
                "appendix_iocs": self._clean_display_data(refined_iocs[15:200]) # Overflow to Appendix
            }
            with open(log_file, "a", encoding="utf-8") as log: 
                log.write(f"[{datetime.now()}] Context Prepared. Keys: {list(context.keys())}\n")
                if 'dynamic_verdict' in context:
                    log.write(f"[{datetime.now()}] Verdict Type: {type(context['dynamic_verdict'])}\n")
        except Exception as e:
            with open(log_file, "a", encoding="utf-8") as log:
                log.write(f"[{datetime.now()}] Context Preparation Failed: {e}\n")
                import traceback
                traceback.print_exc(file=log)
            raise e

        # 2. Render Template
        try:
            template = self.template_env.get_template("report.md.j2")
            rendered_md = template.render(context)
            
            # 3. Write Output
            with open(out_file, "w", encoding="utf-8") as f:
                f.write(rendered_md)
            
            msg = f"    [+] Report Generated: {out_file}"
            print(msg)
            with open(log_file, "a", encoding="utf-8") as log: log.write(f"[{datetime.now()}] {msg}\n")
            
        except Exception as e:
            msg = f"    [!] Jinja2 Rendering Failed: {e}"
            print(msg)
            import traceback
            traceback.print_exc()
            
            with open(log_file, "a", encoding="utf-8") as log:
                log.write(f"[{datetime.now()}] {msg}\n")
                traceback.print_exc(file=log)
        
        # 4. Generate per-user activity reports
        try:
            events_source = context.get('all_iocs', analyzer.visual_iocs if analyzer else [])
            user_reporter = UserActivityReporter(self.hostname)
            user_reporter.generate(events_source, self.output_path)
        except Exception as e:
            print(f"    [!] UserReporter warning: {e}")
            import traceback
            traceback.print_exc()

    def _group_all_iocs(self, iocs, analyzer=None, threshold=300):
        refined_iocs = [] # [Hybrid Fix] Dynamic Noise Filter Integration
        # [Refactor v2.0] GARBAGE_PATTERNS now loaded from YAML via intel_module
        # [Refactor] Load patterns from Intel module
        from tools.lachesis.intel import IntelManager

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # [Fix] Ultra-Hard Noise Filter (WinSxS / Store Apps / Updates)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Get patterns from Intel (Not Hardcoded here)
        HARD_NOISE_PATTERNS = IntelManager.get_renderer_noise_patterns()
        RESCUE_TAGS = IntelManager.get_rescue_tags()
        GARBAGE_PATTERNS = IntelManager.get_garbage_patterns()

        import re
        
        # Flattened grouping for Section 7
        grouped_iocs = []
        
        # EXTREME_NOISE: Always filter, even if rescued
        EXTREME_NOISE_LIST = [
            "_none_", "_10.0.", "_amd64_", "_x86_",  # WinSxS component hashes
            "~31bf3856ad364e35~",  # Microsoft SxS catalog hash signature
            "windows\\\\winsxs", 
            "windowsapps\\\\",      # UWP Store Apps
            "infusedapps\\\\",      # Pre-installed UWP
            "8wekyb3d8bbwe",      # Microsoft Store app SID pattern
            "deletedalluserpackages",
            "nativeimages_",      # .NET Native Images (GAC cache)
            "\\\\assembly\\\\gac",    # .NET GAC assemblies
            "microsoft.windows.cortana",  # Cortana UWP
            "msfeedssync",
            "mobsync",
            "tzsync",
            "appmodel-runtime",           # AppX Runtime logs
            "contentdeliverymanager",     # Windows CDM
            "devicesearchcache",          # Cortana search cache
            # XAMPP Legitimate Components (non-threat library files)
            "xampp\\\\php\\\\pear",
            "xampp\\\\apache\\\\manual",
            "xampp\\\\tomcat\\\\webapps\\\\docs",
            "xampp\\\\tomcat\\\\webapps\\\\examples",
            "xampp\\\\src\\\\",
            "xampp\\\\licenses",
            "xampp\\\\locale",
            "xampp\\\\php\\\\docs",
            "xampp\\\\cgi-bin",
            "\\\\pear\\\\docs",
            "\\\\pear\\\\tests",
            "xampp\\\\perl\\\\lib",
            "xampp\\\\perl\\\\vendor",
            "filezillaftp\\\\source",
            "apache\\\\icons",
            "mercurymail\\\\",
            "mysql\\\\data\\\\",
            "phpmyadmin\\\\js\\\\",
            "phpmyadmin\\\\libraries\\\\",
            "phpmyadmin\\\\themes\\\\",
            "webalizer\\\\",
            "sendmail\\\\",
            "xampp\\\\tmp\\\\sess_",
            "tomcat\\\\webapps\\\\manager",
            "tomcat\\\\webapps\\\\host-manager",
            "tomcat\\\\webapps\\\\root",
            "phpmyadmin\\\\doc\\\\",
            "htdocs\\\\img\\\\",
            "security\\\\htdocs\\\\",
            "htdocs\\\\dashboard\\\\",
            "htdocs\\\\docs\\\\",
            "dashboard\\\\images\\\\",
            "dashboard\\\\css\\\\",
            "dashboard\\\\docs\\\\",
            "php\\\\extras\\\\",
            "php\\\\tests\\\\",
            "perl\\\\bin\\\\",
            "perl\\\\site\\\\",
            "apache\\\\include\\\\",
            "apache\\\\include\\\\",
            "apache\\\\modules\\\\",
            "mysql\\\\share\\\\",
            "phpmyadmin\\\\locale\\\\",
            "webdav\\\\",
            "\\\\flags\\\\",
            "\\\\install\\\\",
            "phpids\\\\tests\\\\",
            "content.ie5\\\\",
            "dvwa\\\\dvwa\\\\images\\\\",
            "dvwa\\\\dvwa\\\\css\\\\",
            "dvwa\\\\external\\\\",
            "apache\\\\bin\\\\iconv\\\\",
            "php\\\\ext\\\\",
            "tomcat\\\\lib\\\\",
            ".frm",
            ".myd",
            ".myi",
            "performance_schema\\\\",
            "xampp\\\\img\\\\",
            "hackable\\\\users\\\\",
            "favicon.ico",
            "xampp\\\\apache\\\\bin\\\\",
            "xampp\\\\mysql\\\\bin\\\\",
            "xampp\\\\php\\\\",
            "xampp\\\\tomcat\\\\bin\\\\",
            ".dll",
            ".jar",
            ".so",
            ".chm",
            ".hlp",
            "readme.txt",
            "license.txt",
            "install.txt",
            "changes.txt",
            "information_schema\\\\",
            "mysql\\\\mysql\\\\",
            "catalina\\\\",
            ".class",
        ]
        
        # Escape literal patterns but keep regex strings as is
        # Note: HARD_NOISE_PATTERNS are already regexes from intel.py
        def combine_to_regex(patterns, escape=False):
            if not patterns: return None
            valid_patterns = [p for p in patterns if p]
            if not valid_patterns: return None
            if escape:
                valid_patterns = [re.escape(p) for p in valid_patterns]
            return re.compile("|".join(valid_patterns), re.IGNORECASE)

        extreme_noise_re = combine_to_regex(EXTREME_NOISE_LIST, escape=True)
        hard_noise_re = combine_to_regex(HARD_NOISE_PATTERNS)
        garbage_re = combine_to_regex(GARBAGE_PATTERNS, escape=True)

        filtered_iocs = []
        # Logging removed for production cleanliness, but structure kept for logic clarity
        
        for ev in iocs:
            # [Grimoire v6.3/v6.4] Image Hygiene + Evidence Shield
            v = str(ev.get('Value', '')).lower()
            p = str(ev.get('Path', '')).lower()
            t = str(ev.get('Target_Path', '')).lower()
            c = str(ev.get('CommandLine', '')).lower()
            check_val = f"{v} | {p} | {t} | {c}"
            norm_check = check_val.replace("/", "\\").replace(".\\", "").replace("\\\\", "\\")
            tags = str(ev.get('Tag', '')).upper()
            score = int(ev.get('Score', 0))
            fname = v.split("\\")[-1].split("/")[-1]
            
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # [Fix] Hard Noise Filter Check (OPTIMIZED v6.9)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            
            # 1. EXTREME NOISE (Unconditionally drop)
            if extreme_noise_re and extreme_noise_re.search(norm_check):
                continue
            
            # 2. Rescuable Noise
            is_rescued = any(rt in tags for rt in RESCUE_TAGS)
            
            if not is_rescued:
                # Hard Filter Check
                if hard_noise_re and hard_noise_re.search(norm_check):
                    continue
                
                # AppData Packages check (common noise source)
                if "appdata\\local\\packages" in norm_check and score < 500:
                    continue
            
            # [v6.4] Evidence Shield - Recon keywords protect images
            RECON_KEYWORDS = ["xampp", "phpmyadmin", "admin", "dashboard", "kibana", 
                              "phishing", "c2", "login", "webshell", "backdoor", "exploit"]
            image_exts = [".png", ".jpg", ".gif", ".ico", ".bmp"]
            if any(fname.endswith(ext) for ext in image_exts):
                if any(kw in norm_check for kw in RECON_KEYWORDS):
                    ev['Score'] = max(score, 600)
                    if "INTERNAL_RECON" not in tags:
                        ev['Tag'] = (tags + ",INTERNAL_RECON").strip(',')
                    filtered_iocs.append(ev)
                    continue  # Protected - keep and skip noise checks
            
            # [v6.3] Image Hygiene - Extended system/cache paths
            noise_exts = [".mui", ".nls", ".dll", ".sys", ".jpg", ".png", ".gif", ".ico", ".xml", ".dat"]
            system_resource_paths = [
                "windows\\system32", "windows\\syswow64", "windows\\web\\",
                "windows\\branding\\", "program files\\windowsapps",
                "programdata\\microsoft\\windows\\systemdata",
            ]
            browser_cache_paths = [
                "appdata\\local\\microsoft\\windows\\inetcache",
                "appdata\\local\\google\\chrome\\user data\\default\\cache",
                "temporary internet files", "content.ie5",
            ]
            
            if any(fname.endswith(ext) for ext in noise_exts):
                if any(t in tags for t in ["RECON", "EXFIL", "MASQUERADE", "SCREENSHOT", "LATERAL"]):
                    pass  # Keep
                elif any(sp in norm_check for sp in system_resource_paths):
                    continue
                elif any(bp in norm_check for bp in browser_cache_paths):
                    continue
            
            # 3. Trusted Analyzer Logic
            if analyzer and hasattr(analyzer, '_is_noise'):
                if analyzer._is_noise(ev):
                    continue
            
            # 4. Redundant Check (Garbage Patterns)
            if score < 300 and not any(x in tags for x in ["LATERAL", "RANSOM", "WIPER"]):
                if garbage_re and garbage_re.search(norm_check):
                    continue

            # [Grimoire v8.1] Rule 4 (Strict White Noise Filter) - System32 & Old Files
            # If in System32/SysWOW64 and NO malicious tags, treat as trusted background noise.
            is_system_path = any(x in norm_check for x in ["windows\\system32", "windows\\syswow64"])
            malicious_markers = ["TIMESTOMP", "MALWARE", "WEBSHELL", "C2", "RANSOM", "ROOTKIT", "UNUSUAL", "MALICIOUS", "STAGING"]
            has_mal_tag = any(m in tags for m in malicious_markers)
            
            if is_system_path and not has_mal_tag:
                continue

            # Handle Old Files (Ancient leftovers) without Timestomp
            # If year is old (e.g. 2013-2015) and NOT part of the incident timeline, and NO timestomp tag
            time_str = str(ev.get('Time', ''))
            is_ancient = any(y in time_str for y in ["2010-", "2011-", "2012-", "2013-", "2014-"])
            if is_ancient and not has_mal_tag:
                continue

            # [Grimoire v7.0/v9.1] Enforce Dynamic Score Threshold
            # Use provided threshold (300 for Summary, 50 for Timeline)
            if score < threshold and not has_mal_tag and not any(t in tags for t in ["CRITICAL", "EXFIL", "LATERAL"]):
                continue

            filtered_iocs.append(ev)
        
        # [Grimoire v8.0] Rule B: Incident-Centric De-duplication
        # Merge multiple detections (Execution, Creation, etc.) for the same file at the same time
        dedup_map = {}
        for ev in filtered_iocs:
            # [Grimoire v9.1] Rule ğŸ…±ï¸ Round Time to Minutes for aggressive de-duplication
            # The user specifically asked for "same minute" merging.
            t_full = str(ev.get('Time', ''))
            t_min = t_full[:16] if len(t_full) >= 16 else t_full
            v = str(ev.get('Value', '')).lower().replace("/", "\\")
            key = (t_min, v)
            
            if key not in dedup_map:
                dedup_map[key] = ev.copy()
                # Initialize Actions list for tracking what was merged
                dedup_map[key]['_actions'] = [str(ev.get('Type', ev.get('Event_Category', 'Unknown')))]
            else:
                existing = dedup_map[key]
                # Inherit Max Score
                existing['Score'] = max(int(existing.get('Score', 0)), int(ev.get('Score', 0)))
                # Combine Tags
                tags1 = set(str(existing.get('Tag', '')).split(","))
                tags2 = set(str(ev.get('Tag', '')).split(","))
                existing['Tag'] = ",".join(list(filter(None, tags1.union(tags2))))
                # Collect Actions
                act = str(ev.get('Type', ev.get('Event_Category', 'Unknown')))
                if act not in existing['_actions']:
                    existing['_actions'].append(act)

        # Finalize merged events
        iocs = []
        for ev in dedup_map.values():
            if len(ev.get('_actions', [])) > 1:
                # [Grimoire v9.0] Rule 2: Semantic Merging with better labels
                acts_lower = [a.lower() for a in ev['_actions']]
                
                # Determine "Mission-based" Label
                label = "MULTIPLE_ACTIVITY"
                if any("credential" in a for a in acts_lower): label = "ğŸ” CREDENTIAL_THEFT_OPERATION"
                elif any("webshell" in a for a in acts_lower): label = "ğŸ•·ï¸ WEBSHELL_OPERATION"
                elif any("ransom" in a for a in acts_lower): label = "â˜ ï¸ RANSOMWARE_ACTIVITY"
                elif any("lateral" in a for a in acts_lower): label = "ğŸš€ LATERAL_MOVEMENT_OP"
                elif any("malware" in a for a in acts_lower): label = "ğŸ¦  MALWARE_EXECUTION"
                elif any("anti-forensics" in a for a in acts_lower): label = "ğŸ§¹ ANTI_FORENSICS_OP"
                elif any("execution" in a for a in acts_lower): label = "âš™ï¸ COMMAND_EXECUTION"
                
                ev['Type'] = label
                # Detailed Activity Description in Note
                acts_summary = ", ".join(ev['_actions'])
                ev[
                    'Note'
                ] = f"Incident Details: Executed tool activity detected. Actions: {acts_summary}. Unified high-risk artifact."
            iocs.append(ev)

        # Helper: Extract directory path
        def get_parent(path):
            if not path: return ""
            return str(Path(path).parent)

        # 1. Bucket by Type + Tag + ParentDir
        buckets = {}
        processed_ids = set() # To avoid double processing if logic changes

        # Sort by directory to help sequential processing
        try:
            sorted_iocs = sorted(iocs, key=lambda x: get_parent(x.get('Value', '')))
        except:
            sorted_iocs = iocs
        
        # Grouping candidates
        for ev in sorted_iocs:
            cat = self._get_event_category(ev)
            tag = str(ev.get('Tag', ''))
            val = str(ev.get('Value', ''))
            score = int(ev.get('Score', 0))
            
            # Smart Filename Extraction for Grouping
            filename = str(ev.get('FileName') or ev.get('Target_FileName') or ev.get('Target_Path') or '')
            if not filename and "USN" in tag:
                # Try parsing from Note or Summary if it looks like a path
                # Note format: "Lateral Movement (USN)" or similar
                # Summary might be better.
                summ = str(ev.get('Summary', ''))
                if "." in summ and "\\" in summ: # rudimentary path check
                     filename = summ
                elif str(ev.get('Note', '')).find(":\\") > 0:
                     filename = str(ev.get('Note', ''))

            # --- PHASE 0: Score Adjustment (Context-Aware) ---
            path_for_adjust = str(ev.get('Value', '') or ev.get('Path', ''))
            new_tags = []
            
            # [Grimoire v8.0] Rule C: Contextual Demotion (Media files & User Activity)
            # If a media file is marked as SENSITIVE_DATA_ACCESS but lacks exfil evidence, demote it.
            media_exts = [".mp4", ".ogv", ".gif", ".jpg", ".png", ".avi", ".mov"]
            if any(path_for_adjust.lower().endswith(ext) for ext in media_exts):
                if score >= 500 and not any(t in tag for t in ["EXFIL", "C2", "MALICIOUS"]):
                    # Demote to User Activity level unless it's a known exfil path
                    score = min(score, 250) 
                    if "SENSITIVE_DATA_ACCESS" in new_tags: new_tags.remove("SENSITIVE_DATA_ACCESS")
                    new_tags.append("USER_ACTIVITY_MEDIA")

            ev['Score'] = score  # Update the event's score permanently
            if new_tags:
                existing_tags = tag.split(",") if tag else []
                # Keep original tags but add/update with new insights
                combined_tags = list(set(existing_tags + new_tags))
                ev['Tag'] = ",".join(combined_tags)
                tag = ev['Tag'] # Update local var for filtering
            
            rescue_tags = ["CRITICAL", "TIMESTOMP", "KNOWN_WEBSHELL", "C2", "RANSOM", "ROOTKIT"]
            has_rescue_tag = any(t in tag for t in rescue_tags)
            
            # [DEBUG] Trace SetMACE/PuTTY in Renderer
            if "TIMESTOMP" in tag or "REMOTE_ACCESS" in tag or score >= 300:
                  with open("renderer_trace.log", "a", encoding="utf-8") as f:
                       f.write(f"[RENDERER] Validating: {filename} Sc={score} Tag={tag} Rescue={has_rescue_tag}\n")
            
            # [Debug Verbose]
            if "USN" in tag:
                with open("debug_verbose.log", "a", encoding="utf-8") as f:
                     import os
                     if os.path.getsize("debug_verbose.log") < 50000: # Increase limit
                         f.write(f"USN Item: Cat={cat} Tag={tag} Val={val} Sc={score} Time={ev.get('Time')}\n")
            
            # [Phase 6] Category-based Thresholding (Grimoire Engine v6.0)
            # High-Confidence Filtering: Only show Score >= 300 unless critical context
            CATEGORY_THRESHOLDS = {
                "EXECUTION": 300,        # [Strict] Filter reconnaissance and noise (was 150)
                "VULNERABLE APP": 300,   # [Strict] High noise potential (was 150)
                "ANTI-FORENSICS": 300,   # [Strict] Now relying on boosted scores for real findings (was 50)
                "LATERAL MOVEMENT": 300, # [Strict] (was 50)
                "TIMESTOMP": 300,        # [Strict] (was 50)
                "DEFAULT": 300           # Base threshold (was 50)
            }
            
            threshold = CATEGORY_THRESHOLDS.get(cat, CATEGORY_THRESHOLDS["DEFAULT"])
            if "VULNERABLE" in cat: threshold = CATEGORY_THRESHOLDS["VULNERABLE APP"] # Match partial keys

            # Rescue Tags: Keep these even if score is low (Contextual Importance)
            # [Grimoire v6.1] Force Merge: Added LATERAL, RECON, SENSITIVE, EXFIL to rescue list
            rescue_tags = [
                "CRITICAL", "KNOWN_WEBSHELL", "C2", "RANSOM", "ROOTKIT", "MANUAL_REVIEW",
                "LATERAL", "UNC_", "INTERNAL_RECON", "SENSITIVE", "EXFIL", "DATA_EXFIL" 
            ]
            has_rescue_tag = any(t.upper() in tag.upper() for t in rescue_tags)
            
            # Strict Drop for low scores (using Category Thresholds)
            if score < threshold and not has_rescue_tag:
                 continue

            # --- PHASE 1: Grouping Logic ---
            
            should_group = False
            group_key = ""
            
            # [Grimoire v6.2] Toolkit Grouping (Parent-Child)
            # If a directory contains a high-score tool (>= 700), group all files in that directory
            parent_dir = get_parent(val).lower()
            well_known_tool_dirs = ["setmace", "mimikatz", "sdelete", "psexec", "lazagne", "wce"]
            is_tooldir = any(t in parent_dir for t in well_known_tool_dirs)
            
            if is_tooldir:
                should_group = True
                # Extract tool name from path
                tool_name = "Unknown Toolkit"
                for t in well_known_tool_dirs:
                    if t in parent_dir: tool_name = t.upper(); break
                group_key = f"TOOLKIT|{tool_name}"
            
            # 1.1 VULNERABLE APP (Bulk Grouping)
            elif "VULNERABLE APP" in cat:
                should_group = True
                group_key = f"VULN|{tag}" 

            # [Feature] Filename Prefix Grouping (choco, sdelete)
            # Groups choco.exe, choco.exe.manifest, choco.exe.pf -> choco.exe
            elif any(prefix in filename.lower() for prefix in ["choco", "sdelete", "bcwipe"]):
                should_group = True
                # Extract prefix
                prefix = next(p for p in ["choco", "sdelete", "bcwipe"] if p in filename.lower())
                group_key = f"TOOL_PREFIX|{prefix}"
                filename = f"{prefix} (Aggregated Artifacts)" # For display 

            # 1.2 RECON / EXFILTRATION (Google Search Grouping)
            # [User Request] Group repetitive Google URLs to improve readability
            elif "RECON" in tag or "EXFIL" in tag:
                # Check for Google Search URL patterns
                if "google" in val.lower() and ("search?" in val or "url?" in val):
                    should_group = True
                    group_key = "RECON_GOOGLE_SEARCH"
                    filename = "Google Search URLs (Reconnaissance)" # Override filename for display

            # [Grimoire v7.0] CCLEANER Aggregation (Execution + Prefetch + Registry)
            elif "ccleaner" in filename.lower() or "ccleaner" in val.lower():
                should_group = True
                group_key = "TOOL_CCLEANER_GROUP"
                filename = "CCleaner Tool Activity"

            # [Grimoire v7.0/v9.1] WebShell Burst Aggregation (pwin10, classic_{...} etc.)
            elif any(re.search(p, filename.lower()) for p in [r"classic_\{", r"^[0-9a-f]{4,}.*distr", r"pwin10_"]):
                should_group = True
                # Group by parent directory to keep bursts separated by location
                parent = get_parent(path_for_adjust)
                group_key = f"WEBSHELL_BURST|{parent}"
                filename = "WebShell Generation Burst"

            # 1.2 WEBSHELL (Obfuscation Grouping) - Now mostly handled by Score Cut (<90 dropped)
            elif "WEBSHELL" in cat:
                if score <= 85 and "OBFUSCATION" in tag and not has_rescue_tag:
                    parent = get_parent(val)
                    group_key = f"OBFUSC|{parent}"
                    should_group = True

            # 1.3 LATERAL / USN Condensation
            # Group by Filename for USN bursts
            elif "LATERAL" in cat:
                 # Check if USN related (Value is Action like 'FileCreate') OR Tag has USN
                 if ("USN" in tag or "|" in val):
                      if filename:
                          group_key = f"USN|{filename}"
                          should_group = True
                      else:
                          # Fallback: Group by Timestamp (Minute precision to catch bursts)
                          # Time format often: 2015-09-03 10:03:01
                          t = str(ev.get('Time', ''))
                          if len(t) >= 16:
                              group_key = f"USN|BURST|{t[:16]}" # Group by minute e.g. 2015-09-03 10:03
                              should_group = True
                          
                          # [Debug]
                          if "USN" in tag:
                               with open("debug_grouping.log", "a", encoding="utf-8") as f:
                                   f.write(f"USN Item: T={t} F={filename} Group={should_group} Key={group_key}\n")

            if not should_group:
                grouped_iocs.append(ev)
                continue
            
            if group_key not in buckets: buckets[group_key] = []
            buckets[group_key].append(ev)
            
        # 2. Process Buckets
        with open("debug_grouping.log", "a", encoding="utf-8") as f:
             f.write(f"Buckets: {len(buckets)} keys. Sizes: {[len(v) for v in buckets.values()]}\n")

        for key, bucket in buckets.items():
            # Threshold: 5 items (User suggested 10 for USN, but 5 is safe default)
            threshold = 5 
            if "USN" in key: threshold = 5 # Strict condensation for USN
            
            if len(bucket) >= threshold:
                first = bucket[0]
                
                # Determine Label
                if "VULN" in key:
                    label_type = "VULNERABLE APP"
                    label_desc = f"Files (Related to {first.get('Tag', '')})"
                    
                elif "RECON_GOOGLE_SEARCH" in key:
                    label_type = "RECONNAISSANCE"
                    label_desc = "Google Search URLs related to Exfiltration/Recon"
                    
                elif "OBFUSC" in key:
                    label_type = "WEBSHELL" 
                    filenames = [os.path.basename(str(b.get('Value'))) for b in bucket[:3]]
                    examples = ", ".join(filenames)
                    label_desc = f"Files (Potential Noise / Obfuscated Libs) - e.g. {examples}..."
                    
                elif "TOOL_PREFIX" in key:
                    prefix_name = key.split("|")[1].upper()
                    label_type = "EXECUTION" 
                    if "STAGING" in str(first.get('Tag', '')): label_type = "STAGING TOOL"
                    elif "ANTI" in str(first.get('Tag', '')): label_type = "ANTI-FORENSICS"
                    label_desc = f"{prefix_name} Artifact Cluster ({len(bucket)}x events: Exe, Pf, Reg etc.)"
                    
                elif "USN" in key:
                    label_type = "LATERAL MOVEMENT"
                    # User: 10x USN Events (FileCreate/DataExtend) - tmpudvfh.php
                    # Using Set of Actions for description
                    actions = sorted(list(set(str(b.get('Value')) for b in bucket)))
                    action_str = "/".join(actions[:3]) # Limit to 3 actions
                    if len(actions) > 3: action_str += "..."
                    
                    if "BURST" in key:
                        label_desc = f"USN Activity Burst ({action_str}) - Unknown File"
                    else:
                        fname = key.split("|")[1]
                        # Clean filename if it's a full path
                        fname_short = os.path.basename(fname)
                        label_desc = f"USN Events ({action_str}) - File: {fname_short}"
                
                # Rule 1: Aggressive WebShell Burst with Rich Context (v9.0)
                elif "WEBSHELL_BURST" in key:
                    label_type = "ğŸ•·ï¸ WEBSHELL_BURST (Mass Creation)"
                    names = sorted(list(set(str(b.get('FileName') or '').lower() for b in bucket)))
                    
                    # Analyze variants for 'Target' field
                    if any("pwin10" in n for n in names):
                        target_pattern = "pwin10_{20,40,80}_(anim)_distr.png variants"
                    else:
                        target_pattern = ", ".join(names[:3]) + "..." if len(names) > 3 else ", ".join(names)
                    
                    summary = f"WebShell script batch creation detected (Count: {len(bucket)} files)"
                    impact = "Multiple backdoors established in Web directory for persistence."
                    
                    label_desc = f"{summary}\nTarget: {target_pattern}\nImpact: {impact}\n\n[Technical Details]\n{target_pattern}"

                else:
                    label_type = "GROUP"
                    label_desc = "Events"
                
                # [Optimization v8.0] Inherit best Score and aggregate Actions (Rule B style but for buckets)
                max_score = max(int(b.get('Score', 0)) for b in bucket)
                all_tags = set()
                for b in bucket:
                    for t in str(b.get('Tag', '')).split(","):
                        if t: all_tags.add(t)
                
                # Check timestamps span
                times = [b.get('Time') for b in bucket if b.get('Time')]
                t_str = "Unknown"
                if times: 
                    min_t, max_t = min(times), max(times)
                    t_str = f"{min_t} - {max_t}"
                    if min_t == max_t: t_str = str(min_t)
                
                summary_ev = first.copy()
                summary_ev['Type'] = label_type 
                summary_ev['Value'] = f"{len(bucket)}x {label_desc}" # This value will be shown in table
                summary_ev['Note'] = "Grouped Artifacts"
                summary_ev['Time'] = t_str
                summary_ev['Score'] = max_score
                summary_ev['Tag'] = ",".join(all_tags)
                
                grouped_iocs.append(summary_ev)
            else:
                grouped_iocs.extend(bucket)
        
        return sorted(grouped_iocs, key=lambda x: str(x.get('Score', 0)), reverse=True)

    def _split_iocs_top15(self, refined_iocs):
        """
        [Feature 5] Noise Zero: Split IOCs into Top 15 Critical and Overflow/Contextual.
        
        Returns:
            tuple: (section_7_1, section_7_2)
                - section_7_1: Top 15 IOCs with Score >= 500
                - section_7_2: Overflow (16+) + Contextual (300-499), sorted by score
        """
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # [Fix] Ensure Sorting BEFORE Slicing - Critical Bug Fix!
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        # ã¾ãšå…¨ä½“ã‚’ã‚¹ã‚³ã‚¢é™é †ã§ã‚½ãƒ¼ãƒˆï¼ˆè¶…é‡è¦ï¼ï¼‰
        refined_iocs.sort(key=lambda x: int(x.get('Score', 0)), reverse=True)

        # 1. Extract candidates (Score >= 500 and 300-499)
        high_conf_candidates = [i for i in refined_iocs if int(i.get('Score', 0)) >= 500]
        context_candidates = [i for i in refined_iocs if 300 <= int(i.get('Score', 0)) < 500]
        
        # 2. Top 15 slicing (now guaranteed to be highest scores first)
        section_7_1 = high_conf_candidates[:15]
        overflow_high = high_conf_candidates[15:]  # 16th onwards
        
        # 3. Merge overflow with contextual and re-sort
        # This ensures Score 1000 at position 16 appears at top of Section 7.2
        section_7_2 = overflow_high + context_candidates
        section_7_2.sort(key=lambda x: int(x.get('Score', 0)), reverse=True)
        
        return (section_7_1, section_7_2)

    # --- Context Helper Methods ---

    def _get_attack_methods(self, analyzer):
        t = self.txt
        visual_iocs = analyzer.visual_iocs
        has_paradox = any("TIME_PARADOX" in str(ioc.get('Type', '')) for ioc in visual_iocs)
        has_masquerade = any("MASQUERADE" in str(ioc.get('Type', '')) for ioc in visual_iocs)
        has_phishing = any("PHISHING" in str(ioc.get('Type', '')) for ioc in visual_iocs)
        has_timestomp = any("TIMESTOMP" in str(ioc.get('Type', '')) for ioc in visual_iocs)
        has_anti = any("ANTI" in str(ioc.get('Type', '')) or "ANTIFORENSICS" in str(ioc.get('Tag', '')) for ioc in visual_iocs)
        has_lateral = any("LATERAL" in str(ioc.get('Type', '')) or "LATERAL" in str(ioc.get('Tag', '')) or "UNC_EXECUTION" in str(ioc.get('Tag', '')) for ioc in visual_iocs)
        has_recon = any("RECON" in str(ioc.get('Type', '')) or "INTERNAL_RECON" in str(ioc.get('Tag', '')) for ioc in visual_iocs)

        methods = []
        if has_phishing: methods.append(t.get('attack_phishing', "Phishing"))
        if has_masquerade: methods.append(t.get('attack_masquerade', "Masquerading"))
        if has_timestomp: methods.append(t.get('attack_timestomp', "Timestomping"))
        if has_paradox: methods.append(t.get('attack_paradox', "Time Paradox"))
        if has_anti: methods.append(t.get('attack_anti', "Anti-Forensics"))
        if has_lateral: methods.append(t.get('attack_lateral', "Lateral Movement"))
        if has_recon: methods.append(t.get('attack_recon', "Internal Reconnaissance"))
        if not methods: methods.append(t.get('attack_default', "General Intrusion"))
        return methods

    def _get_display_value(self, row):
        """
        Smart Formatting: Prioritize meaningful columns over generic 'system' or placeholders.
        """
        candidates = [
            row.get("Value"),         # [Display Decoupling] Use already cleaned/truncated Value
            row.get("Summary"),       # [Display Decoupling] Fallback to cleaned Summary
            row.get("FileName"),
            row.get("Target_Path"),
            row.get("Target_FileName"), 
            row.get("CommandLine"),
            row.get("ParentPath"),   
            row.get("Reg_Key"),
            row.get("Service_Name"), 
            row.get("Payload"),
            row.get("Message"),      
            row.get("Action"),
        ]
        fallback_hash = None
        for c in candidates:
            val = str(c).strip()
            if not c or val in ["", "None", "N/A"]: continue
            
            # Check if likely hash (32=MD5, 40=SHA1, 64=SHA256) and hex-only
            if len(val) in [32, 40, 64] and re.match(r"^[a-fA-F0-9]+$", val):
                if not fallback_hash: fallback_hash = val
                continue # Skip hash, look for better name
                
            return val # Found meaningful name
            
        return fallback_hash if fallback_hash else "Unknown Activity"

    def _compress_timeline_for_mermaid(self, events, time_window_minutes=5):
        """
        Timeline Aggregation: Group dense events into buckets to declutter visualization.
        """
        compressed = []
        if not events: return []
        
        # Sort by time just in case
        sorted_events = sorted(events, key=lambda x: x.get('Time', ''))
        
        # Parse first event time
        try:
            current_bucket = sorted_events[0].copy()
            current_bucket['dt'] = datetime.strptime(str(current_bucket['Time']).split('.')[0], '%Y-%m-%dT%H:%M:%S')
        except:
            return events # Fallback if parsing fails

        count = 1
        bucket_tags = set()
        if 'Tag' in current_bucket: bucket_tags.update(str(current_bucket['Tag']).split(','))

        for i in range(1, len(sorted_events)):
            ev = sorted_events[i].copy()
            try:
                ev_dt = datetime.strptime(str(ev['Time']).split('.')[0], '%Y-%m-%dT%H:%M:%S')
            except:
                continue

            delta = ev_dt - current_bucket['dt']
            if delta.total_seconds() < (time_window_minutes * 60):
                # Merge into bucket
                count += 1
                if 'Tag' in ev: bucket_tags.update(str(ev['Tag']).split(','))
            else:
                # Flush bucket
                current_bucket['Display'] = f"{', '.join(sorted(bucket_tags))} ({count} Events)" if count > 1 else current_bucket.get('Summary', '')
                current_bucket['Tag'] = ', '.join(sorted(bucket_tags))
                compressed.append(current_bucket)
                
                # Start new bucket
                current_bucket = ev
                current_bucket['dt'] = ev_dt
                count = 1
                bucket_tags = set()
                if 'Tag' in current_bucket: bucket_tags.update(str(current_bucket['Tag']).split(','))
        
        # Flush last bucket
        current_bucket['Display'] = f"{', '.join(sorted(bucket_tags))} ({count} Events)" if count > 1 else current_bucket.get('Summary', '')
        current_bucket['Tag'] = ', '.join(sorted(bucket_tags))
        compressed.append(current_bucket)
        
        return compressed

    def _prepare_key_indicators(self, events):
        grouped = {}
        cat_titles = {
            "INITIAL ACCESS": "ğŸ£ Initial Access", "ANTI-FORENSICS": "ğŸ™ˆ Anti-Forensics",
            "SYSTEM MANIPULATION": "ğŸš¨ System Time Manipulation", "PERSISTENCE": "âš“ Persistence",
            "EXECUTION": "âš¡ Execution", "TIMESTOMP (FILE)": "ğŸ•’ Timestomp (Files)",
            "WEBSHELL": "ğŸ•¸ï¸ WebShell Intrusion", "LATERAL MOVEMENT": "ğŸ› Lateral Movement",
            "VULNERABLE APP": "ğŸ”“ Vulnerable Application",
            "REMOTE_ACCESS": "ğŸ“¡ Remote Access"
        }
        
        temp_groups = {}
        for ev in events:
            # [v7.0 User Request] Raised threshold from 50 to 500 to filter diagnostic noise
            if ev.get('Score', 0) < 500 and "CRITICAL" not in str(ev.get('Type', '')): continue
            cat = self._get_event_category(ev)
            if cat not in temp_groups: temp_groups[cat] = []
            
            impact = "-"
            extra = ev.get('Extra', {})
            tag = str(ev.get('Tag', ''))
            
            # [Display Decoupling] Use cleaned Value for impact if it looks like code/JSON
            # Otherwise use specialized logic
            cmd_line = ev.get('CommandLine') or ev.get('Payload') or extra.get('CommandLine', '')
            
            if "{" in str(cmd_line) or "}" in str(cmd_line) or "ScriptBlock" in str(cmd_line):
                # Use cleaned Value instead of raw CommandLine
                impact = f"`{ev.get('Value')}`"
            elif cmd_line and len(str(cmd_line)) > 3:
                impact = f"`{str(cmd_line)[:50]}...`" if len(str(cmd_line)) > 50 else f"`{cmd_line}`"
            elif "SYSTEM_TIME" in tag or "4616" in tag or "TIME_PARADOX" in str(ev.get('Type', '')):
                impact = "**System Clock Altered**"
            elif cat == "INITIAL ACCESS":
                tgt = extra.get('Target_Path', 'Unknown')
                if tgt and tgt != "Unknown":
                    impact = f"Target: {tgt[:30]}..."
            ev['Impact'] = impact
            # [Refinement] Smart Formatting
            ev['Value'] = self._get_display_value(ev)
            temp_groups[cat].append(ev)

        # [Grouping] Collapse repetitive events
        for k in temp_groups:
            if k == "VULNERABLE APP" or k == "EXECUTION":
                bucket = []
                collapsed_group = []
                
                # Simple groupings by Tag
                tag_map = {}
                for ev in temp_groups[k]:
                    t = ev.get('Tag', 'Other')
                    if t not in tag_map: tag_map[t] = []
                    tag_map[t].append(ev)
                
                for t, ev_list in tag_map.items():
                    if len(ev_list) > 5:
                        first = ev_list[0]
                        last = ev_list[-1]
                        summary_ev = first.copy()
                        summary_ev['Value'] = f"{len(ev_list)}x Files ({t})"
                        summary_ev['Impact'] = f"Bulk Artifacts (e.g. {first.get('Value')})"
                        summary_ev['Time'] = f"{first.get('Time')} - {last.get('Time')}"
                        collapsed_group.append(summary_ev)
                    else:
                        collapsed_group.extend(ev_list)
                temp_groups[k] = collapsed_group

            temp_groups[k].sort(key=lambda x: str(x.get('Time', '9999') or '9999'))


        ordered_keys = sorted(temp_groups.keys(), key=lambda k: 0 if "SYSTEM" in k else 1)
        final_groups = {cat_titles.get(k, k): temp_groups[k] for k in ordered_keys}
        return final_groups

    def _prepare_origin_seeds(self, seeds, include_keyword, origin_stories, exclude=None):
        results = []
        seen_targets = set()
        for seed in seeds:
            reason = seed.get("Reason", "")
            target = seed.get('Target_File', '')
            
            if include_keyword in reason and (not exclude or exclude not in reason):
                # [Fix] LNK Noise Filter (Initial Access Table)
                t_lower = target.lower()
                is_std_app = any(p in t_lower for p in ["system32", "program files", "control panel", "windows power"])
                is_known_lnk = any(k in t_lower for k in ["command prompt", "file explorer", "task manager", "run.lnk"])
                if is_std_app or is_known_lnk: continue

                if target in seen_targets: continue
                seen_targets.add(target)
                
                name = target
                origin_desc = "â“ No Trace Found (Low Confidence)"
                story = next((s for s in origin_stories if s["Target"] == name), None)
                if story:
                    ev = story["Evidence"][0]
                    url = ev.get("URL", "")
                    url_display = (url[:50] + "...") if len(url) > 50 else url
                    gap = ev.get('Time_Gap', '-')
                    conf = story.get("Confidence", "LOW")
                    reason_story = story.get("Reason", "")
                    icon = "âœ…" if conf == "HIGH" else "âš ï¸" if conf == "MEDIUM" else "â“"
                    prefix = "**Confirmed**" if conf == "HIGH" else "Inferred" if conf == "MEDIUM" else "Weak"
                    origin_desc = f"{icon} **{prefix}**: {reason_story}<br/>ğŸ”— `{url_display}`<br/>*(Gap: {gap})*"
                
                seed['Origin_Desc'] = origin_desc
                results.append(seed)
        return results

    def _prepare_anti_forensics(self, ioc_list, dfs):
        t = self.txt
        af_tools = [ioc for ioc in ioc_list if "ANTI" in str(ioc.get("Type", "")) or "WIPE" in str(ioc.get("Type", ""))]
        processed = []
        seen = set()
        
        for tool in af_tools:
            name = tool.get("Value", "Unknown").upper()
            if name in seen: continue
            seen.add(name)
            
            run_count = self._extract_dual_run_count(tool, dfs)
            desc = t.get('note_anti_cleanup', "Cleanup tool.")
            if "BCWIPE" in name: desc = t.get('note_anti_bcwipe', "Military wiper.")
            elif "CCLEANER" in name: desc = t.get('note_anti_ccleaner', "System cleaner.")
            
            note = t.get('note_anti_cleanup', "")
            
            processed.append({
                "Name": name,
                "RunCount": run_count,
                "LastRun": (tool.get("Time") or "Unknown").replace("T", " ")[:19],

                "Desc": desc,
                "AnalystNote": note
            })
        return processed

    def _prepare_technical_findings_from_list(self, ioc_list, analyzer, origin_stories):
        # [Fix] Apply global threshold (500) to Detailed Findings too.
        high_conf_events = [ioc for ioc in ioc_list if (int(ioc.get('Score', 0) or 0) >= 500) or analyzer.is_force_include_ioc(ioc)]
        
        # [Rule ğŸ…±ï¸ Grouped display] Structure: { category: [ {Insight: "Comment", Artifacts: [ioc1, ioc2]}, ... ] }
        cat_groups = {}
        
        for ioc in high_conf_events:
            cat = self._get_event_category(ioc)
            if "ANTI" in cat: continue
            if cat not in cat_groups: cat_groups[cat] = {} # insight -> list of iocs
            
            insight = analyzer.generate_ioc_insight(ioc)
            val = ioc.get('Value', '')
            story = next((s for s in origin_stories if s["Target"] == val), None) if origin_stories else None
            if story and story.get("Confidence") == "HIGH":
                 gap = story['Evidence'][0].get('Time_Gap', '-')
                 web_note = self.txt.get('web_download_confirmed', "Web Download").format(gap=gap)
                 insight = web_note + (insight if insight else "")
            
            narrative = self._generate_tech_narrative(ioc, ioc_list)
            if narrative:
                insight = (insight + "\n\n" + narrative) if insight else narrative
            
            if insight not in cat_groups[cat]:
                cat_groups[cat][insight] = []
            cat_groups[cat][insight].append(ioc)
        
        # Final Format for Template
        final_groups = {}
        for cat, insight_map in cat_groups.items():
            final_groups[cat] = []
            for insight, artifacts in insight_map.items():
                first = artifacts[0].copy()
                # [Rule ğŸ…±ï¸ Grouped display] Add list of all files to insight text
                if len(artifacts) > 1:
                    file_list = "\n".join([f"- `{a.get('Value') or a.get('FileName')}`" for a in artifacts])
                    insight = f"### {insight}\n\n**Total Related Artifacts: {len(artifacts)}**\n{file_list}"
                
                first['Insight'] = insight
                final_groups[cat].append(first)

        if "LATERAL MOVEMENT" in final_groups:
             # Flatten and re-group for USN is tricky, let's keep it simple for now
             pass
             
        return final_groups



    def _condense_usn_events(self, events):
        """
        USN Journal high-volume condenser. Groups by (TimeSecond, FileName).
        Target: Source/Tag is USN, Category LATERAL or FILE.
        Handles both Visual IOCs and Timeline Events.
        """
        condensed = []
        buffer = {} # Key: (TimeSecond, FileName), Value: EventDict
        
        for ev in events:
            # Check fields
            tag = str(ev.get('Tag', ''))
            src = str(ev.get('Source', ''))
            cat = str(ev.get('Category', '') or ev.get('Type', '')).upper()
            
            is_usn_source = "USN" in tag or "USN" in src
            # [User Request] Modified to accept FILE category (due to demotion)
            is_target_cat = "LATERAL" in cat or "FILE" in cat
            
            if not (is_usn_source and is_target_cat):
                condensed.append(ev)
                continue

            # Grouping Key: Time (Second) ONLY to achieve max decluttering
            # User request: "Group single lines if same second"
            t_str = str(ev.get('Time', ''))
            ts_sec = t_str[:19] # "2015-09-03 10:03:01"
            
            # Filename extraction
            fname = str(ev.get('FileName') or ev.get('Target_FileName') or ev.get('Target_Path') or '')
            if not fname:   
                 val = str(ev.get('Value', ''))
                 summ = str(ev.get('Summary', ''))
                 if ":\\" in summ: fname = summ 
                 elif ":\\" in val: fname = val
                 elif ":\\" in str(ev.get('Note', '')): fname = str(ev.get('Note', ''))
                 else: fname = 'Unknown'

            key = ts_sec # Key is just Time

            if key not in buffer:
                new_ev = ev.copy()
                act = str(ev.get('Value', '') or ev.get('Summary', ''))
                new_ev['Action_Set'] = {act} 
                new_ev['USN_Count'] = 1
                new_ev['File_Set'] = {fname} # Collect filenames
                buffer[key] = new_ev
            else:
                buffer[key]['USN_Count'] += 1
                act = str(ev.get('Value', '') or ev.get('Summary', ''))
                buffer[key]['Action_Set'].add(act)
                buffer[key]['File_Set'].add(fname)
                current_score = int(buffer[key].get('Score', 0) or 0)
                new_score = int(ev.get('Score', 0) or 0)
                buffer[key]['Score'] = max(current_score, new_score)

        # Flush Buffer
        for key, ev in buffer.items():
            count = ev.pop('USN_Count')
            files = ev.pop('File_Set', set())
            files.discard('Unknown')
            
            if count > 1:
                actions = "|".join(sorted(list(ev.pop('Action_Set'))))
                # Summarize files
                file_list = sorted(list(files))
                if len(file_list) > 3:
                    f_summary = f"{file_list[0]}, {file_list[1]}... (+{len(file_list)-2})"
                else:
                    f_summary = ", ".join(file_list)
                
                summary_text = f"**{count}x USN Events ({actions})**"
                if f_summary:
                    summary_text += f" - {f_summary}"

                
                # Update fields for display
                # For VisualIOCs (Section 4): Update Value, Tag
                if 'Value' in ev:
                    ev['Value'] = summary_text
                    ev['Tag'] = fname
                
                # For Timeline (Section 3): Update Summary, Source?
                # Template uses: {{ ev.Summary }}
                if 'Summary' in ev:
                    ev['Summary'] = summary_text
                    # Append filename to Summary or ensure it's visible?
                    # Section 3 columns: Time | Category | Summary | Source
                    # Put filename in Summary
                    ev['Summary'] = f"{summary_text} - {fname}"

            else:
                 ev.pop('Action_Set', None)
            
            condensed.append(ev)
            
        # Re-sort by Time to ensure order
        try:
            condensed.sort(key=lambda x: str(x.get('Time', '')))
        except:
            pass
            
        return condensed

    def _prepare_technical_findings(self, analyzer, origin_stories):
        # Wrapper for backward compatibility if needed, using raw visual_iocs
        return self._prepare_technical_findings_from_list(analyzer.visual_iocs, analyzer, origin_stories)

    def _prepare_stats(self, analyzer, analysis_data, dfs, refined_iocs=None):
        raw_count = analyzer.total_events_analyzed
        
        # Use refined IOCs if available, else raw
        target_iocs = refined_iocs if refined_iocs is not None else analyzer.visual_iocs
        crit_count = len(target_iocs)
        
        noise_removed = sum(analyzer.noise_stats.values()) if analyzer.noise_stats else 0
        total_processed = raw_count + noise_removed
        crit_ratio = (crit_count / total_processed * 100) if total_processed > 0 else 0
        
        crit_breakdown = []
        grouped = {}
        for ev in target_iocs:
            cat = self._get_event_category(ev)
            grouped.setdefault(cat, []).append(ev)
        for cat, items in grouped.items():
            max_score = max([int(x.get('Score', 0) or 0) for x in items])
            impact = "Evidence destruction" if "ANTI" in cat else "Evasion" if "TIME" in cat else "Compromise"
            crit_breakdown.append({"Type": cat, "Count": len(items), "MaxScore": max_score, "Impact": impact})
            
        med_breakdown = {}
        for m in analysis_data["medium_events"]:
             c = m.get('Category', 'Unknown')
             med_breakdown[c] = med_breakdown.get(c, 0) + 1
 
        return {
            "total_processed": total_processed,
            "crit_count": crit_count,
            "crit_ratio": crit_ratio,
            "noise_removed": noise_removed,
            "critical_breakdown": crit_breakdown,
            "medium_count": len(analysis_data["medium_events"]),
            "medium_breakdown": med_breakdown,
            "noise_stats": analyzer.noise_stats
        }

    def _prepare_recommendations(self, analyzer):
        rec = {"p0_evtlog": False, "p0_crx": False}
        if any("ANTI" in str(ioc.get('Type', '')) for ioc in analyzer.visual_iocs):
            rec["p0_evtlog"] = True
        if any("MASQUERADE" in str(ioc.get('Type', '')) for ioc in analyzer.visual_iocs):
            rec["p0_crx"] = True
        return rec

    # --- Legacy & Utility Methods ---

    def _get_event_category(self, ev):
        typ = str(ev.get('Type', '')).upper()
        tag = str(ev.get('Tag', '')).upper()
        if "ADS" in tag or "MASQUERADE" in tag: return "EXECUTION" # Classify ADS features as Execution
        if "SYSTEM_TIME" in tag or "TIME_CHANGE" in tag or "4616" in tag or "ROLLBACK" in tag: return "SYSTEM MANIPULATION"
        if "PHISH" in typ or "LNK" in typ: return "INITIAL ACCESS"
        if "WIPE" in typ or "ANTI" in typ or "ANTIFORENSICS" in tag: return "ANTI-FORENSICS"
        if "PERSIST" in typ or "SAM_SCAVENGE" in tag or "DIRTY_HIVE" in tag: return "PERSISTENCE"
        if "VULN" in typ or "VULN" in tag: return "VULNERABLE APP"
        if "REMOTE_ACCESS" in typ: return "REMOTE_ACCESS"
        # [Case 10 Fix] HOSTS_FILE_MODIFICATION and DEFENDER_DISABLE -> EXECUTION
        if "HOSTS_FILE" in tag or "DEFENDER_DISABLE" in tag: return "EXECUTION"
        if "EXEC" in typ or "RUN" in typ: return "EXECUTION"
        
        # [v2.0] WEBSHELL åˆ¤å®š - allowlist ã§ã‚·ã‚¹ãƒ†ãƒ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é™¤å¤–
        if "WEBSHELL" in typ or "WEBSHELL" in tag:
            # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å–å¾—ã—ã¦ allowlist ãƒã‚§ãƒƒã‚¯
            filename = str(ev.get('Value', '') or ev.get('FileName', '') or ev.get('Target_Path', ''))
            path = str(ev.get('ParentPath', '') or ev.get('Full_Path', ''))
            combined = f"{path}/{filename}"
            
            # allowlist ã«ãƒãƒƒãƒã™ã‚‹å ´åˆã¯ OTHER ACTIVITY ã«åˆ†é¡
            for pattern in WEBSHELL_ALLOWLIST_PATTERNS:
                if re.search(pattern, filename) or re.search(pattern, combined):
                    return "OTHER ACTIVITY"  # WEBSHELL ã§ã¯ãªãä¸€èˆ¬ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ã«
            return "WEBSHELL"
        
        if "LATERAL" in typ or "LATERAL" in tag: return "LATERAL MOVEMENT"
        if "TIMESTOMP" in typ: return "TIMESTOMP (FILE)"
        
        return "OTHER ACTIVITY"

    
    def _is_visual_noise(self, name):
        name = str(name).strip()
        if len(name) < 3: return True
        return False
        
    def _get_short_summary(self, ev):
        val = ev.get('Value', '')
        if not val or val == "Unknown":
            val = ev.get('Summary', '')
            if not val: val = str(ev.get('Tag', 'Event'))
        if "SYSTEM_TIME" in str(ev.get('Tag', '')) or "4616" in str(val): return "System Time Changed"
        
        # [Fix] JSONèª¤çˆ†é˜²æ­¢ï¼šJSONè¨˜å·ãŒå«ã¾ã‚Œã¦ã„ãªã„å ´åˆã®ã¿ basename ã‚’é©ç”¨
        # "}]}}" ã®ã‚ˆã†ãªæ–­ç‰‡ã«ã‚‚å¯¾å¿œã§ãã‚‹ã‚ˆã†åˆ¤å®šã‚’å¼·åŒ–
        is_json_fragment = "{" in val or "}" in val or "]" in val or "[" in val or '"' in val
        is_path_like = ("\\" in val or "/" in val)
        
        if is_path_like and not is_json_fragment:
            val = os.path.basename(val.replace("\\", "/"))
            
        return val[:15] + ".." if len(val) > 15 else val

    def _render_mermaid_vertical_clustered(self, events):
        if not events: return "\n(No critical events found for visualization)\n"
        f = ["\n### ğŸ¹ Attack Flow Visualization (Timeline)\n"]
        f.append("```mermaid")
        f.append("graph TD")
        f.append("    classDef init fill:#e63946,stroke:#333,stroke-width:2px,color:white;")
        f.append("    classDef exec fill:#f4a261,stroke:#333,stroke-width:2px,color:black;")
        f.append("    classDef persist fill:#2a9d8f,stroke:#333,stroke-width:2px,color:white;")
        f.append("    classDef anti fill:#264653,stroke:#333,stroke-width:2px,color:white;")
        f.append("    classDef time fill:#a8dadc,stroke:#457b9d,stroke-width:4px,color:black;")
        f.append("    classDef phishing fill:#ff6b6b,stroke:#c92a2a,stroke-width:2px,color:white;")
        f.append("    classDef web fill:#d62828,stroke:#333,stroke-width:2px,color:white;")
        f.append("    classDef lateral fill:#e9c46a,stroke:#333,stroke-width:2px,color:black;")
        
        critical_events = [ev for ev in events if ev.get('Score', 0) >= 60 or "CRITICAL" in str(ev.get('Type', ''))]
        # [Refinement] Timeline Aggregation (Mermaid)
        sorted_events = self._compress_timeline_for_mermaid(critical_events)
        
        if not sorted_events: return "\n(No critical events found)\n"
        
        has_paradox = any("TIME_PARADOX" in str(ev.get('Type', '')) for ev in events)
        if has_paradox:
            f.append("    subgraph T_PRE [\"âš ï¸ TIME MANIPULATION\"]")
            f.append("        N_TP[\"âª <b>SYSTEM ROLLBACK DETECTED</b><br/>Time Paradox Anomaly\"]:::time")
            f.append("    end")
        
        subgraphs = []
        current_subgraph = {"nodes": [], "start_time": None, "end_time": None}
        def parse_dt(t_str):
            try: return datetime.fromisoformat(str(t_str).replace("Z", ""))
            except: return datetime.min
        last_dt = None
        node_id_counter = 0
        burst_buffer = [] 
        
        def flush_burst_buffer(buffer, target_list, counter):
            if not buffer: return counter
            first_ev = buffer[0]
            cat = self._get_event_category(first_ev)
            if len(buffer) >= 3 and ("INITIAL" in cat or "EXECUTION" in cat):
                node_id = f"N{counter}"; counter += 1
                start_t = str(buffer[0].get('Time', ''))[11:16]
                count = len(buffer)
                icon = "âš¡"
                if "INITIAL" in cat: icon = "ğŸ£"
                elif "EXEC" in cat: icon = "âš™ï¸"
                short_summary = self._get_short_summary(first_ev)
                label = f"{start_t} {icon} {count}x Events<br/>({short_summary} etc.)"
                style = ":::exec"
                if "INITIAL" in cat: style = ":::phishing"
                target_list.append(f"{node_id}[\"{label}\"]{style}")
                return counter
            else:
                for ev in buffer:
                    node_id = f"N{counter}"; counter += 1
                    t_str = str(ev.get('Time', ''))[11:16]
                    s_sum = self._get_short_summary(ev)
                    ev_cat = self._get_event_category(ev)
                    icon = "ğŸ”¹"; style = ":::default"
                    if "SYSTEM" in ev_cat: icon = "â°"; style = ":::time"
                    elif "ANTI" in ev_cat: icon = "ğŸ—‘ï¸"; style = ":::anti"
                    elif "PERSIST" in ev_cat: icon = "âš“"; style = ":::persist"
                    elif "INITIAL" in ev_cat: icon = "ğŸ£"; style = ":::init"
                    elif "WEBSHELL" in ev_cat: icon = "ğŸ•¸ï¸"; style = ":::web"
                    elif "LATERAL" in ev_cat: icon = "ğŸ›"; style = ":::lateral"
                    elif "PHISH" in ev_cat: icon = "ğŸ£"; style = ":::phishing"
                    label = f"{t_str} {icon} {s_sum}"
                    target_list.append(f"{node_id}[\"{label}\"]{style}")
                return counter

        for ev in sorted_events:
            if self._is_visual_noise(ev.get("Value", "")): continue
            dt = parse_dt(ev.get('Time', ''))
            if last_dt and (dt - last_dt).total_seconds() > 3600:
                node_id_counter = flush_burst_buffer(burst_buffer, current_subgraph["nodes"], node_id_counter)
                burst_buffer = []
                subgraphs.append(current_subgraph)
                current_subgraph = {"nodes": [], "start_time": dt, "end_time": dt}
            if current_subgraph["start_time"] is None: current_subgraph["start_time"] = dt
            current_subgraph["end_time"] = dt
            last_dt = dt
            if not burst_buffer: burst_buffer.append(ev)
            else:
                last_in_buff = burst_buffer[-1]
                last_buff_dt = parse_dt(last_in_buff.get('Time', ''))
                same_cat = self._get_event_category(ev) == self._get_event_category(last_in_buff)
                close_time = (dt - last_buff_dt).total_seconds() < 120 
                if same_cat and close_time: burst_buffer.append(ev)
                else:
                    node_id_counter = flush_burst_buffer(burst_buffer, current_subgraph["nodes"], node_id_counter)
                    burst_buffer = [ev]
        node_id_counter = flush_burst_buffer(burst_buffer, current_subgraph["nodes"], node_id_counter)
        subgraphs.append(current_subgraph)

        sg_counter = 0; prev_sg_id = None
        if has_paradox: prev_sg_id = "T_PRE"
        for sg in subgraphs:
            if not sg["nodes"]: continue
            sg_id = f"T{sg_counter}"
            start_s = sg["start_time"].strftime("%H:%M")
            end_s = sg["end_time"].strftime("%H:%M")
            label = f"â° {start_s} - {end_s}"
            f.append(f"    subgraph {sg_id} [\"{label}\"]")
            for n in sg["nodes"]: f.append(f"        {n}")
            f.append("    end")
            if prev_sg_id:
                if prev_sg_id == "T_PRE": f.append(f"    N_TP --> {sg['nodes'][0].split('[')[0]}")
                else: f.append(f"    {prev_sg_id} --> {sg_id}")
            prev_sg_id = sg_id; sg_counter += 1
        f.append("```\n")
        return "\n".join(f)

    def _render_attack_chain_mermaid(self, visual_iocs):
        """
        [User Request] Convert to Sequence Diagram for better readability.
        Phases: Prep -> Phishing -> Exec -> Recon -> Anti
        """
        # 0. Localization Map
        is_jp = self.lang == 'jp'
        txt_map = {
            "p_prep": "ğŸ› ï¸ æº–å‚™æ®µéš" if is_jp else "ğŸ› ï¸ Prep/Tools",
            "p_phish": "ğŸ£ åˆæœŸä¾µå…¥" if is_jp else "ğŸ£ Initial Access",
            "p_exec": "âš™ï¸ å®Ÿè¡Œãƒ»æ°¸ç¶šåŒ–" if is_jp else "âš™ï¸ Execution",
            "p_recon": "ğŸ” åµå¯Ÿæ´»å‹•" if is_jp else "ğŸ” Recon/Exfil",
            "p_anti": "ğŸ§¹ è¨¼æ‹ éš æ»…" if is_jp else "ğŸ§¹ Anti-Forensics",
            "note_time": "ğŸ“… ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ç¯„å›²: " if is_jp else "ğŸ“… Timeline Scope: ",
            "msg_prep": "æ”»æ’ƒãƒ„ãƒ¼ãƒ«ã‚’äº‹å‰é…ç½® ({}ä»¶)" if is_jp else "Tools Staged ({} items)",
            "msg_phish": "LNKå®Ÿè¡Œ / ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰å±•é–‹" if is_jp else "LNKs/Payloads Triggered",
            "msg_exec": "ä¸æ­£ãƒ—ãƒ­ã‚»ã‚¹å®Ÿè¡Œ / ä¾µå®³æ´»å‹•" if is_jp else "Malicious Process Activity",
            "msg_recon": "æƒ…å ±æŒã¡å‡ºã— / å†…éƒ¨åµå¯Ÿ" if is_jp else "Exfil & Cleanup Initiated",
            "note_anti": "âš ï¸ è¨¼æ‹ éš æ»… / Timestompæ¤œçŸ¥" if is_jp else "âš ï¸ Evidence Wiping/Timestomp Detected"
        }

        # 1. Bucket Events by Phase
        prep_events = []
        phish_events = []
        exec_events = []
        recon_events = []
        anti_events = []
        
        for ioc in visual_iocs:
            tag = str(ioc.get('Tag', '')).upper()
            val = str(ioc.get('Value', '')).upper()
            typ = str(ioc.get('Type', '')).upper()
            
            # Classification Logic [v6.0 - Enhanced for Case7]
            # Anti-Forensics: Timestomp, Wipe, CCleaner
            if "TIMESTOMP" in tag or "WIPE" in tag or "ANTIFORENSIC" in tag or "CCLEANER" in val:
                anti_events.append(ioc)
            # Prep/Tools: SysInternals, Admin tools, Masquerade
            elif "SYSINTERNALS" in tag or "MASQUERADE" in tag or "ADMIN_TOOL" in tag or "UNCOMMON" in tag or "SYSINTERNALS" in val:
                prep_events.append(ioc)
            # Initial Access: Phishing, LNK
            elif "PHISHING" in tag or "LNK" in typ or "INIT_ACCESS" in tag:
                phish_events.append(ioc)
            # Recon: Discovery, Exfil, Network, LotL
            elif "RECON" in tag or "EXFIL" in tag or "NETWORK" in tag or "LOTL" in tag or "DISCOVERY" in tag:
                recon_events.append(ioc)
            # Execution: Run, Process, Exec
            elif "EXEC" in tag or "PROCESS" in tag or "RUN" in val or "EXECUTION" in typ or "EXEC" in typ:
                exec_events.append(ioc)
            # [Fix] Include Auth Failures / Brute Force in Execution
            elif "AUTH_FAILURE" in tag or "BRUTE_FORCE" in tag or "4625" in str(ioc.get('Note','')):
                exec_events.append(ioc)
            # [Fix] Include Persistence in Execution
            elif "PERSIST" in tag or "PERSISTENCE" in typ:
                exec_events.append(ioc)
            # Fallback: High score items go to execution
            elif int(ioc.get('Score', 0) or 0) >= 200:
                exec_events.append(ioc)
        
        # [Fix] Global Noise Filter for Mermaid
        # Exclude items with Score < 50 unless Critical
        prep_events = [e for e in prep_events if int(e.get('Score',0)) >= 50]
        # Allow Phishing (LNK) even if low score if meaningful? No, standard LNKs are noise.
        phish_events = [e for e in phish_events if int(e.get('Score',0)) >= 100 or "CRITICAL" in str(e.get('Tag',''))]
        exec_events = [e for e in exec_events if int(e.get('Score',0)) >= 100]
        # Recon: ssh-add-manual.htm often score 50. Filter it.
        recon_events = [e for e in recon_events if int(e.get('Score',0)) >= 100]
        # Anti: Keep even low score? Evidence destruction is rare.
        anti_events = [e for e in anti_events if int(e.get('Score',0)) >= 100]
                
        if not (prep_events or phish_events or exec_events or recon_events or anti_events):
            return ""

        f = []
        f.append("\n### ğŸ¹ Attack Flow Visualization (Verb-Based Timeline)\n")
        f.append("```mermaid")
        f.append("sequenceDiagram")
        
        # [v6.1] Dynamic Participants based on actual events (Verb-Based)
        # Define Participants with Action Verbs
        txt_verb_map = {
            "p_download": "ğŸ“¥ Download" if not is_jp else "ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            "p_execute": "âš¡ Execute" if not is_jp else "âš¡ å®Ÿè¡Œ",
            "p_discover": "ğŸ” Discover" if not is_jp else "ğŸ” åµå¯Ÿ",
            "p_cleanup": "ğŸ§¹ Cleanup" if not is_jp else "ğŸ§¹ éš æ»…"
        }
        
        f.append(f"    participant Download as {txt_verb_map['p_download']}")
        f.append(f"    participant Execute as {txt_verb_map['p_execute']}")
        f.append(f"    participant Discover as {txt_verb_map['p_discover']}")
        f.append(f"    participant Cleanup as {txt_verb_map['p_cleanup']}")
        
        # Note for Timeline
        dates = sorted([x.get('Time') for x in visual_iocs if x.get('Time')])
        if dates:
            start_d = dates[0].split('T')[0] if 'T' in str(dates[0]) else str(dates[0])[:10]
            end_d = dates[-1].split('T')[0] if 'T' in str(dates[-1]) else str(dates[-1])[:10]
            date_label = start_d if start_d == end_d else f"{start_d} ~ {end_d}"
            f.append(f"    Note over Download,Cleanup: ğŸ“… {date_label}")



        # Generate Connections (Verb-Based Story Flow)
        # 1. Download Phase (Zone.Identifier, Prep tools)
        download_events = [e for e in prep_events + phish_events if any(k in str(e.get('Tag', '')) for k in ['ZONE', 'DOWNLOAD', 'PHISH'])]
        if not download_events:
            download_events = prep_events[:2]  # Fallback
        
        if download_events:
            sorted_dl = sorted(download_events, key=lambda x: str(x.get('Time', '')))
            f.append(f"    Download->>Execute: {'ãƒ„ãƒ¼ãƒ«ãƒ»ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰å–å¾—' if is_jp else 'Payload Retrieved'}")
            self._render_group_with_date_split(f, "Download", sorted_dl, is_jp, "")
        
        # 2. Execute Phase (UserAssist, Amcache, Prefetch hits)
        if exec_events or prep_events:
            exec_combined = exec_events if exec_events else prep_events
            f.append(f"    Execute->>Discover: {'ä¸æ­£å®Ÿè¡Œ / ä¾µå®³é–‹å§‹' if is_jp else 'Malicious Process Started'}")
            self._render_group_with_date_split(f, "Execute", exec_combined, is_jp, "")
            
        # 3. Discover Phase (Recon, LotL)
        if recon_events:
            f.append(f"    Discover->>Cleanup: {'å†…éƒ¨åµå¯Ÿ / æƒ…å ±åé›†' if is_jp else 'Internal Recon & Enum'}")
            self._render_group_with_date_split(f, "Discover", recon_events, is_jp, "")

        # 4. Cleanup Phase (Anti-Forensics, Timestomp)
        if anti_events:
            # For cleanup, we want to emphasize the gap if any
            f.append(f"    Cleanup-->>Cleanup: {'âš ï¸ è¨¼æ‹ éš æ»…' if is_jp else 'Evidence Destruction'}")
            self._render_group_with_date_split(f, "Cleanup", anti_events, is_jp, "")

        f.append("```\n")
        return "\n".join(f).replace("VOID_VISUALIZATION", "-")

    def _render_group_with_date_split(self, f, target_participant, events, is_jp, action_label_base):
        """
        Helper to invoke render notes, splitting by date if multiple dates exist.
        """
        if not events: return

        # Sort by time first
        events = sorted(events, key=lambda x: str(x.get('Time', '')))
        
        # Group by Date (YYYY-MM-DD)
        from collections import defaultdict
        date_groups = defaultdict(list)
        for e in events:
            t = str(e.get('Time', ''))
            # Robust date extraction
            if 'T' in t: d = t.split('T')[0]
            elif len(t) >= 10: d = t[:10]
            else: d = "Unknown_Date"
            date_groups[d].append(e)
        
        sorted_dates = sorted(date_groups.keys())
        
        # Render each date group
        last_displayed_dt_obj = None
        
        for d in sorted_dates:
            group_evs = date_groups[d]
            curr_dt_obj = None
            try: curr_dt_obj = datetime.strptime(d, "%Y-%m-%d")
            except: pass

            # [P4] Filter: Only show high-score or critical events in Mermaid notes
            CRITICAL_TAGS = ["WEBSHELL", "LATERAL", "ANTI", "EXFIL", "C2", "TIMESTOMP"]
            high_priority_evs = [e for e in group_evs if 
                int(e.get('Score', 0) or 0) >= 500 or 
                any(ct in str(e.get('Tag', '')).upper() for ct in CRITICAL_TAGS)]
            
            note_content = None

            # Determine if we display detailed notes, summary, or nothing
            if high_priority_evs:
                 # [Final Polish] Aggressive Note Compression
                 DISPLAY_LIMIT = 3
                 display_evs = high_priority_evs[:DISPLAY_LIMIT]
                 remain_count = len(high_priority_evs) - DISPLAY_LIMIT
                 
                 summary_html = self._summarize_with_time_simple(display_evs, max_items=DISPLAY_LIMIT)
                 if remain_count > 0:
                      summary_html += f"<br/>(...and {remain_count} more)"
                 note_content = summary_html


            
            # If no content to display, skip this date entirely (and don't update last_displayed_dt_obj)
            if not note_content:
                continue

            # [Feature] Timeline Gap Visualization (Coalesced)
            # Only visualize gap if we are about to display a new note
            if curr_dt_obj and last_displayed_dt_obj:
                delta = (curr_dt_obj - last_displayed_dt_obj).days
                # [Final Polish 3] Only show gaps > 30 days to reduce noise
                if delta > 30:
                     f.append(f"    Note over Download,Cleanup: â³ ... {delta} Days Gap ...")
            
            # Render the note
            f.append(f"    Note right of {target_participant}: {note_content}")
            
            # Update last displayed tracker
            if curr_dt_obj:
                 last_displayed_dt_obj = curr_dt_obj

    def _summarize_with_time_simple(self, evs, max_items=2):
        if not evs: return ""
        lines = []
        for i, e in enumerate(evs[:max_items]):
            val_raw = str(e.get('Value', ''))
            val = val_raw # Default
            
            # [Display Decoupling] Mermaid specific truncation
            if "{" in val or "}" in val or "ScriptBlock" in val:
                 val = f"{val[:17]}.." if len(val) > 17 else val
            else:
                 val = val.split('\\')[-1]
                 if len(val) > 20: val = val[:18] + ".."
            
            time_str = str(e.get('Time') or '')
            if time_str == 'None': time_str = ''
            
            time_display = ""
            try:
                if 'T' in time_str:
                    dt_part = time_str.split('T')
                    date_part = dt_part[0].split('-') # YYYY-MM-DD
                    time_part = dt_part[1][:5] # HH:MM
                    time_display = f"{date_part[1]}/{date_part[2]} {time_part}"
                elif len(time_str) >= 16:
                    date_part = time_str[:10].split('-')
                    time_part = time_str[11:16]
                    time_display = f"{date_part[1]}/{date_part[2]} {time_part}"
                else:
                    time_display = time_str[:16]
            except:
                time_display = time_str

            source = ""
            note = str(e.get('Note', ''))
            tag = str(e.get('Tag', '')).upper()
            
            # [Fix] Critical: Robust check for 'system' labeling
            val_clean = val_raw.strip().lower()
            score_int = int(e.get('Score', 0) or 0)
            
            if "AUTH_FAILURE" in tag or "BRUTE_FORCE_DETECTED" in tag:
                val = "AUTH_FAILURE" 
                if "BRUTE_FORCE_DETECTED" in tag: val += " (Brute Force)"
            elif val_clean == "system":
                 # Robust check: if score is critical (300), force rewrite
                 if score_int >= 300:
                     val = "AUTH_FAILURE (EID:4625)"
                 elif "Logon Failure" in note or "4625" in note:
                     val = "AUTH_FAILURE"
            
            if 'UserAssist' in note: source = "[UA]"
            elif 'Amcache' in note: source = "[AC]"
            elif 'Prefetch' in note: source = "[PF]"
            elif 'Zone' in note: source = "[ZI]"
            
            line = f"{time_display} {val}{source}" if time_display else f"{val}{source}"
            lines.append(line)
        
        if len(evs) > max_items:
            lines.append(f"(+{len(evs) - max_items} more)")
        
        return "<br/>".join(lines)


    def _render_plutos_section_text(self, dfs, analyzer=None):
        f_mock = []
        class MockFile:
            def write(self, s): f_mock.append(s)
        self._write_plutos_section(MockFile(), dfs, analyzer)
        return "".join(f_mock)

    def _render_plutos_section_text_OLD_UNUSED(self, dfs):
        # Kept for reference if needed, but logic moved to _render_plutos_section_text
        pass

    def _write_plutos_section(self, f, dfs, analyzer=None):
        f.write("\n## ğŸŒ 5. é‡è¦ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŠã‚ˆã³æŒã¡å‡ºã—ç—•è·¡ (Critical Network & Exfiltration)\n")
        f.write("PlutosGateã‚¨ãƒ³ã‚¸ãƒ³ã«ã‚ˆã‚Šæ¤œå‡ºã•ã‚ŒãŸã€**ãƒ‡ãƒ¼ã‚¿ã®æŒã¡å‡ºã—**ã€**ãƒ¡ãƒ¼ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ä¸æ­£ã‚³ãƒ”ãƒ¼**ã€ãŠã‚ˆã³**é«˜ãƒªã‚¹ã‚¯ãªå¤–éƒ¨é€šä¿¡**ã®ç—•è·¡ã€‚\n\n")
        f.write("### ğŸš¨ 5.1 æ¤œå‡ºã•ã‚ŒãŸé‡å¤§ãªè„…å¨ (Critical Threats Detected)\n")
        critical_table = self._generate_critical_threats_table(dfs, analyzer)
        f.write(critical_table + "\n\n")
        net_map = self._generate_critical_network_map(dfs, analyzer)
        if net_map:
            f.write("### ğŸ—ºï¸ 5.2 ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç›¸é–¢å›³ (Critical Activity Map)\n")
            f.write(net_map + "\n\n")
            f.write("> **Note:** èµ¤è‰²ã¯å¤–éƒ¨ã¸ã®æŒã¡å‡ºã—ã‚„C2é€šä¿¡ã€ã‚ªãƒ¬ãƒ³ã‚¸è‰²ã¯å†…éƒ¨ã¸ã®æ¨ªå±•é–‹ã‚’ç¤ºå”†ã—ã¾ã™ã€‚\n\n")
        else:
            f.write("â€» è¦–è¦šåŒ–å¯èƒ½ãªãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒˆãƒãƒ­ã‚¸ãƒ¼ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚\n\n")
        f.write("---\n")

    def _generate_critical_threats_table(self, dfs, analyzer=None):
        rows = []
        
        # 1. Existing Plutos Logic (SRUM/Exfil) - Kept as is
        srum_df = dfs.get("Plutos_Srum")
        if srum_df is not None and srum_df.height > 0:
            try:
                if "Heat_Score" in srum_df.columns:
                    df = srum_df.filter(pl.col("Heat_Score").cast(pl.Int64, strict=False) >= 60)
                    for r in df.iter_rows(named=True):
                        ts = str(r.get("Timestamp", "")).split(".")[0]
                        proc = str(r.get("Process", "")).split("\\")[-1]
                        sent_mb = int(r.get("BytesSent", 0) or 0) // 1024 // 1024
                        rows.append({
                            "Time": ts, "Icon": "ğŸ“¤", "Verdict": f"**{r.get('Plutos_Verdict', 'HIGH_HEAT')}**",
                            "Details": f"Proc: {proc}<br>Sent: {sent_mb} MB", "Ref": "See: Plutos_Report_srum.csv"
                        })
            except: pass

        exfil_df = dfs.get("Plutos_Exfil")
        if exfil_df is not None and exfil_df.height > 0:
            try:
                for r in exfil_df.iter_rows(named=True):
                    ts = str(r.get("Timestamp", "")).split(".")[0]
                    fname = r.get("FileName", "Unknown")
                    url = str(r.get("URL", ""))[:30] + "..." if r.get("URL") else ""
                    rows.append({
                        "Time": ts, "Icon": "ğŸš¨", "Verdict": "**EXFIL_CORRELATION**",
                        "Details": f"File: **{fname}**<br>URL: {url}", "Ref": "See: Plutos_Report_exfil_correlation.csv"
                    })
            except: pass

        # 2. [v5.3] Inject Analyzer Findings (Lateral, Recon, Sensitive Doc)
        if analyzer and analyzer.visual_iocs:
             # [Fix] Reuse EXTREME_NOISE patterns for filtering this section too
             EXTREME_NOISE = [
                "_none_", "_10.0.", "_amd64_", "_x86_", "~31bf3856ad364e35~",
                "windows\\winsxs", "windowsapps\\", "infusedapps\\", 
                "8wekyb3d8bbwe", "deletedalluserpackages", "nativeimages_", 
                "\\assembly\\gac", "microsoftsolitaire", "mspaint_", "microsoftedge_8wekyb3d8bbwe",
                "microsoft.windows.cortana", "appmodel-runtime", "contentdeliverymanager", "devicesearchcache",
                # XAMPP Legitimate Components
                "xampp\\php\\pear", "xampp\\apache\\manual", "xampp\\tomcat\\webapps\\docs",
                "xampp\\tomcat\\webapps\\examples", "xampp\\src\\", "xampp\\licenses", "xampp\\locale",
                "xampp\\php\\docs", "xampp\\cgi-bin", "\\pear\\docs", "\\pear\\tests",
                # XAMPP Extended Noise
                "xampp\\perl\\lib", "xampp\\perl\\vendor", "filezillaftp\\source",
                "apache\\icons", "mercurymail\\", "mysql\\data\\",
                "phpmyadmin\\js\\", "phpmyadmin\\libraries\\", "phpmyadmin\\themes\\",
                "webalizer\\", "sendmail\\",
                # XAMPP Additional Noise (Tomcat, Static, Sessions)
                "xampp\\tmp\\sess_", "tomcat\\webapps\\manager", "tomcat\\webapps\\host-manager",
                "tomcat\\webapps\\root", "phpmyadmin\\doc\\", "htdocs\\img\\", "security\\htdocs\\",
                # XAMPP Dashboard & Docs
                "htdocs\\dashboard\\", "htdocs\\docs\\", "dashboard\\images\\", "dashboard\\css\\", "dashboard\\docs\\",
                # XAMPP Libraries & Extras
                "php\\extras\\", "php\\tests\\", "perl\\bin\\", "perl\\site\\",
                # XAMPP Apache/MySQL Config & Locales
                "apache\\include\\", "apache\\modules\\", "mysql\\share\\", "phpmyadmin\\locale\\",
                # XAMPP Test & Misc
                "webdav\\", "\\flags\\", "\\install\\", "phpids\\tests\\",
                # Browser Cache & DVWA Static Resources
                "content.ie5\\", "dvwa\\dvwa\\images\\", "dvwa\\dvwa\\css\\", "dvwa\\external\\",
                # XAMPP System Libraries (Loader noise)
                "apache\\bin\\iconv\\", "php\\ext\\", "tomcat\\lib\\",
                # MySQL Metadata (Running service artifacts)
                ".frm", ".myd", ".myi", "performance_schema\\",
                # XAMPP Icons & Static Assets
                "xampp\\img\\", "hackable\\users\\", "favicon.ico",
                # AGGRESSIVE NOISE FILTERS (Server process loader artifacts)
                "xampp\\apache\\bin\\", "xampp\\mysql\\bin\\", "xampp\\php\\", "xampp\\tomcat\\bin\\",
                # Library/Binary extensions
                ".dll", ".jar", ".so", ".chm", ".hlp", ".class",
                # Documentation noise
                "readme.txt", "license.txt", "install.txt", "changes.txt",
                # MySQL system tables
                "information_schema\\", "mysql\\mysql\\", "catalina\\"
             ]
             
             for ioc in analyzer.visual_iocs:
                 tag = str(ioc.get("Tag", "")).upper()
                 val = str(ioc.get("Value", ""))
                 val_lower = val.lower()
                 score = int(ioc.get("Score", 0) or 0)
                 
                 # [Fix] Skip EXTREME_NOISE in this section as well
                 if any(xp in val_lower for xp in EXTREME_NOISE):
                     continue
                 
                 # Criteria for Section 5.1 inclusion
                 is_target = False
                 icon = "â“"; verdict = "UNKNOWN"
                 
                 if "LATERAL" in tag or "UNC_" in tag:
                     is_target = True; icon = "ğŸ›"; verdict = "**LATERAL_MOVEMENT**"
                 elif "INTERNAL_RECON" in tag:
                     is_target = True; icon = "ğŸ”"; verdict = "**INTERNAL_RECON**"
                 elif "SENSITIVE" in tag:
                     is_target = True; icon = "ğŸ”"; verdict = "**SENSITIVE_ACCESS**"
                 elif "EXFIL" in tag or "DATA_EXFIL" in str(ioc.get("Type", "")):
                     is_target = True; icon = "ğŸ“¤"; verdict = "**DATA_EXFILTRATION**"
                 
                 if is_target:
                     t_str = str(ioc.get("Time", "")).replace("T", " ")[:19]
                     
                     # [P2'] IE Cache Aggregation (Skip individual addition, will handle bulk later)
                     # Identify scattered IE cache files by Tag or Path pattern
                     if "INTERNAL_RECON_WEB" in tag or "CONTENT.IE5" in val.upper():
                         # Store separately, do not add to 'rows' yet
                         if not hasattr(self, '_ie_cache_pool'): self._ie_cache_pool = []
                         self._ie_cache_pool.append(ioc)
                         continue

                     # [P4'] dsadd.exe Enrichment
                     if "dsadd" in val_lower:
                         val = f"{val} âš ï¸ **(AD User Manipulation Tool - PrivEsc Prep)**"
                     
                     rows.append({
                         "Time": t_str, "Icon": icon, "Verdict": verdict,
                         "Details": f"{val}", "Ref": "Timeline Analysis", "Tag": tag
                     })

        # [P2'] Inject Aggregated IE Cache Entry
        if hasattr(self, '_ie_cache_pool') and self._ie_cache_pool:
            ie_pool = getattr(self, '_ie_cache_pool')
            first_time = str(ie_pool[0].get("Time", "")).replace("T", " ")[:19]
            count = len(ie_pool)
            rows.append({
                "Time": first_time, "Icon": "ğŸŒ", "Verdict": "**SUSPICIOUS_WEB_CACHE**",
                "Details": f"ğŸ“¦ **Suspicious IE Cache Files x{count}** (Potential external resource loading)",
                "Ref": f"Aggregated {count} web artifacts"
            })

        if not rows: return self.txt.get('plutos_no_activity', "No suspicious network activity detected.\n")

        rows.sort(key=lambda x: x["Time"])
        
        # [v6.5] Timestamp Aggregation Algorithm
        # Group by second, compress INTERNAL_RECON noise, preserve critical keywords
        CRITICAL_KEYWORDS = ["password", "shell", "cmd", "whoami", "credentials", "webshell", "c99", "backdoor"]
        NOISE_EXTENSIONS = [".bat", ".xml", ".csv", ".csm", ".ibd", ".opt", ".php", ".pyd", ".manifest"]
        
        aggregated_rows = []
        i = 0
        while i < len(rows):
            current = rows[i]
            t_sec = current["Time"][:19]  # Group by second
            
            # Check if this is a critical item that should NOT be compressed
            is_critical = any(kw in current["Details"].lower() for kw in CRITICAL_KEYWORDS)
            
            if is_critical or current["Verdict"] not in ["**INTERNAL_RECON**"]:
                # Keep critical items and non-INTERNAL_RECON items as-is
                aggregated_rows.append(current)
                i += 1
                continue
            
            # For INTERNAL_RECON, check if we can aggregate with following entries
            group = [current]
            j = i + 1
            while j < len(rows):
                next_row = rows[j]
                next_t_sec = next_row["Time"][:19]
                
                # Stop if different timestamp or different verdict type
                if next_t_sec != t_sec or next_row["Verdict"] != current["Verdict"]:
                    break
                
                # Don't aggregate critical items
                if any(kw in next_row["Details"].lower() for kw in CRITICAL_KEYWORDS):
                    break
                
                group.append(next_row)
                j += 1
            
            if len(group) >= 3:
                # Compress into single row: "Noise x(count) (examples)"
                examples = [g["Details"].split("\\")[-1].split("/")[-1][:25] for g in group[:3]]
                compressed = f"ğŸ“¦ Noise x{len(group)} ({', '.join(examples)}...)"
                aggregated_rows.append({
                    "Time": t_sec, "Icon": "ğŸ”‡", "Verdict": "**COMPRESSED_NOISE**",
                    "Details": compressed, "Ref": f"Aggregated {len(group)} entries"
                })
                i = j  # Skip all grouped items
            else:
                # Not enough to compress, output individually
                for g in group:
                    aggregated_rows.append(g)
                i = j

        # [P0] Top 20 Limit - Show only most important entries
        MAX_DISPLAY = 20
        critical_rows = [r for r in aggregated_rows if any(kw in r.get("Details", "").lower() for kw in CRITICAL_KEYWORDS)]
        other_rows = [r for r in aggregated_rows if r not in critical_rows]
        
        # Prioritize critical, then by time
        display_rows = critical_rows[:10] + other_rows[:MAX_DISPLAY - len(critical_rows[:10])]
        remaining_count = len(aggregated_rows) - len(display_rows)

        md = "| Time / Period | Verdict | Summary | Reference |\n|---|---|---|---|\n"
        for row in display_rows:
            md += f"| {row['Time']} | {row['Icon']} {row['Verdict']} | {row['Details']} | {row['Ref']} |\n"
        
        if remaining_count > 0:
            md += f"\n> ğŸ“¦ **ãã®ä»– {remaining_count} ä»¶ã®æ¤œå‡ºã¯çœç•¥ã•ã‚Œã¾ã—ãŸã€‚** è©³ç´°ã¯ `IOC_Full.csv` ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n"
        
        return md
    
    def _generate_critical_network_map(self, dfs, analyzer=None):
        """Generate Mermaid network topology diagram from IOC data"""
        if not analyzer or not analyzer.visual_iocs:
            return ""
        
        # Extract network-related IOCs
        unc_shares = set()  # UNC paths -> (IP, share_name, files)
        remote_ips = set()
        remote_tools = []
        exfil_tools = []
        web_targets = []
        
        ip_pattern = re.compile(r'(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})')
        unc_pattern = re.compile(r'\\\\([\d\.]+|[a-zA-Z0-9_\-]+)\\([^\\]+)')
        
        for ioc in analyzer.visual_iocs:
            val = str(ioc.get("Value", ""))
            path = str(ioc.get("Path", ""))
            tag = str(ioc.get("Tag", "")).upper()
            ioc_type = str(ioc.get("Type", "")).upper()
            combined = val + " " + path
            
            # Extract UNC paths
            unc_match = unc_pattern.search(combined)
            if unc_match:
                ip_or_host = unc_match.group(1)
                share_name = unc_match.group(2)
                # Extract file info
                file_part = combined.split(share_name + "\\")[-1] if share_name in combined else ""
                unc_shares.add((ip_or_host, share_name, file_part[:30] if file_part else ""))
                remote_ips.add(ip_or_host)
            
            # Extract standalone IPs
            ip_match = ip_pattern.search(combined)
            if ip_match:
                ip_cand = ip_match.group(1)
                # [Case 6 Noise Fix] Filter out logic-level IPs (1.0.0.0) or version numbers
                if "127.0.0.1" not in ip_cand and "0.0.0.0" not in ip_cand:
                    try:
                        p1 = int(ip_cand.split('.')[0])
                        # Allow valid public DNS, otherwise ignore < 10 (likely version numbers)
                        if p1 >= 10 or ip_cand in ["1.1.1.1", "8.8.8.8", "8.8.4.4"]:
                             remote_ips.add(ip_cand)
                    except: pass
            
            # Categorize tools
            val_lower = val.lower()
            if "REMOTE_ACCESS" in ioc_type or "REMOTE_ACCESS" in tag:
                if "putty" in val_lower or "ssh" in val_lower or "plink" in val_lower:
                    remote_tools.append(val.split("\\")[-1].split("/")[-1])
            
            if "DATA_EXFIL" in ioc_type or "EXFIL" in tag:
                if any(x in val_lower for x in ["dd.exe", "ssh-add", "wget", "curl", "nc.exe", "netcat"]):
                    exfil_tools.append(val.split("\\")[-1].split("/")[-1])
            
            # Web reconnaissance
            if "xampp" in val_lower or "phpmyadmin" in val_lower or "http" in val_lower:
                web_targets.append(val[:30])
        
        # Need at least some network activity to generate map
        if not remote_ips and not unc_shares:
            return ""
        
        # Build Mermaid diagram
        lines = ["```mermaid", "graph LR"]
        
        # Host node with users
        primary_user = getattr(self, 'primary_user', 'User') or 'User'
        # Get joker user if exists
        users_str = primary_user
        for ioc in analyzer.visual_iocs:
            tag = str(ioc.get("Tag", "")).upper()
            if "NEW_USER_CREATION" in tag:
                new_user = str(ioc.get("Value", "")).split("\\")[-1].split("/")[-1]
                if new_user and new_user.lower() not in users_str.lower():
                    users_str = f"{primary_user}/{new_user}"
                    break
        
        lines.append(f'    A["{self.hostname}<br/>{users_str}"]')
        
        node_id = ord('B')
        
        # Add remote IPs/shares
        for ip in list(remote_ips)[:3]:  # Limit to 3 IPs
            node = chr(node_id)
            node_id += 1
            
            # Find share info for this IP
            share_info = ""
            files_on_share = []
            for unc_ip, share_name, file_part in unc_shares:
                if unc_ip == ip:
                    share_info = share_name
                    if file_part:
                        files_on_share.append(file_part)
            
            if share_info:
                lines.append(f'    {node}["{ip}<br/>{share_info}"]')
                lines.append(f'    A -->|SMB| {node}')
                
                # Add files from share
                if files_on_share:
                    file_node = chr(node_id)
                    node_id += 1
                    files_display = "<br/>".join(files_on_share[:3])
                    lines.append(f'    {file_node}["{files_display}"]')
                    lines.append(f'    {node} --> {file_node}')
            else:
                lines.append(f'    {node}["{ip}"]')
                lines.append(f'    A -->|Network| {node}')
        
        # Add remote access tools
        if remote_tools:
            tool_node = chr(node_id)
            node_id += 1
            unique_tools = list(set(remote_tools))[:3]
            tools_display = "<br/>".join(unique_tools)
            lines.append(f'    {tool_node}["ğŸ”§ Remote Tools<br/>{tools_display}"]')
            lines.append(f'    A -->|SSH/Remote| {tool_node}')
        
        # Add exfil tools
        if exfil_tools:
            exfil_node = chr(node_id)
            node_id += 1
            unique_exfil = list(set(exfil_tools))[:3]
            exfil_display = "<br/>".join(unique_exfil)
            lines.append(f'    {exfil_node}["âš ï¸ Exfil Prep<br/>{exfil_display}"]')
            lines.append(f'    A -->|Exfil Prep| {exfil_node}')
            # Style as warning
            lines.append(f'    style {exfil_node} fill:#f96')
        
        # Add web targets if detected
        if web_targets and list(remote_ips):
            # Link web activity to first IP
            first_ip_node = 'B'  # Assuming first IP is node B
            lines.append(f'    A -->|HTTP| {first_ip_node}')
        
        lines.append("```")
        
        return "\n".join(lines)

    def _extract_dual_run_count(self, ioc, dfs):
        ua_count = "N/A"; pf_count = "N/A"
        text_sources = [ioc.get("Value", ""), ioc.get("Summary", ""), ioc.get("Action", ""), ioc.get("Target_Path", "")]
        for text in text_sources:
            if not text: continue
            match = re.search(r"\(Run:\s*(\d+)\)", str(text), re.IGNORECASE)
            if match: ua_count = match.group(1); break
        return f"UA: {ua_count} | PF: {pf_count}"

    def export_pivot_config(self, pivot_seeds, path, primary_user):
        if not pivot_seeds: return
        
        # Group by category
        categorized = {}
        for seed in pivot_seeds:
            cat = seed.get("Category", "GENERAL")
            if cat not in categorized:
                categorized[cat] = []
            # Remove Category from individual entries to avoid redundancy
            entry = {k: v for k, v in seed.items() if k != "Category"}
            categorized[cat].append(entry)
        
        # Limit each category to top 10
        for cat in categorized:
            categorized[cat] = categorized[cat][:10]
        
        # Priority order for categories
        priority_order = ["CRITICAL_PHISHING", "CRITICAL_RECON", "CRITICAL_ANTI_FORENSICS", "CRITICAL_LATERAL", "GENERAL"]
        ordered_targets = {}
        for cat in priority_order:
            if cat in categorized and categorized[cat]:
                ordered_targets[cat] = categorized[cat]
        # Add any remaining categories
        for cat in categorized:
            if cat not in ordered_targets and categorized[cat]:
                ordered_targets[cat] = categorized[cat]
        
        config = {
            "Case_Context": {
                "Hostname": self.hostname,
                "Primary_User": primary_user,
                "Generated_At": datetime.now().isoformat()
            },
            "Deep_Dive_Targets": ordered_targets
        }
        try:
            with open(path, "w", encoding="utf-8") as f:
                json.dump(config, f, indent=2, ensure_ascii=False)
            print(f"    -> [Lachesis] Pivot Config generated: {path}")
        except Exception as e:
            print(f"    [!] Failed to export Pivot Config: {e}")

    def export_json_grimoire(self, analysis_result, analyzer, json_path, primary_user):
        serializable_events = []
        for ev in analysis_result.get("events", []):
            serializable_events.append({
                "Time": str(ev.get('Time')),
                "Category": ev.get('Category'),
                "Summary": ev.get('Summary'),
                "Source": ev.get('Source')
            })
        grimoire_data = {
            "Metadata": {
                "Host": self.hostname, 
                "Case": "Investigation", 
                "Primary_User": primary_user, 
                "Generated_At": datetime.now().isoformat()
            },
            "Verdict": {
                "Flags": list(analysis_result.get("verdict_flags", [])), 
                "Lateral_Summary": analysis_result.get("lateral_summary", "")
            },
            "Timeline": serializable_events,
            "IOCs": {"File": analyzer.visual_iocs, "Network": []}
        }
        try:
            with open(json_path, "w", encoding="utf-8") as f:
                json.dump(grimoire_data, f, indent=2, ensure_ascii=False)
            print(f"    -> [Chimera Ready] JSON Grimoire saved: {json_path}")
        except Exception as e:
            print(f"    [!] Failed to export JSON Grimoire: {e}")