import json
import re
import polars as pl
import os
from datetime import datetime, timedelta
from tools.lachesis.intel import TEXT_RES

class LachesisRenderer:
    def __init__(self, output_path, lang="jp"):
        self.output_path = output_path
        self.lang = lang if lang in TEXT_RES else "jp"
        self.txt = TEXT_RES[self.lang]
        self.hostname = "Unknown"
        self.headers = {
            "en": {
                "exec": "Executive Summary", 
                "origin": "Initial Access Vector", 
                "chain": "Critical Chain", 
                "tech": "High Confidence Findings", 
                "iocs": "Key Indicators"
            },
            "jp": {
                "exec": "„Ç®„Ç∞„Çº„ÇØ„ÉÜ„Ç£„Éñ„Éª„Çµ„Éû„É™„Éº", 
                "origin": "ÂàùÊúü‰æµÂÖ•ÁµåË∑ØÂàÜÊûê (Initial Access Vector)", 
                "chain": "Ë™øÊüª„Çø„Ç§„É†„É©„Ç§„É≥ (Critical Chain)", 
                "tech": "ÊäÄË°ìÁöÑË©≥Á¥∞ (High Confidence Findings)", 
                "iocs": "ÈáçË¶ÅÊåáÊ®ô (Key Indicators)"
            }
        }

    def render_report(self, analysis_data, analyzer, enricher, origin_stories, dfs_for_ioc, metadata):
        self.hostname = metadata.get("hostname", "Unknown")
        out_file = self.output_path
        
        with open(out_file, "w", encoding="utf-8") as f:
            self._write_header(f, metadata["os_info"], metadata["primary_user"], analysis_data["time_range"])
            self._write_toc(f)
            
            # 1. Executive Summary
            self._write_executive_summary_visual(f, analyzer, analysis_data["time_range"], metadata["primary_user"])
            
            # 2. Initial Access Vector
            self._write_initial_access_vector(f, analyzer.pivot_seeds, origin_stories)
            
            # 3. Timeline
            self._write_timeline_visual(f, analysis_data["phases"], analyzer, enricher)
            
            # 4. Technical Findings (Pass origin_stories for LNK enrichment)
            self._write_technical_findings(f, analyzer, dfs_for_ioc, origin_stories) 
            
            # 5. Detection Statistics
            self._write_detection_statistics(f, analysis_data["medium_events"], analyzer, dfs_for_ioc)

            # 6. Conclusions & Recommendations
            self._write_recommendations(f, analyzer)
            
            # 7. Appendix (IOCs)
            self._write_ioc_appendix_unified(f, analyzer) 
            
            f.write(f"\n---\n*Report woven by SkiaHelios (The Triad v5.2 Perfection)* ü¶Å")
        
        print(f"[*] Lachesis v5.2 is weaving the Grimoire into {out_file}...")

    def _write_header(self, f, os_info, primary_user, time_range):
        t = self.txt
        f.write(f"# {t['title']} - {self.hostname}\n\n")
        f.write(f"### üõ°Ô∏è {t['coc_header']}\n")
        f.write("| Item | Details |\n|---|---|\n")
        f.write(f"| **Target Host** | **{self.hostname}** |\n")
        f.write(f"| **OS Info** | {os_info} |\n") 
        f.write(f"| **Primary User** | {primary_user} |\n")
        f.write(f"| **Incident Scope** | **{time_range}** |\n") 
        f.write(f"| **Report Date** | {datetime.now().strftime('%Y-%m-%d')} |\n\n---\n\n")

    def _write_toc(self, f):
        t = self.txt
        f.write("## üìö Table of Contents\n")
        f.write(f"- [{t['h1_exec']}](#{self._make_anchor(t['h1_exec'])})\n")
        f.write(f"- [{t['h1_origin']}](#{self._make_anchor(t['h1_origin'])})\n")
        f.write(f"- [{t['h1_time']}](#{self._make_anchor(t['h1_time'])})\n")
        f.write(f"- [{t['h1_tech']}](#{self._make_anchor(t['h1_tech'])})\n")
        f.write(f"- [{t['h1_stats']}](#{self._make_anchor(t['h1_stats'])})\n")
        f.write(f"- [{t['h1_rec']}](#{self._make_anchor(t['h1_rec'])})\n")
        f.write(f"- [{t['h1_app']}](#{self._make_anchor(t['h1_app'])})\n")
        f.write(f"- [Pivot Config (Deep Dive Targets)](#deep-dive-recommendation)\n")
        f.write("\n---\n\n")

    def _make_anchor(self, text):
        return text.lower().replace(" ", "-").replace(".", "").replace("&", "").replace("(", "").replace(")", "").replace("/", "")

    def _is_visual_noise(self, name):
        name = str(name).strip()
        if len(name) < 3: return True
        return False

    def _write_executive_summary_visual(self, f, analyzer, time_range, primary_user):
        t = self.txt
        f.write(f"## {t['h1_exec']}\n")
        
        visual_iocs = analyzer.visual_iocs
        has_time_change = any("SYSTEM_TIME" in str(ioc.get('Tag', '')) or "4616" in str(ioc.get('Value', '')) for ioc in visual_iocs)
        has_paradox = any("TIME_PARADOX" in str(ioc.get('Type', '')) for ioc in visual_iocs) or has_time_change
        has_masquerade = any("MASQUERADE" in str(ioc.get('Type', '')) for ioc in visual_iocs)
        has_phishing = any("PHISHING" in str(ioc.get('Type', '')) for ioc in visual_iocs)
        has_timestomp = any("TIMESTOMP" in str(ioc.get('Type', '')) for ioc in visual_iocs)
        has_anti = any("ANTI_FORENSICS" in str(ioc.get('Type', '')) for ioc in visual_iocs)
        
        conclusion = ""
        if has_paradox:
            conclusion = (
                f"**ÁµêË´ñ:**\n{time_range} „ÅÆÊúüÈñì„Å´„Åä„ÅÑ„Å¶„ÄÅÁ´ØÊú´ {self.hostname} „Å´ÂØæ„Åô„Çã **È´òÂ∫¶„Å™Èö†ËîΩÂ∑•‰Ωú„Çí‰º¥„ÅÜÈáçÂ§ß„Å™‰æµÂÆ≥Ê¥ªÂãï** „ÇíÁ¢∫Ë™ç„Åó„Åæ„Åó„Åü„ÄÇ\n\n"
                f"‚ö†Ô∏èüö® **SYSTEM TIME MANIPULATION DETECTED** üö®‚ö†Ô∏è\n"
                f"**„Ç∑„Çπ„ÉÜ„É†ÊôÇÂàª„ÅÆÂ∑ª„ÅçÊàª„ÅóÔºàTime ParadoxÔºâ** „ÅåÊ§úÁü•„Åï„Çå„Åæ„Åó„Åü„ÄÇÊîªÊíÉËÄÖ„ÅØÊôÇÂàª„ÇíÊìç‰Ωú„Åô„Çã„Åì„Å®„Åß„Éï„Ç©„É¨„É≥„Ç∏„ÉÉ„ÇØË™øÊüª„ÇíÂ¶®ÂÆ≥„Åó„ÄÅ"
                f"„É≠„Ç∞„ÅÆ„Çø„Ç§„É†„É©„Ç§„É≥„ÇíÊÑèÂõ≥ÁöÑ„Å´Á†¥Â£ä„Åó„Çà„ÅÜ„Å®„Åó„ÅüÁóïË∑°„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ„Çø„Ç§„É†„É©„Ç§„É≥ÂàÜÊûê„Å´„ÅØÊ•µ„ÇÅ„Å¶ÊÖéÈáç„Å™Á≤æÊüª„ÅåÂøÖË¶Å„Åß„Åô„ÄÇ\n"
            )
        elif has_masquerade or has_anti:
            conclusion = f"**ÁµêË´ñ:**\n{time_range} „ÅÆÊúüÈñì„Å´„Åä„ÅÑ„Å¶„ÄÅÁ´ØÊú´ {self.hostname} „Å´ÂØæ„Åô„Çã **Ë®ºÊã†Èö†ÊªÖ„ÉªÂÅΩË£Ö„Çí‰º¥„ÅÜÈáçÂ§ß„Å™‰æµÂÆ≥Ê¥ªÂãï** „ÇíÁ¢∫Ë™ç„Åó„Åæ„Åó„Åü„ÄÇ\n"
        elif visual_iocs:
            conclusion = f"**ÁµêË´ñ:**\n{time_range} „ÅÆÊúüÈñì„Å´„Åä„ÅÑ„Å¶„ÄÅÁ´ØÊú´ {self.hostname} „Å´ÂØæ„Åô„Çã **CRITICAL „É¨„Éô„É´„ÅÆ‰æµÂÆ≥Ê¥ªÂãï** „ÇíÁ¢∫Ë™ç„Åó„Åæ„Åó„Åü„ÄÇ\n"
        else:
            conclusion = f"**ÁµêË´ñ:**\nÊú¨Ë™øÊüªÁØÑÂõ≤„Å´„Åä„ÅÑ„Å¶„ÄÅÈáçÂ§ß„Å™„Ç§„É≥„Ç∑„Éá„É≥„Éà„ÅÆÁóïË∑°„ÅØÊ§úÂá∫„Åï„Çå„Åæ„Åõ„Çì„Åß„Åó„Åü„ÄÇ\n"
        
        f.write(conclusion)
        
        attack_methods = []
        if has_phishing: attack_methods.append("„Éï„Ç£„ÉÉ„Ç∑„É≥„Ç∞ÔºàLNKÔºâ„Å´„Çà„ÇãÂàùÊúü‰æµÂÖ•")
        if has_masquerade: attack_methods.append("ÂÅΩË£Ö„Éï„Ç°„Ç§„É´Ë®≠ÁΩÆÔºàMasqueradingÔºâ")
        if has_timestomp: attack_methods.append("„Çø„Ç§„É†„Çπ„Çø„É≥„ÉóÂÅΩË£ÖÔºàTimestompÔºâ")
        if has_paradox: attack_methods.append("**„Ç∑„Çπ„ÉÜ„É†ÊôÇÈñìÂ∑ª„ÅçÊàª„ÅóÔºàSystem RollbackÔºâ**")
        if has_anti: attack_methods.append("ÁóïË∑°„ÉØ„Ç§„Éî„É≥„Ç∞ÔºàAnti-ForensicsÔºâ")
        
        if not attack_methods: attack_methods = ["‰∏çÂØ©„Å™„Ç¢„ÇØ„ÉÜ„Ç£„Éì„ÉÜ„Ç£"]
            
        f.write(f"**‰∏ª„Å™ÊîªÊíÉÊâãÂè£:** {', '.join(attack_methods)}„ÄÇ\n\n")
        f.write("> **Deep Dive Êé®Â•®:** Ë©≥Á¥∞„Å™Ë™øÊüª„ÇíË°å„ÅÜÈöõ„ÅØ„ÄÅÊ∑ª‰ªò„ÅÆ `Pivot_Config.json` „Å´Ë®òËºâ„Åï„Çå„Åü **CRITICAL_PHISHING** „Çø„Éº„Ç≤„ÉÉ„ÉàÁæ§„Åã„ÇâÈñãÂßã„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇÁâπ„Å´„Ç§„Éô„É≥„Éà„É≠„Ç∞ÔºàID 4688Ôºâ„Åã„Çâ„ÅÆ„Ç≥„Éû„É≥„Éâ„É©„Ç§„É≥Âæ©ÂÖÉ„ÅåÊúÄÂÑ™ÂÖà‰∫ãÈ†Ö„Åß„Åô„ÄÇ\n\n")
        
        f.write(self._render_mermaid_vertical_clustered(visual_iocs))
        f.write(self._render_key_indicators(visual_iocs))
        f.write("\n")

    def _write_initial_access_vector(self, f, pivot_seeds, origin_stories):
        t = self.txt
        f.write(f"## {t['h1_origin']}\n")
        phishing_lnks = [s for s in pivot_seeds if "PHISHING" in s.get("Reason", "")]
        drop_items = [s for s in pivot_seeds if "DROP" in s.get("Reason", "") and "PHISHING" not in s.get("Reason", "")]
        
        if phishing_lnks:
            f.write("**„Éï„Ç£„ÉÉ„Ç∑„É≥„Ç∞„Å´„Çà„ÇãÂàùÊúü‰æµÂÖ•„ÅåÈ´òÁ¢∫Â∫¶„ÅßÁ¢∫Ë™ç„Åï„Çå„Åæ„Åó„Åü„ÄÇ**\n")
            f.write(f"- Recent„Éï„Ç©„É´„ÉÄÁ≠â„Å´„Åä„ÅÑ„Å¶„ÄÅ**{len(phishing_lnks)}‰ª∂** „ÅÆ‰∏çÂØ©„Å™LNK„Éï„Ç°„Ç§„É´Ôºà„Ç∑„Éß„Éº„Éà„Ç´„ÉÉ„ÉàÔºâ„Å∏„ÅÆ„Ç¢„ÇØ„Çª„Çπ„ÅåÊ§úÁü•„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n")
            f.write("\n| „Çµ„É≥„Éó„É´LNK | „Ç¢„ÇØ„Çª„ÇπÊôÇÂàª (UTC) | ÊµÅÂÖ•ÂÖÉ (Origin Trace) |\n|---|---|---|\n")
            for seed in phishing_lnks[:10]:
                self._write_origin_row(f, seed, origin_stories)
            f.write("\n")

        if drop_items:
            f.write("**‰∏çÂØ©„Å™„ÉÑ„Éº„É´„Éª„Éï„Ç°„Ç§„É´„ÅÆÊåÅ„Å°Ëæº„ÅøÔºàDropped ArtifactsÔºâ:**\n")
            f.write("\n| „Éï„Ç°„Ç§„É´Âêç | Áô∫Ë¶ãÂ†¥ÊâÄ | ÊµÅÂÖ•ÂÖÉ (Origin Trace) |\n|---|---|---|\n")
            for seed in drop_items[:10]:
                self._write_origin_row(f, seed, origin_stories)
            f.write("\n")

        if not phishing_lnks and not drop_items:
            f.write("ÊòéÁ¢∫„Å™Â§ñÈÉ®‰æµÂÖ•„Éô„ÇØ„Çø„Éº„ÅØËá™ÂãïÊ§úÁü•„Åï„Çå„Åæ„Åõ„Çì„Åß„Åó„Åü„ÄÇ\n\n")

    def _write_origin_row(self, f, seed, origin_stories):
        name = seed['Target_File']
        time = str(seed.get('Timestamp_Hint', '')).replace('T', ' ')[:19]
        origin_desc = "‚ùì No Trace Found (Low Confidence)"
        story = next((s for s in origin_stories if s["Target"] == name), None)
        
        if story:
            ev = story["Evidence"][0]
            url = ev.get("URL", "")
            url_display = (url[:50] + "...") if len(url) > 50 else url
            gap = ev.get('Time_Gap', '-')
            conf = story.get("Confidence", "LOW")
            reason = story.get("Reason", "")
            
            icon = "‚úÖ" if conf == "HIGH" else "‚ö†Ô∏è" if conf == "MEDIUM" else "‚ùì"
            prefix = "**Confirmed**" if conf == "HIGH" else "Inferred" if conf == "MEDIUM" else "Weak"
            origin_desc = f"{icon} **{prefix}**: {reason}<br/>üîó `{url_display}`<br/>*(Gap: {gap})*"
        
        col2 = time if time else f"`{seed.get('Target_Path', '')[:20]}`"
        f.write(f"| `{name}` | {col2} | {origin_desc} |\n")

    def _render_mermaid_vertical_clustered(self, events):
        if not events: return "\n(No critical events found for visualization)\n"
        
        f = ["\n### üèπ Attack Flow Visualization (Timeline)\n"]
        f.append("```mermaid")
        f.append("graph TD")
        
        f.append("    classDef init fill:#e63946,stroke:#333,stroke-width:2px,color:white;")
        f.append("    classDef exec fill:#f4a261,stroke:#333,stroke-width:2px,color:black;")
        f.append("    classDef persist fill:#2a9d8f,stroke:#333,stroke-width:2px,color:white;")
        f.append("    classDef anti fill:#264653,stroke:#333,stroke-width:2px,color:white;")
        f.append("    classDef time fill:#a8dadc,stroke:#457b9d,stroke-width:4px,color:black;")
        f.append("    classDef phishing fill:#ff6b6b,stroke:#c92a2a,stroke-width:2px,color:white;")
        
        critical_events = [ev for ev in events if ev.get('Score', 0) >= 60 or "CRITICAL" in str(ev.get('Type', ''))]
        sorted_events = sorted(critical_events, key=lambda x: x.get('Time', '9999'))
        
        if not sorted_events: return "\n(No critical events found)\n"

        has_paradox = any("TIME_PARADOX" in str(ev.get('Type', '')) for ev in events)
        if has_paradox:
            f.append("    subgraph T_PRE [\"‚ö†Ô∏è TIME MANIPULATION\"]")
            f.append("        N_TP[\"‚è™ <b>SYSTEM ROLLBACK DETECTED</b><br/>Time Paradox Anomaly\"]:::time")
            f.append("    end")

        subgraphs = []
        current_subgraph = {"nodes": [], "start_time": None, "end_time": None}
        
        def parse_dt(t_str):
            try: return datetime.fromisoformat(str(t_str).replace("Z", ""))
            except: return datetime.min

        last_dt = None
        node_id_counter = 0
        burst_buffer = [] 
        
        def flush_burst_buffer(buffer, target_list, counter):
            if not buffer: return counter
            first_ev = buffer[0]
            cat = self._get_event_category(first_ev)
            
            if len(buffer) >= 3 and ("INITIAL" in cat or "EXECUTION" in cat):
                node_id = f"N{counter}"
                counter += 1
                start_t = str(buffer[0].get('Time', ''))[11:16]
                count = len(buffer)
                icon = "‚ö°"
                if "INITIAL" in cat: icon = "üé£"
                elif "EXEC" in cat: icon = "‚öôÔ∏è"
                
                short_summary = self._get_short_summary(first_ev)
                label = f"{start_t} {icon} {count}x Events<br/>({short_summary} etc.)"
                style = ":::exec"
                if "INITIAL" in cat: style = ":::phishing"
                target_list.append(f"{node_id}[\"{label}\"]{style}")
                return counter
            else:
                for ev in buffer:
                    node_id = f"N{counter}"
                    counter += 1
                    t_str = str(ev.get('Time', ''))[11:16]
                    s_sum = self._get_short_summary(ev)
                    ev_cat = self._get_event_category(ev)
                    icon = "üîπ"
                    style = ":::default"
                    if "SYSTEM" in ev_cat: icon = "‚è∞"; style = ":::time"
                    elif "ANTI" in ev_cat: icon = "üóëÔ∏è"; style = ":::anti"
                    elif "PERSIST" in ev_cat: icon = "‚öì"; style = ":::persist"
                    elif "INITIAL" in ev_cat: icon = "üé£"; style = ":::init"
                    elif "PHISH" in ev_cat: icon = "üé£"; style = ":::phishing"
                    
                    label = f"{t_str} {icon} {s_sum}"
                    target_list.append(f"{node_id}[\"{label}\"]{style}")
                return counter

        for ev in sorted_events:
            if self._is_visual_noise(ev.get("Value", "")): continue
            dt = parse_dt(ev.get('Time', ''))
            
            if last_dt and (dt - last_dt).total_seconds() > 3600:
                node_id_counter = flush_burst_buffer(burst_buffer, current_subgraph["nodes"], node_id_counter)
                burst_buffer = []
                subgraphs.append(current_subgraph)
                current_subgraph = {"nodes": [], "start_time": dt, "end_time": dt}
            
            if current_subgraph["start_time"] is None: current_subgraph["start_time"] = dt
            current_subgraph["end_time"] = dt
            last_dt = dt
            
            if not burst_buffer:
                burst_buffer.append(ev)
            else:
                last_in_buff = burst_buffer[-1]
                last_buff_dt = parse_dt(last_in_buff.get('Time', ''))
                same_cat = self._get_event_category(ev) == self._get_event_category(last_in_buff)
                close_time = (dt - last_buff_dt).total_seconds() < 120 
                
                if same_cat and close_time:
                    burst_buffer.append(ev)
                else:
                    node_id_counter = flush_burst_buffer(burst_buffer, current_subgraph["nodes"], node_id_counter)
                    burst_buffer = [ev]

        node_id_counter = flush_burst_buffer(burst_buffer, current_subgraph["nodes"], node_id_counter)
        subgraphs.append(current_subgraph)

        sg_counter = 0
        prev_sg_id = None

        if has_paradox:
            prev_sg_id = "T_PRE"
        
        for sg in subgraphs:
            if not sg["nodes"]: continue
            sg_id = f"T{sg_counter}"
            start_s = sg["start_time"].strftime("%H:%M")
            end_s = sg["end_time"].strftime("%H:%M")
            label = f"‚è∞ {start_s} - {end_s}"
            
            f.append(f"    subgraph {sg_id} [\"{label}\"]")
            for n in sg["nodes"]: f.append(f"        {n}")
            f.append("    end")
            
            if prev_sg_id:
                if prev_sg_id == "T_PRE":
                     f.append(f"    N_TP --> {sg['nodes'][0].split('[')[0]}")
                else:
                     f.append(f"    {prev_sg_id} --> {sg_id}")
            prev_sg_id = sg_id
            sg_counter += 1

        f.append("```\n")
        return "\n".join(f)

    def _get_event_category(self, ev):
        typ = str(ev.get('Type', '')).upper()
        tag = str(ev.get('Tag', '')).upper()
        if "SYSTEM_TIME" in tag or "TIME_CHANGE" in tag or "4616" in tag or "ROLLBACK" in tag: return "SYSTEM MANIPULATION"
        if "PHISH" in typ or "LNK" in typ: return "INITIAL ACCESS"
        if "WIPE" in typ or "ANTI" in typ: return "ANTI-FORENSICS"
        if "PERSIST" in typ: return "PERSISTENCE"
        if "EXEC" in typ or "RUN" in typ: return "EXECUTION"
        if "TIMESTOMP" in typ: return "TIMESTOMP (FILE)"
        return "OTHER ACTIVITY"

    def _get_short_summary(self, ev):
        val = ev.get('Value', '')
        if not val or val == "Unknown":
            val = ev.get('Summary', '')
            if not val: val = str(ev.get('Tag', 'Event'))
        if "SYSTEM_TIME" in str(ev.get('Tag', '')) or "4616" in str(val): return "System Time Changed"
        if "\\" in val or "/" in val: val = os.path.basename(val.replace("\\", "/"))
        return val[:15] + ".." if len(val) > 15 else val

    def _render_key_indicators(self, events):
        output = ["\n### üíé Key Indicators (Critical Only)\n"]
        grouped = {}
        for ev in events:
            if ev.get('Score', 0) < 50 and "CRITICAL" not in str(ev.get('Type', '')): continue
            cat = self._get_event_category(ev)
            if cat not in grouped: grouped[cat] = []
            grouped[cat].append(ev)

        cat_titles = {
            "INITIAL ACCESS": "üé£ Initial Access", "ANTI-FORENSICS": "üôà Anti-Forensics",
            "SYSTEM MANIPULATION": "üö® System Time Manipulation", "PERSISTENCE": "‚öì Persistence",
            "EXECUTION": "‚ö° Execution", "TIMESTOMP (FILE)": "üïí Timestomp (Files)"
        }
        keys = sorted(grouped.keys(), key=lambda k: 0 if "SYSTEM" in k else 1)

        for cat in keys:
            items = grouped[cat]
            output.append(f"#### {cat_titles.get(cat, cat)}")
            output.append("| Time (UTC) | Value / Artifact | Impact/Target | Score |")
            output.append("|---|---|---|---|")
            items.sort(key=lambda x: x.get('Time', '9999'))
            for ioc in items:
                t = str(ioc.get('Time', 'N/A')).replace('T', ' ')[:19]
                val = ioc.get('Value', '-')
                if not val or val == "Unknown": val = ioc.get('Summary', '-')
                score = ioc.get('Score', 0)
                impact = "-"
                extra = ioc.get('Extra', {})
                tag = str(ioc.get('Tag', ''))
                if "SYSTEM_TIME" in tag or "4616" in tag or "TIME_PARADOX" in str(ioc.get('Type', '')):
                    impact = "**System Clock Altered**"
                elif cat == "INITIAL ACCESS":
                    tgt = extra.get('Target_Path', 'Unknown')
                    if tgt and tgt != "Unknown":
                        impact = f"Target: {tgt[:30]}..."
                output.append(f"| {t} | `{val}` | {impact} | {score} |")
            output.append("\n")
        return "\n".join(output)

    def _write_technical_findings(self, f, analyzer, dfs, origin_stories):
        t = self.txt
        f.write(f"## {t['h1_tech']}\n")
        high_conf_events = [ioc for ioc in analyzer.visual_iocs if analyzer.is_force_include_ioc(ioc) or "ANTI" in str(ioc.get("Type", ""))]
        self._write_anti_forensics_section(f, high_conf_events, dfs)
        f.write("### üîç Detailed Findings by Category\n\n")
        
        groups = {}
        for ioc in high_conf_events:
            cat = self._get_event_category(ioc)
            if "ANTI" in cat: continue
            if cat not in groups: groups[cat] = []
            groups[cat].append(ioc)
            
        for cat, items in groups.items():
            f.write(f"#### {cat}\n")
            
            # [Fix Issue #3] Special handling for Initial Access LNKs
            if "INITIAL ACCESS" in cat:
                self._render_grouped_lnk_findings(f, items, origin_stories, analyzer)
            else:
                items.sort(key=lambda x: x.get('Time', '9999'))
                for ioc in items:
                    dt = str(ioc.get('Time', '')).replace('T', ' ')[:19]
                    val = ioc.get('Value', '') or ioc.get('Summary', '')
                    f.write(f"- **{dt}** | `{val}`\n")
                    insight = analyzer.generate_ioc_insight(ioc)
                    if insight: f.write(f"  - üïµÔ∏è **Analyst Note:** {insight}\n")
            f.write("\n")

    def _render_grouped_lnk_findings(self, f, items, origin_stories, analyzer):
        """Helper to render LNK findings with grouping to avoid repetition"""
        high_interest = []
        generic_lnks = []
        
        for ioc in items:
            name = ioc.get("Value", "")
            is_special = False
            
            # Check for Origin Story (Confirmed Download)
            story = next((s for s in origin_stories if s["Target"] == name), None) if origin_stories else None
            if story and story.get("Confidence") == "HIGH": is_special = True
            
            # Check for DEFCON/Masquerade
            if "DEFCON" in name.upper() or "MASQUERADE" in str(ioc.get("Extra", {}).get("Risk", "")): is_special = True
            
            if is_special: high_interest.append(ioc)
            else: generic_lnks.append(ioc)
            
        # Render High Interest Items
        if high_interest:
            f.write("**ÁâπË®ò‰∫ãÈ†Ö (High Interest Artifacts):**\n")
            high_interest.sort(key=lambda x: x.get('Time', '9999'))
            for ioc in high_interest:
                dt = str(ioc.get('Time', '')).replace('T', ' ')[:19]
                val = ioc.get('Value', '')
                f.write(f"- **{dt}** | `{val}`\n")
                insight = analyzer.generate_ioc_insight(ioc)
                
                # Append Origin Info if available
                story = next((s for s in origin_stories if s["Target"] == val), None) if origin_stories else None
                if story and story.get("Confidence") == "HIGH":
                     gap = story['Evidence'][0].get('Time_Gap', '-')
                     insight = f"‚úÖ **Web Download Confirmed** (Gap: {gap})<br/>" + (insight if insight else "")
                
                if insight: f.write(f"  - üïµÔ∏è **Analyst Note:** {insight}\n")

        # Render Generic Items Summary
        if generic_lnks:
            f.write(f"\n**„Åù„ÅÆ‰ªñ„ÅÆLNK ({len(generic_lnks)}‰ª∂):**\n")
            f.write("ÁîªÂÉè„Éï„Ç°„Ç§„É´Âêç„ÇíË£Ö„Å£„Åü„Ç∑„Éß„Éº„Éà„Ç´„ÉÉ„ÉàÁæ§„Åß„Åô„ÄÇTarget_PathÊÉÖÂ†±„ÅØ„ÉØ„Ç§„Éî„É≥„Ç∞„Å´„Çà„ÇäÊ¨†ËêΩ„Åó„Å¶„ÅÑ„Åæ„Åô„Åå„ÄÅ‰ΩúÊàê„Éë„Çø„Éº„É≥„Åã„Çâ„Éï„Ç£„ÉÉ„Ç∑„É≥„Ç∞Áî±Êù•„Å®Êñ≠ÂÆö„Åï„Çå„Åæ„Åô„ÄÇ\n")
            generic_lnks.sort(key=lambda x: x.get('Time', '9999'))
            for ioc in generic_lnks:
                dt = str(ioc.get('Time', '')).replace('T', ' ')[:19]
                val = ioc.get('Value', '')
                f.write(f"- {dt} | `{val}`\n")

    def _write_anti_forensics_section(self, f, ioc_list, dfs):
        af_tools = [ioc for ioc in ioc_list if "ANTI" in str(ioc.get("Type", "")) or "WIPE" in str(ioc.get("Type", ""))]
        if not af_tools: return
        f.write("### üö® Anti-Forensics Activities (Evidence Destruction)\n\n")
        f.write("‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è **ÈáçÂ§ß„Å™Ë®ºÊã†Èö†ÊªÖÊ¥ªÂãï„ÇíÊ§úÂá∫** ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è\n\n")
        f.write("ÊîªÊíÉËÄÖ„ÅØ‰æµÂÖ•Âæå„ÄÅ‰ª•‰∏ã„ÅÆ„ÉÑ„Éº„É´„Çí‰ΩøÁî®„Åó„Å¶Ê¥ªÂãïÁóïË∑°„ÇíÊÑèÂõ≥ÁöÑ„Å´ÊäπÊ∂à„Åó„Å¶„ÅÑ„Åæ„ÅôÔºö\n\n")
        seen_tools = set()
        for tool in af_tools:
            name = tool.get("Value", "Unknown").upper()
            if name in seen_tools: continue
            seen_tools.add(name)
            run_count = self._extract_dual_run_count(tool, dfs)
            last_run = tool.get("Time", "Unknown").replace("T", " ")[:19]
            desc = "„Éá„Éº„ÇøÊäπÊ∂à„ÉÑ„Éº„É´"
            if "BCWIPE" in name: desc = "Ëªç‰∫ã„É¨„Éô„É´„ÅÆ„Éï„Ç°„Ç§„É´„ÉØ„Ç§„Éî„É≥„Ç∞„ÉÑ„Éº„É´„ÄÇÈÄöÂ∏∏„ÅÆÂæ©ÂÖÉ„Çí‰∏çÂèØËÉΩ„Å´„Åó„Åæ„Åô„ÄÇ"
            elif "CCLEANER" in name: desc = "„Ç∑„Çπ„ÉÜ„É†„ÇØ„É™„Éº„Éä„Éº„ÄÇ„Éñ„É©„Ç¶„Ç∂Â±•Ê≠¥„ÇÑMRU„ÅÆÂâäÈô§„Å´‰ΩøÁî®„Åï„Çå„Åæ„Åô„ÄÇ"
            f.write(f"#### {name}\n")
            f.write(f"- üìä **Run Count**: **{run_count}**\n")
            f.write(f"- üïê **Last Execution**: {last_run} (UTC)\n")
            f.write(f"- ‚ö†Ô∏è **Severity**: CRITICAL\n")
            f.write(f"- üîç **Description**: {desc}\n\n")
            f.write(f"üïµÔ∏è **Analyst Note**:\n")
            if "BCWIPE" in name:
                 f.write("„Åì„ÅÆ„ÉÑ„Éº„É´„ÅÆÂÆüË°å„Å´„Çà„Çä„ÄÅLNK„Éï„Ç°„Ç§„É´„ÄÅPrefetch„ÄÅ‰∏ÄÊôÇ„Éï„Ç°„Ç§„É´Á≠â„ÅÆË®ºÊã†„ÅåÁâ©ÁêÜÁöÑ„Å´‰∏äÊõ∏„ÅçÂâäÈô§„Åï„Çå„ÅüÂèØËÉΩÊÄß„ÅåÊ•µ„ÇÅ„Å¶È´ò„ÅÑ„Åß„Åô„ÄÇ\n")
            else:
                 f.write("ÊîªÊíÉÊ¥ªÂãïÁµÇ‰∫ÜÂæå„ÅÆÁóïË∑°ÂâäÈô§ÔºàCleanupÔºâ„Å´‰ΩøÁî®„Åï„Çå„Åü„Å®Êé®ÂÆö„Åï„Çå„Åæ„Åô„ÄÇ\n")
            f.write("\n---\n\n")
        f.write("### üìâ Missing Evidence Impact Assessment\n\n")
        f.write("‰ª•‰∏ã„ÅÆË®ºÊã†„Åå„ÄÅAnti-Forensics„ÉÑ„Éº„É´„Å´„Çà„Å£„Å¶Â§±„Çè„Çå„Åü„Å®Âà§Êñ≠„Åï„Çå„Åæ„ÅôÔºö\n\n")
        f.write("| Ë®ºÊã†„Ç´„ÉÜ„Ç¥„É™ | ÊúüÂæÖ„Åï„Çå„ÇãÊÉÖÂ†± | ÁèæÁä∂ | Êé®ÂÆöÂéüÂõ† |\n|---|---|---|---|\n")
        f.write("| LNK Target Paths | `cmd.exe ...` Á≠â„ÅÆÂºïÊï∞ | ‚ùå Ê¨†ËêΩ | BCWipe/SDelete„Å´„Çà„ÇãÂâäÈô§ |\n")
        f.write("| Prefetch (Tools) | ÂÆüË°åÂõûÊï∞„Éª„Çø„Ç§„É†„Çπ„Çø„É≥„Éó | ‚ùå Ê¨†ËêΩ | CCleaner/BCWipe„Å´„Çà„ÇãÂâäÈô§ |\n")
        f.write("| ‰∏ÄÊôÇ„Éï„Ç°„Ç§„É´ | „Éö„Ç§„É≠„Éº„ÉâÊú¨‰Ωì | ‚ùå Ê¨†ËêΩ | „ÉØ„Ç§„Éî„É≥„Ç∞„Å´„Çà„ÇãÁâ©ÁêÜÂâäÈô§ |\n\n")
        f.write("üïµÔ∏è **Analyst Note**:\n")
        f.write("„Åì„Çå„Çâ„ÅÆË®ºÊã†Ê¨†ËêΩ„ÅØ„Äå„ÉÑ„Éº„É´„ÅÆÈôêÁïå„Äç„Åß„ÅØ„Å™„Åè„ÄÅ**„ÄåÊîªÊíÉËÄÖ„Å´„Çà„ÇãÈ´òÂ∫¶„Å™Èö†ËîΩÂ∑•‰Ωú„Äç**„ÅÆÁµêÊûú„Åß„Åô„ÄÇ\n")
        f.write("Ghost Detection (USN„Ç∏„É£„Éº„Éä„É´) „Å´„Çà„Çä„Éï„Ç°„Ç§„É´„ÅÆ„ÄåÂ≠òÂú®„Åó„Å¶„ÅÑ„Åü‰∫ãÂÆü„Äç„ÅÆ„Åø„ÇíÁ¢∫Ë™ç„Åß„Åç„Å¶„ÅÑ„Åæ„Åô„ÄÇ\n\n")

    def _extract_dual_run_count(self, ioc, dfs):
        ua_count = "N/A"
        pf_count = "N/A"
        text_sources = [ioc.get("Value", ""), ioc.get("Summary", ""), ioc.get("Action", ""), ioc.get("Target_Path", "")]
        for text in text_sources:
            if not text: continue
            match = re.search(r"\(Run:\s*(\d+)\)", str(text), re.IGNORECASE)
            if match: ua_count = match.group(1); break
        
        target_name = ioc.get("Value", "").lower().strip()
        if target_name and dfs and dfs.get('Prefetch') is not None:
            target_base = os.path.basename(target_name.replace("\\", "/")).split(" ")[0]
            df = dfs['Prefetch']
            try:
                cols = {c.lower(): c for c in df.columns}
                exec_col = next((cols[c] for c in cols if "executable" in c), None)
                run_col = next((cols[c] for c in cols if "run" in c and "count" in c), None)
                if exec_col and run_col:
                    hits = df.filter(pl.col(exec_col).str.to_lowercase().str.contains(target_base, literal=True))
                    if hits.height > 0:
                        max_run = hits.select(pl.col(run_col).cast(pl.Int64, strict=False)).max().item()
                        if max_run is not None: pf_count = str(max_run)
            except: pass
        if ua_count == "N/A" and target_name and dfs and dfs.get('UserAssist') is not None:
            target_base = os.path.basename(target_name.replace("\\", "/")).split(" ")[0]
            df = dfs['UserAssist']
            try:
                cols = {c.lower(): c for c in df.columns}
                name_col = next((cols[c] for c in cols if "valuename" in c or "program" in c or "value" in c), None)
                run_col = next((cols[c] for c in cols if "run" in c and "count" in c), None)
                if not run_col: run_col = next((cols[c] for c in cols if "count" in c and "account" not in c), None)
                if name_col and run_col:
                    hits = df.filter(pl.col(name_col).str.to_lowercase().str.contains(target_base, literal=True))
                    if hits.height > 0:
                        max_run = hits.select(pl.col(run_col).cast(pl.Int64, strict=False)).max().item()
                        if max_run is not None: ua_count = str(max_run)
            except: pass
        return f"UA: {ua_count} | PF: {pf_count}"

    def _write_timeline_visual(self, f, phases, analyzer, enricher):
        t = self.txt
        f.write(f"## {t['h1_time']}\n(Detailed Timeline)\n\n")
        for idx, phase in enumerate(phases):
            if not phase: continue
            if isinstance(phase[0], dict) and 'Time' in phase[0]:
                date_str = str(phase[0]['Time']).split('T')[0]
            else: date_str = "Unknown"
            f.write(f"### üìÖ Phase {idx+1} ({date_str})\n")
            f.write(f"| Time (UTC) | Category | Event Summary | Source |\n|---|---|---|---|\n") 
            for ev in phase:
                summary = ev['Summary']
                time_display = str(ev.get('Time','')).replace('T', ' ').split('.')[0]
                cat_name = t['cats'].get(ev.get('Category'), ev.get('Category'))
                row_str = f"| {time_display} | {cat_name} | **{summary}** | {ev['Source']} |"
                f.write(f"{row_str}\n")
            f.write("\n")

    def _write_detection_statistics(self, f, medium_events, analyzer, dfs):
        t = self.txt
        f.write(f"## {t['h1_stats']}\n")
        
        # [Fix Issue #1] Correct Stats Presentation
        filtered_count = sum(analyzer.noise_stats.values()) if hasattr(analyzer, "noise_stats") else 0
        critical_count = len(analyzer.visual_iocs)
        total_events = analyzer.total_events_analyzed if hasattr(analyzer, "total_events_analyzed") else (filtered_count + critical_count + len(medium_events))
        if total_events == 0: total_events = 1 
        
        f.write("### üìä Overall Analysis Summary\n")
        f.write("| Category | Count | Note |\n|---|---|---|\n")
        f.write(f"| **Total Events Analyzed** | **{total_events}** | After filtering |\n")
        
        crit_pct = (critical_count / total_events) * 100
        f.write(f"| Critical Detections | {critical_count} | {crit_pct:.2f}% of analyzed |\n")
        f.write(f"| Filtered Out (Noise) | {filtered_count} | Removed before analysis |\n\n")
        
        f.write("### üéØ Critical Detection Breakdown\n")
        f.write("| Type | Count | Max Score | Impact |\n|---|---|---|---|\n")
        type_counts = {}
        for ioc in analyzer.visual_iocs:
            typ = ioc.get("Type", "Unknown")
            if "PHISHING" in typ: typ = "PHISHING / LNK"
            elif "TIMESTOMP" in typ: typ = "TIMESTOMP"
            elif "ANTI" in typ: typ = "ANTI_FORENSICS"
            elif "MASQUERADE" in typ: typ = "MASQUERADE"
            type_counts[typ] = type_counts.get(typ, 0) + 1
        for typ, count in sorted(type_counts.items(), key=lambda x: x[1], reverse=True):
            score = 300 if "ANTI" in typ or "MASQ" in typ else 250
            impact = "Evidence destruction" if "ANTI" in typ else ("Initial access" if "PHISH" in typ else "Evasion")
            f.write(f"| **{typ}** | **{count}** | {score} | {impact} |\n")
        f.write("\n")
        
        # [Fix Issue #2] Medium Events Breakdown
        f.write("### ‚ö†Ô∏è Medium Confidence Events\n")
        if medium_events:
            f.write(f"**Total Count:** {len(medium_events)} ‰ª∂ (Timeline CSVÂèÇÁÖß)\n")
            
            # Category Breakdown
            med_counts = {}
            for ev in medium_events:
                cat = ev.get('Category', 'Other')
                med_counts[cat] = med_counts.get(cat, 0) + 1
            
            f.write(f"**‰∏ª„Å™„Ç´„ÉÜ„Ç¥„É™ÂàÜÂ∏É:**\n")
            for cat, count in sorted(med_counts.items(), key=lambda x: x[1], reverse=True)[:5]:
                f.write(f"- {cat}: {count}‰ª∂\n")
            
            f.write("\n**‰ª£Ë°®ÁöÑ„Å™„Ç§„Éô„É≥„Éà (Top 5):**\n")
            f.write("| Time | Summary |\n|---|---|\n")
            for ev in medium_events[:5]:
                t_str = str(ev.get('Time','')).replace('T',' ')[:19]
                sum_str = str(ev.get('Summary', ''))[:80] + "..."
                f.write(f"| {t_str} | {sum_str} |\n")
            f.write("\n")
            
        f.write("### üìâ Filtered Noise Statistics\n")
        f.write("| Filter Reason | Count |\n|---|---|\n")
        if hasattr(analyzer, "noise_stats") and analyzer.noise_stats:
            for reason, count in sorted(analyzer.noise_stats.items(), key=lambda x: x[1], reverse=True):
                f.write(f"| {reason} | {count} |\n")
        else: f.write("| No noise filtered | 0 |\n")
        f.write("\n")

    def _write_recommendations(self, f, analyzer):
        t = self.txt
        f.write(f"## {t['h1_rec']}\n")
        f.write("Êú¨„Ç§„É≥„Ç∑„Éá„É≥„Éà„Å´„Åä„Åë„Çã„Éï„Ç©„É¨„É≥„Ç∏„ÉÉ„ÇØË™øÊüªÁµêÊûú„Å´Âü∫„Å•„Åç„ÄÅ‰ª•‰∏ã„ÅÆÊé®Â•®„Ç¢„ÇØ„Ç∑„Éß„É≥„ÇíÊèêÊ°à„Åó„Åæ„Åô„ÄÇ\n\n")
        
        # Determine Priority based on findings
        has_phishing = any("PHISHING" in str(ioc.get("Type", "")) for ioc in analyzer.visual_iocs)
        has_masquerade = any("MASQUERADE" in str(ioc.get("Type", "")) for ioc in analyzer.visual_iocs)
        has_anti = any("ANTI" in str(ioc.get("Type", "")) for ioc in analyzer.visual_iocs)

        f.write("### üìã Recommended Actions\n")
        f.write("| Priority | Action | Timeline | Reason |\n|---|---|---|---|\n")
        
        if has_anti or has_phishing:
             f.write("| üî• **P0** | **Event Log (4688) Command Line Recovery** | **Immediate** | LNKÂºïÊï∞„Åå„ÉØ„Ç§„Éî„É≥„Ç∞„Åï„Çå„Å¶„ÅÑ„Çã„Åü„ÇÅ„ÄÅ„Ç§„Éô„É≥„Éà„É≠„Ç∞„ÅåÂîØ‰∏Ä„ÅÆÂÆüË°å„Ç≥„Éû„É≥„ÉâÁâπÂÆöÊ∫ê„Åß„Åô„ÄÇ |\n")
        
        if has_masquerade:
             f.write("| üî• **P0** | **Analyze Suspicious Chrome Extension (.crx)** | 24 Hours | Ê∞∏Á∂öÂåñ„Éê„ÉÉ„ÇØ„Éâ„Ç¢„Å®„Åó„Å¶Ê©üËÉΩ„Åó„Å¶„ÅÑ„ÇãÂèØËÉΩÊÄß„ÅåÈ´ò„ÅÑ„Åü„ÇÅ„ÄÅ„É™„Éê„Éº„Çπ„Ç®„É≥„Ç∏„Éã„Ç¢„É™„É≥„Ç∞„ÅåÂøÖË¶Å„Åß„Åô„ÄÇ |\n")
        
        f.write("| üî• **P0** | **Network Log Analysis (C2 Identification)** | 24 Hours | Â§ñÈÉ®ÈÄö‰ø°ÂÖàIP„ÇíÁâπÂÆö„Åó„ÄÅ„Éï„Ç°„Ç§„Ç¢„Ç¶„Ç©„Éº„É´„Åß„Éñ„É≠„ÉÉ„ÇØ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ |\n")
        f.write("| üü° P1 | **Lateral Movement Check** | 1 Week | Âêå‰∏Ä„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØÂÜÖ„ÅÆ‰ªñÁ´ØÊú´„Å∏„ÅÆÊ®™Â±ïÈñã„ÇíË™øÊüª„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ |\n")
        f.write("| üü° P1 | **Credential Reset** | Immediate | ‰æµÂÆ≥„Åï„Çå„ÅüÁ´ØÊú´„Åß‰ΩøÁî®„Åï„Çå„ÅüÂÖ®„É¶„Éº„Ç∂„Éº„ÅÆ„Éë„Çπ„ÉØ„Éº„Éâ„É™„Çª„ÉÉ„Éà„ÇíÊé®Â•®„Åó„Åæ„Åô„ÄÇ |\n\n")

    def _write_ioc_appendix_unified(self, f, analyzer):
        t = self.txt
        f.write(f"## {t['h1_app']}\n(Full IOC List)\n")
        f.write("Êú¨Ë™øÊüª„ÅßÁ¢∫Ë™ç„Åï„Çå„Åü„Åô„Åπ„Å¶„ÅÆ‰æµÂÆ≥ÊåáÊ®ôÔºàIOCÔºâ„ÅÆ‰∏ÄË¶ß„Åß„Åô„ÄÇ\n\n")
        if analyzer.visual_iocs:
            f.write("### üìÇ File IOCs (Malicious/Suspicious Files)\n")
            f.write("| File Name | Path | Source | Note |\n|---|---|---|---|\n")
            seen = set()
            sorted_iocs = sorted(analyzer.visual_iocs, key=lambda x: 0 if "CRITICAL" in x.get("Reason", "").upper() else 1)
            for ioc in sorted_iocs:
                val = ioc['Value']
                path = ioc.get('Path', '-')
                if self._is_visual_noise(val): continue
                key = f"{val}|{path}"
                if key in seen: continue
                seen.add(key)
                reason = ioc.get("Reason", "Unknown")
                f.write(f"| `{val}` | `{path}` | {ioc['Type']} ({reason}) | {ioc.get('Time', 'N/A')} |\n")
            f.write("\n")
        if hasattr(analyzer, "infra_ips_found") and analyzer.infra_ips_found:
            f.write("### üåê Network IOCs (Suspicious Connections)\n")
            f.write("| Remote IP | Context |\n|---|---|\n")
            for ip in analyzer.infra_ips_found:
                 f.write(f"| `{ip}` | Detected in Event Logs |\n")
            f.write("\n")

    def export_pivot_config(self, pivot_seeds, path, primary_user):
        if not pivot_seeds: return
        config = {
            "Case_Context": {
                "Hostname": self.hostname,
                "Primary_User": primary_user,
                "Generated_At": datetime.now().isoformat()
            },
            "Deep_Dive_Targets": pivot_seeds[:20]
        }
        try:
            with open(path, "w", encoding="utf-8") as f:
                json.dump(config, f, indent=2, ensure_ascii=False)
            print(f"    -> [Lachesis] Pivot Config generated: {path}")
        except Exception as e:
            print(f"    [!] Failed to export Pivot Config: {e}")

    def export_json_grimoire(self, analysis_result, analyzer, json_path, primary_user):
        serializable_events = []
        for ev in analysis_result.get("events", []):
            serializable_events.append({
                "Time": str(ev.get('Time')),
                "User": ev.get('User'),
                "Category": ev.get('Category'),
                "Summary": ev.get('Summary'),
                "Source": ev.get('Source'),
                "Criticality": ev.get('Criticality', 0)
            })
        ips = list(analyzer.infra_ips_found) if hasattr(analyzer, "infra_ips_found") else []
        iocs = {"File": analyzer.visual_iocs, "Network": ips, "Cmd": []}
        grimoire_data = {
            "Metadata": {
                "Host": self.hostname, 
                "Case": "Investigation", 
                "Primary_User": primary_user, 
                "Generated_At": datetime.now().isoformat()
            },
            "Verdict": {
                "Flags": list(analysis_result.get("verdict_flags", [])), 
                "Lateral_Summary": analysis_result.get("lateral_summary", "")
            },
            "Timeline": serializable_events,
            "IOCs": iocs
        }
        try:
            with open(json_path, "w", encoding="utf-8") as f:
                json.dump(grimoire_data, f, indent=2, ensure_ascii=False)
            print(f"    -> [Chimera Ready] JSON Grimoire saved: {json_path}")
        except Exception as e:
            print(f"    [!] Failed to export JSON Grimoire: {e}")