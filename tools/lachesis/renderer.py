import json
import re
import polars as pl
import os
from datetime import datetime, timedelta
from tools.lachesis.intel import TEXT_RES

class LachesisRenderer:
    def __init__(self, output_path, lang="jp"):
        self.output_path = output_path
        self.lang = lang if lang in TEXT_RES else "jp"
        self.txt = TEXT_RES[self.lang]
        self.hostname = "Unknown"
        self.headers = {
            "en": {
                "exec": "Executive Summary", 
                "origin": "Initial Access Vector", 
                "chain": "Critical Chain", 
                "tech": "High Confidence Findings", 
                "iocs": "Key Indicators"
            },
            "jp": {
                "exec": "ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ãƒ»ã‚µãƒãƒªãƒ¼", 
                "origin": "åˆæœŸä¾µå…¥çµŒè·¯åˆ†æ (Initial Access Vector)", 
                "chain": "èª¿æŸ»ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ (Critical Chain)", 
                "tech": "æŠ€è¡“çš„è©³ç´° (High Confidence Findings)", 
                "iocs": "é‡è¦æŒ‡æ¨™ (Key Indicators)"
            }
        }

    def render_report(self, analysis_data, analyzer, enricher, origin_stories, dfs_for_ioc, metadata):
        self.hostname = metadata.get("hostname", "Unknown")
        out_file = self.output_path
        
        with open(out_file, "w", encoding="utf-8") as f:
            self._write_header(f, metadata["os_info"], metadata["primary_user"], analysis_data["time_range"])
            self._write_toc(f)
            
            # 1. Executive Summary
            self._write_executive_summary_visual(f, analyzer, analysis_data["time_range"], metadata["primary_user"])
            
            # 2. Initial Access Vector
            self._write_initial_access_vector(f, analyzer.pivot_seeds, origin_stories)
            
            # 3. Timeline
            self._write_timeline_visual(f, analysis_data["phases"], analyzer, enricher)
            
            # 4. Technical Findings (Pass origin_stories for LNK enrichment)
            self._write_technical_findings(f, analyzer, dfs_for_ioc, origin_stories) 
            
            # 5. Network & Lateral Movement (Plutos)
            self._write_plutos_section(f, dfs_for_ioc)
            
            # 6. Detection Statistics
            self._write_detection_statistics(f, analysis_data["medium_events"], analyzer, dfs_for_ioc)

            # 7. Conclusions & Recommendations
            self._write_recommendations(f, analyzer)
            
            # 7. Appendix (IOCs)
            self._write_ioc_appendix_unified(f, analyzer) 
            
            f.write(f"\n---\n*Report woven by SkiaHelios (The Triad v5.2 Perfection)* ğŸ¦")
        
        print(f"[*] Lachesis v5.2 is weaving the Grimoire into {out_file}...")

    def _write_header(self, f, os_info, primary_user, time_range):
        t = self.txt
        f.write(f"# {t['title']} - {self.hostname}\n\n")
        f.write(f"### ğŸ›¡ï¸ {t['coc_header']}\n")
        f.write("| Item | Details |\n|---|---|\n")
        f.write(f"| **Target Host** | **{self.hostname}** |\n")
        f.write(f"| **OS Info** | {os_info} |\n") 
        f.write(f"| **Primary User** | {primary_user} |\n")
        f.write(f"| **Incident Scope** | **{time_range}** |\n") 
        f.write(f"| **Report Date** | {datetime.now().strftime('%Y-%m-%d')} |\n\n---\n\n")

    def _write_toc(self, f):
        t = self.txt
        f.write("## ğŸ“š Table of Contents\n")
        f.write(f"- [{t['h1_exec']}](#{self._make_anchor(t['h1_exec'])})\n")
        f.write(f"- [{t['h1_origin']}](#{self._make_anchor(t['h1_origin'])})\n")
        f.write(f"- [{t['h1_time']}](#{self._make_anchor(t['h1_time'])})\n")
        f.write(f"- [{t['h1_tech']}](#{self._make_anchor(t['h1_tech'])})\n")
        f.write(f"- [{t['h1_stats']}](#{self._make_anchor(t['h1_stats'])})\n")
        f.write(f"- [{t['h1_rec']}](#{self._make_anchor(t['h1_rec'])})\n")
        f.write(f"- [{t['h1_app']}](#{self._make_anchor(t['h1_app'])})\n")
        f.write(f"- [Pivot Config (Deep Dive Targets)](#deep-dive-recommendation)\n")
        f.write("\n---\n\n")

    def _make_anchor(self, text):
        return text.lower().replace(" ", "-").replace(".", "").replace("&", "").replace("(", "").replace(")", "").replace("/", "")

    def _is_visual_noise(self, name):
        name = str(name).strip()
        if len(name) < 3: return True
        return False

    def _write_executive_summary_visual(self, f, analyzer, time_range, primary_user):
        t = self.txt
        f.write(f"## {t['h1_exec']}\n")
        
        visual_iocs = analyzer.visual_iocs
        has_time_change = any("SYSTEM_TIME" in str(ioc.get('Tag', '')) or "4616" in str(ioc.get('Value', '')) for ioc in visual_iocs)
        has_paradox = any("TIME_PARADOX" in str(ioc.get('Type', '')) for ioc in visual_iocs) or has_time_change
        has_masquerade = any("MASQUERADE" in str(ioc.get('Type', '')) for ioc in visual_iocs)
        has_phishing = any("PHISHING" in str(ioc.get('Type', '')) for ioc in visual_iocs)
        has_timestomp = any("TIMESTOMP" in str(ioc.get('Type', '')) for ioc in visual_iocs)
        has_anti = any("ANTI_FORENSICS" in str(ioc.get('Type', '')) for ioc in visual_iocs)
        
        conclusion = ""
        if has_paradox:
            conclusion = (
                f"**çµè«–:**\n{time_range} ã®æœŸé–“ã«ãŠã„ã¦ã€ç«¯æœ« {self.hostname} ã«å¯¾ã™ã‚‹ **é«˜åº¦ãªéš è”½å·¥ä½œã‚’ä¼´ã†é‡å¤§ãªä¾µå®³æ´»å‹•** ã‚’ç¢ºèªã—ã¾ã—ãŸã€‚\n\n"
                f"âš ï¸ğŸš¨ **SYSTEM TIME MANIPULATION DETECTED** ğŸš¨âš ï¸\n"
                f"**ã‚·ã‚¹ãƒ†ãƒ æ™‚åˆ»ã®å·»ãæˆ»ã—ï¼ˆTime Paradoxï¼‰** ãŒæ¤œçŸ¥ã•ã‚Œã¾ã—ãŸã€‚æ”»æ’ƒè€…ã¯æ™‚åˆ»ã‚’æ“ä½œã™ã‚‹ã“ã¨ã§ãƒ•ã‚©ãƒ¬ãƒ³ã‚¸ãƒƒã‚¯èª¿æŸ»ã‚’å¦¨å®³ã—ã€"
                f"ãƒ­ã‚°ã®ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã‚’æ„å›³çš„ã«ç ´å£Šã—ã‚ˆã†ã¨ã—ãŸç—•è·¡ãŒã‚ã‚Šã¾ã™ã€‚ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³åˆ†æã«ã¯æ¥µã‚ã¦æ…é‡ãªç²¾æŸ»ãŒå¿…è¦ã§ã™ã€‚\n"
            )
        elif has_masquerade or has_anti:
            conclusion = f"**çµè«–:**\n{time_range} ã®æœŸé–“ã«ãŠã„ã¦ã€ç«¯æœ« {self.hostname} ã«å¯¾ã™ã‚‹ **è¨¼æ‹ éš æ»…ãƒ»å½è£…ã‚’ä¼´ã†é‡å¤§ãªä¾µå®³æ´»å‹•** ã‚’ç¢ºèªã—ã¾ã—ãŸã€‚\n"
        elif visual_iocs:
            conclusion = f"**çµè«–:**\n{time_range} ã®æœŸé–“ã«ãŠã„ã¦ã€ç«¯æœ« {self.hostname} ã«å¯¾ã™ã‚‹ **CRITICAL ãƒ¬ãƒ™ãƒ«ã®ä¾µå®³æ´»å‹•** ã‚’ç¢ºèªã—ã¾ã—ãŸã€‚\n"
        else:
            conclusion = f"**çµè«–:**\næœ¬èª¿æŸ»ç¯„å›²ã«ãŠã„ã¦ã€é‡å¤§ãªã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆã®ç—•è·¡ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚\n"
        
        f.write(conclusion)
        
        attack_methods = []
        if has_phishing: attack_methods.append("ãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°ï¼ˆLNKï¼‰ã«ã‚ˆã‚‹åˆæœŸä¾µå…¥")
        if has_masquerade: attack_methods.append("å½è£…ãƒ•ã‚¡ã‚¤ãƒ«è¨­ç½®ï¼ˆMasqueradingï¼‰")
        if has_timestomp: attack_methods.append("ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—å½è£…ï¼ˆTimestompï¼‰")
        if has_paradox: attack_methods.append("**ã‚·ã‚¹ãƒ†ãƒ æ™‚é–“å·»ãæˆ»ã—ï¼ˆSystem Rollbackï¼‰**")
        if has_anti: attack_methods.append("ç—•è·¡ãƒ¯ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆAnti-Forensicsï¼‰")
        
        if not attack_methods: attack_methods = ["ä¸å¯©ãªã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£"]
            
        f.write(f"**ä¸»ãªæ”»æ’ƒæ‰‹å£:** {', '.join(attack_methods)}ã€‚\n\n")
        f.write("> **Deep Dive æ¨å¥¨:** è©³ç´°ãªèª¿æŸ»ã‚’è¡Œã†éš›ã¯ã€æ·»ä»˜ã® `Pivot_Config.json` ã«è¨˜è¼‰ã•ã‚ŒãŸ **CRITICAL_PHISHING** ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç¾¤ã‹ã‚‰é–‹å§‹ã—ã¦ãã ã•ã„ã€‚ç‰¹ã«ã‚¤ãƒ™ãƒ³ãƒˆãƒ­ã‚°ï¼ˆID 4688ï¼‰ã‹ã‚‰ã®ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¾©å…ƒãŒæœ€å„ªå…ˆäº‹é …ã§ã™ã€‚\n\n")
        
        f.write(self._render_mermaid_vertical_clustered(visual_iocs))
        f.write(self._render_key_indicators(visual_iocs))
        f.write("\n")

    def _write_initial_access_vector(self, f, pivot_seeds, origin_stories):
        t = self.txt
        f.write(f"## {t['h1_origin']}\n")
        phishing_lnks = [s for s in pivot_seeds if "PHISHING" in s.get("Reason", "")]
        drop_items = [s for s in pivot_seeds if "DROP" in s.get("Reason", "") and "PHISHING" not in s.get("Reason", "")]
        
        if phishing_lnks:
            f.write("**ãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°ã«ã‚ˆã‚‹åˆæœŸä¾µå…¥ãŒé«˜ç¢ºåº¦ã§ç¢ºèªã•ã‚Œã¾ã—ãŸã€‚**\n")
            f.write(f"- Recentãƒ•ã‚©ãƒ«ãƒ€ç­‰ã«ãŠã„ã¦ã€**{len(phishing_lnks)}ä»¶** ã®ä¸å¯©ãªLNKãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆï¼‰ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ãŒæ¤œçŸ¥ã•ã‚Œã¦ã„ã¾ã™ã€‚\n")
            f.write("\n| ã‚µãƒ³ãƒ—ãƒ«LNK | ã‚¢ã‚¯ã‚»ã‚¹æ™‚åˆ» (UTC) | æµå…¥å…ƒ (Origin Trace) |\n|---|---|---|\n")
            for seed in phishing_lnks[:10]:
                self._write_origin_row(f, seed, origin_stories)
            f.write("\n")

        if drop_items:
            f.write("**ä¸å¯©ãªãƒ„ãƒ¼ãƒ«ãƒ»ãƒ•ã‚¡ã‚¤ãƒ«ã®æŒã¡è¾¼ã¿ï¼ˆDropped Artifactsï¼‰:**\n")
            f.write("\n| ãƒ•ã‚¡ã‚¤ãƒ«å | ç™ºè¦‹å ´æ‰€ | æµå…¥å…ƒ (Origin Trace) |\n|---|---|---|\n")
            for seed in drop_items[:10]:
                self._write_origin_row(f, seed, origin_stories)
            f.write("\n")

        if not phishing_lnks and not drop_items:
            f.write("æ˜ç¢ºãªå¤–éƒ¨ä¾µå…¥ãƒ™ã‚¯ã‚¿ãƒ¼ã¯è‡ªå‹•æ¤œçŸ¥ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚\n\n")

    def _write_origin_row(self, f, seed, origin_stories):
        name = seed['Target_File']
        time = str(seed.get('Timestamp_Hint', '')).replace('T', ' ')[:19]
        origin_desc = "â“ No Trace Found (Low Confidence)"
        story = next((s for s in origin_stories if s["Target"] == name), None)
        
        if story:
            ev = story["Evidence"][0]
            url = ev.get("URL", "")
            url_display = (url[:50] + "...") if len(url) > 50 else url
            gap = ev.get('Time_Gap', '-')
            conf = story.get("Confidence", "LOW")
            reason = story.get("Reason", "")
            
            icon = "âœ…" if conf == "HIGH" else "âš ï¸" if conf == "MEDIUM" else "â“"
            prefix = "**Confirmed**" if conf == "HIGH" else "Inferred" if conf == "MEDIUM" else "Weak"
            origin_desc = f"{icon} **{prefix}**: {reason}<br/>ğŸ”— `{url_display}`<br/>*(Gap: {gap})*"
        
        col2 = time if time else f"`{seed.get('Target_Path', '')[:20]}`"
        f.write(f"| `{name}` | {col2} | {origin_desc} |\n")

    def _render_mermaid_vertical_clustered(self, events):
        if not events: return "\n(No critical events found for visualization)\n"
        
        f = ["\n### ğŸ¹ Attack Flow Visualization (Timeline)\n"]
        f.append("```mermaid")
        f.append("graph TD")
        
        f.append("    classDef init fill:#e63946,stroke:#333,stroke-width:2px,color:white;")
        f.append("    classDef exec fill:#f4a261,stroke:#333,stroke-width:2px,color:black;")
        f.append("    classDef persist fill:#2a9d8f,stroke:#333,stroke-width:2px,color:white;")
        f.append("    classDef anti fill:#264653,stroke:#333,stroke-width:2px,color:white;")
        f.append("    classDef time fill:#a8dadc,stroke:#457b9d,stroke-width:4px,color:black;")
        f.append("    classDef phishing fill:#ff6b6b,stroke:#c92a2a,stroke-width:2px,color:white;")
        
        critical_events = [ev for ev in events if ev.get('Score', 0) >= 60 or "CRITICAL" in str(ev.get('Type', ''))]
        sorted_events = sorted(critical_events, key=lambda x: x.get('Time', '9999'))
        
        if not sorted_events: return "\n(No critical events found)\n"

        has_paradox = any("TIME_PARADOX" in str(ev.get('Type', '')) for ev in events)
        if has_paradox:
            f.append("    subgraph T_PRE [\"âš ï¸ TIME MANIPULATION\"]")
            f.append("        N_TP[\"âª <b>SYSTEM ROLLBACK DETECTED</b><br/>Time Paradox Anomaly\"]:::time")
            f.append("    end")

        subgraphs = []
        current_subgraph = {"nodes": [], "start_time": None, "end_time": None}
        
        def parse_dt(t_str):
            try: return datetime.fromisoformat(str(t_str).replace("Z", ""))
            except: return datetime.min

        last_dt = None
        node_id_counter = 0
        burst_buffer = [] 
        
        def flush_burst_buffer(buffer, target_list, counter):
            if not buffer: return counter
            first_ev = buffer[0]
            cat = self._get_event_category(first_ev)
            
            if len(buffer) >= 3 and ("INITIAL" in cat or "EXECUTION" in cat):
                node_id = f"N{counter}"
                counter += 1
                start_t = str(buffer[0].get('Time', ''))[11:16]
                count = len(buffer)
                icon = "âš¡"
                if "INITIAL" in cat: icon = "ğŸ£"
                elif "EXEC" in cat: icon = "âš™ï¸"
                
                short_summary = self._get_short_summary(first_ev)
                label = f"{start_t} {icon} {count}x Events<br/>({short_summary} etc.)"
                style = ":::exec"
                if "INITIAL" in cat: style = ":::phishing"
                target_list.append(f"{node_id}[\"{label}\"]{style}")
                return counter
            else:
                for ev in buffer:
                    node_id = f"N{counter}"
                    counter += 1
                    t_str = str(ev.get('Time', ''))[11:16]
                    s_sum = self._get_short_summary(ev)
                    ev_cat = self._get_event_category(ev)
                    icon = "ğŸ”¹"
                    style = ":::default"
                    if "SYSTEM" in ev_cat: icon = "â°"; style = ":::time"
                    elif "ANTI" in ev_cat: icon = "ğŸ—‘ï¸"; style = ":::anti"
                    elif "PERSIST" in ev_cat: icon = "âš“"; style = ":::persist"
                    elif "INITIAL" in ev_cat: icon = "ğŸ£"; style = ":::init"
                    elif "PHISH" in ev_cat: icon = "ğŸ£"; style = ":::phishing"
                    
                    label = f"{t_str} {icon} {s_sum}"
                    target_list.append(f"{node_id}[\"{label}\"]{style}")
                return counter

        for ev in sorted_events:
            if self._is_visual_noise(ev.get("Value", "")): continue
            dt = parse_dt(ev.get('Time', ''))
            
            if last_dt and (dt - last_dt).total_seconds() > 3600:
                node_id_counter = flush_burst_buffer(burst_buffer, current_subgraph["nodes"], node_id_counter)
                burst_buffer = []
                subgraphs.append(current_subgraph)
                current_subgraph = {"nodes": [], "start_time": dt, "end_time": dt}
            
            if current_subgraph["start_time"] is None: current_subgraph["start_time"] = dt
            current_subgraph["end_time"] = dt
            last_dt = dt
            
            if not burst_buffer:
                burst_buffer.append(ev)
            else:
                last_in_buff = burst_buffer[-1]
                last_buff_dt = parse_dt(last_in_buff.get('Time', ''))
                same_cat = self._get_event_category(ev) == self._get_event_category(last_in_buff)
                close_time = (dt - last_buff_dt).total_seconds() < 120 
                
                if same_cat and close_time:
                    burst_buffer.append(ev)
                else:
                    node_id_counter = flush_burst_buffer(burst_buffer, current_subgraph["nodes"], node_id_counter)
                    burst_buffer = [ev]

        node_id_counter = flush_burst_buffer(burst_buffer, current_subgraph["nodes"], node_id_counter)
        subgraphs.append(current_subgraph)

        sg_counter = 0
        prev_sg_id = None

        if has_paradox:
            prev_sg_id = "T_PRE"
        
        for sg in subgraphs:
            if not sg["nodes"]: continue
            sg_id = f"T{sg_counter}"
            start_s = sg["start_time"].strftime("%H:%M")
            end_s = sg["end_time"].strftime("%H:%M")
            label = f"â° {start_s} - {end_s}"
            
            f.append(f"    subgraph {sg_id} [\"{label}\"]")
            for n in sg["nodes"]: f.append(f"        {n}")
            f.append("    end")
            
            if prev_sg_id:
                if prev_sg_id == "T_PRE":
                     f.append(f"    N_TP --> {sg['nodes'][0].split('[')[0]}")
                else:
                     f.append(f"    {prev_sg_id} --> {sg_id}")
            prev_sg_id = sg_id
            sg_counter += 1

        f.append("```\n")
        return "\n".join(f)

    def _get_event_category(self, ev):
        typ = str(ev.get('Type', '')).upper()
        tag = str(ev.get('Tag', '')).upper()
        if "SYSTEM_TIME" in tag or "TIME_CHANGE" in tag or "4616" in tag or "ROLLBACK" in tag: return "SYSTEM MANIPULATION"
        if "PHISH" in typ or "LNK" in typ: return "INITIAL ACCESS"
        if "WIPE" in typ or "ANTI" in typ: return "ANTI-FORENSICS"
        if "PERSIST" in typ: return "PERSISTENCE"
        if "EXEC" in typ or "RUN" in typ: return "EXECUTION"
        if "TIMESTOMP" in typ: return "TIMESTOMP (FILE)"
        return "OTHER ACTIVITY"

    def _get_short_summary(self, ev):
        val = ev.get('Value', '')
        if not val or val == "Unknown":
            val = ev.get('Summary', '')
            if not val: val = str(ev.get('Tag', 'Event'))
        if "SYSTEM_TIME" in str(ev.get('Tag', '')) or "4616" in str(val): return "System Time Changed"
        if "\\" in val or "/" in val: val = os.path.basename(val.replace("\\", "/"))
        return val[:15] + ".." if len(val) > 15 else val

    def _render_key_indicators(self, events):
        output = ["\n### ğŸ’ Key Indicators (Critical Only)\n"]
        grouped = {}
        for ev in events:
            if ev.get('Score', 0) < 50 and "CRITICAL" not in str(ev.get('Type', '')): continue
            cat = self._get_event_category(ev)
            if cat not in grouped: grouped[cat] = []
            grouped[cat].append(ev)

        cat_titles = {
            "INITIAL ACCESS": "ğŸ£ Initial Access", "ANTI-FORENSICS": "ğŸ™ˆ Anti-Forensics",
            "SYSTEM MANIPULATION": "ğŸš¨ System Time Manipulation", "PERSISTENCE": "âš“ Persistence",
            "EXECUTION": "âš¡ Execution", "TIMESTOMP (FILE)": "ğŸ•’ Timestomp (Files)"
        }
        keys = sorted(grouped.keys(), key=lambda k: 0 if "SYSTEM" in k else 1)

        for cat in keys:
            items = grouped[cat]
            output.append(f"#### {cat_titles.get(cat, cat)}")
            output.append("| Time (UTC) | Value / Artifact | Impact/Target | Score |")
            output.append("|---|---|---|---|")
            items.sort(key=lambda x: x.get('Time', '9999'))
            for ioc in items:
                t = str(ioc.get('Time', 'N/A')).replace('T', ' ')[:19]
                val = ioc.get('Value', '-')
                if not val or val == "Unknown": val = ioc.get('Summary', '-')
                score = ioc.get('Score', 0)
                impact = "-"
                extra = ioc.get('Extra', {})
                tag = str(ioc.get('Tag', ''))
                if "SYSTEM_TIME" in tag or "4616" in tag or "TIME_PARADOX" in str(ioc.get('Type', '')):
                    impact = "**System Clock Altered**"
                elif cat == "INITIAL ACCESS":
                    tgt = extra.get('Target_Path', 'Unknown')
                    if tgt and tgt != "Unknown":
                        impact = f"Target: {tgt[:30]}..."
                output.append(f"| {t} | `{val}` | {impact} | {score} |")
            output.append("\n")
        return "\n".join(output)

    def _write_technical_findings(self, f, analyzer, dfs, origin_stories):
        t = self.txt
        f.write(f"## {t['h1_tech']}\n")
        high_conf_events = [ioc for ioc in analyzer.visual_iocs if analyzer.is_force_include_ioc(ioc) or "ANTI" in str(ioc.get("Type", ""))]
        self._write_anti_forensics_section(f, high_conf_events, dfs)
        f.write("### ğŸ” Detailed Findings by Category\n\n")
        
        groups = {}
        for ioc in high_conf_events:
            cat = self._get_event_category(ioc)
            if "ANTI" in cat: continue
            if cat not in groups: groups[cat] = []
            groups[cat].append(ioc)
            
        for cat, items in groups.items():
            f.write(f"#### {cat}\n")
            
            # [Fix Issue #3] Special handling for Initial Access LNKs
            if "INITIAL ACCESS" in cat:
                self._render_grouped_lnk_findings(f, items, origin_stories, analyzer)
            else:
                items.sort(key=lambda x: x.get('Time', '9999'))
                for ioc in items:
                    dt = str(ioc.get('Time', '')).replace('T', ' ')[:19]
                    val = ioc.get('Value', '') or ioc.get('Summary', '')
                    f.write(f"- **{dt}** | `{val}`\n")
                    insight = analyzer.generate_ioc_insight(ioc)
                    if insight: f.write(f"  - ğŸ•µï¸ **Analyst Note:** {insight}\n")
            f.write("\n")

    def _render_grouped_lnk_findings(self, f, items, origin_stories, analyzer):
        """Helper to render LNK findings with grouping to avoid repetition"""
        high_interest = []
        generic_lnks = []
        
        for ioc in items:
            name = ioc.get("Value", "")
            is_special = False
            
            # Check for Origin Story (Confirmed Download)
            story = next((s for s in origin_stories if s["Target"] == name), None) if origin_stories else None
            if story and story.get("Confidence") == "HIGH": is_special = True
            
            # Check for DEFCON/Masquerade
            if "DEFCON" in name.upper() or "MASQUERADE" in str(ioc.get("Extra", {}).get("Risk", "")): is_special = True
            
            if is_special: high_interest.append(ioc)
            else: generic_lnks.append(ioc)
            
        # Render High Interest Items
        if high_interest:
            f.write("**ç‰¹è¨˜äº‹é … (High Interest Artifacts):**\n")
            high_interest.sort(key=lambda x: x.get('Time', '9999'))
            for ioc in high_interest:
                dt = str(ioc.get('Time', '')).replace('T', ' ')[:19]
                val = ioc.get('Value', '')
                f.write(f"- **{dt}** | `{val}`\n")
                insight = analyzer.generate_ioc_insight(ioc)
                
                # Append Origin Info if available
                story = next((s for s in origin_stories if s["Target"] == val), None) if origin_stories else None
                if story and story.get("Confidence") == "HIGH":
                     gap = story['Evidence'][0].get('Time_Gap', '-')
                     insight = f"âœ… **Web Download Confirmed** (Gap: {gap})<br/>" + (insight if insight else "")
                
                if insight: f.write(f"  - ğŸ•µï¸ **Analyst Note:** {insight}\n")

        # Render Generic Items Summary
        if generic_lnks:
            f.write(f"\n**ãã®ä»–ã®LNK ({len(generic_lnks)}ä»¶):**\n")
            f.write("ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«åã‚’è£…ã£ãŸã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆç¾¤ã§ã™ã€‚Target_Pathæƒ…å ±ã¯ãƒ¯ã‚¤ãƒ”ãƒ³ã‚°ã«ã‚ˆã‚Šæ¬ è½ã—ã¦ã„ã¾ã™ãŒã€ä½œæˆãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰ãƒ•ã‚£ãƒƒã‚·ãƒ³ã‚°ç”±æ¥ã¨æ–­å®šã•ã‚Œã¾ã™ã€‚\n")
            generic_lnks.sort(key=lambda x: x.get('Time', '9999'))
            for ioc in generic_lnks:
                dt = str(ioc.get('Time', '')).replace('T', ' ')[:19]
                val = ioc.get('Value', '')
                f.write(f"- {dt} | `{val}`\n")

    def _write_anti_forensics_section(self, f, ioc_list, dfs):
        af_tools = [ioc for ioc in ioc_list if "ANTI" in str(ioc.get("Type", "")) or "WIPE" in str(ioc.get("Type", ""))]
        if not af_tools: return
        f.write("### ğŸš¨ Anti-Forensics Activities (Evidence Destruction)\n\n")
        f.write("âš ï¸âš ï¸âš ï¸ **é‡å¤§ãªè¨¼æ‹ éš æ»…æ´»å‹•ã‚’æ¤œå‡º** âš ï¸âš ï¸âš ï¸\n\n")
        f.write("æ”»æ’ƒè€…ã¯ä¾µå…¥å¾Œã€ä»¥ä¸‹ã®ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦æ´»å‹•ç—•è·¡ã‚’æ„å›³çš„ã«æŠ¹æ¶ˆã—ã¦ã„ã¾ã™ï¼š\n\n")
        seen_tools = set()
        for tool in af_tools:
            name = tool.get("Value", "Unknown").upper()
            if name in seen_tools: continue
            seen_tools.add(name)
            run_count = self._extract_dual_run_count(tool, dfs)
            last_run = tool.get("Time", "Unknown").replace("T", " ")[:19]
            desc = "ãƒ‡ãƒ¼ã‚¿æŠ¹æ¶ˆãƒ„ãƒ¼ãƒ«"
            if "BCWIPE" in name: desc = "è»äº‹ãƒ¬ãƒ™ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¯ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«ã€‚é€šå¸¸ã®å¾©å…ƒã‚’ä¸å¯èƒ½ã«ã—ã¾ã™ã€‚"
            elif "CCLEANER" in name: desc = "ã‚·ã‚¹ãƒ†ãƒ ã‚¯ãƒªãƒ¼ãƒŠãƒ¼ã€‚ãƒ–ãƒ©ã‚¦ã‚¶å±¥æ­´ã‚„MRUã®å‰Šé™¤ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚"
            f.write(f"#### {name}\n")
            f.write(f"- ğŸ“Š **Run Count**: **{run_count}**\n")
            f.write(f"- ğŸ• **Last Execution**: {last_run} (UTC)\n")
            f.write(f"- âš ï¸ **Severity**: CRITICAL\n")
            f.write(f"- ğŸ” **Description**: {desc}\n\n")
            f.write(f"ğŸ•µï¸ **Analyst Note**:\n")
            if "BCWIPE" in name:
                 f.write("ã“ã®ãƒ„ãƒ¼ãƒ«ã®å®Ÿè¡Œã«ã‚ˆã‚Šã€LNKãƒ•ã‚¡ã‚¤ãƒ«ã€Prefetchã€ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ç­‰ã®è¨¼æ‹ ãŒç‰©ç†çš„ã«ä¸Šæ›¸ãå‰Šé™¤ã•ã‚ŒãŸå¯èƒ½æ€§ãŒæ¥µã‚ã¦é«˜ã„ã§ã™ã€‚\n")
            else:
                 f.write("æ”»æ’ƒæ´»å‹•çµ‚äº†å¾Œã®ç—•è·¡å‰Šé™¤ï¼ˆCleanupï¼‰ã«ä½¿ç”¨ã•ã‚ŒãŸã¨æ¨å®šã•ã‚Œã¾ã™ã€‚\n")
            f.write("\n---\n\n")
        f.write("### ğŸ“‰ Missing Evidence Impact Assessment\n\n")
        f.write("ä»¥ä¸‹ã®è¨¼æ‹ ãŒã€Anti-Forensicsãƒ„ãƒ¼ãƒ«ã«ã‚ˆã£ã¦å¤±ã‚ã‚ŒãŸã¨åˆ¤æ–­ã•ã‚Œã¾ã™ï¼š\n\n")
        f.write("| è¨¼æ‹ ã‚«ãƒ†ã‚´ãƒª | æœŸå¾…ã•ã‚Œã‚‹æƒ…å ± | ç¾çŠ¶ | æ¨å®šåŸå›  |\n|---|---|---|---|\n")
        f.write("| LNK Target Paths | `cmd.exe ...` ç­‰ã®å¼•æ•° | âŒ æ¬ è½ | BCWipe/SDeleteã«ã‚ˆã‚‹å‰Šé™¤ |\n")
        f.write("| Prefetch (Tools) | å®Ÿè¡Œå›æ•°ãƒ»ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ— | âŒ æ¬ è½ | CCleaner/BCWipeã«ã‚ˆã‚‹å‰Šé™¤ |\n")
        f.write("| ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ« | ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰æœ¬ä½“ | âŒ æ¬ è½ | ãƒ¯ã‚¤ãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹ç‰©ç†å‰Šé™¤ |\n\n")
        f.write("ğŸ•µï¸ **Analyst Note**:\n")
        f.write("ã“ã‚Œã‚‰ã®è¨¼æ‹ æ¬ è½ã¯ã€Œãƒ„ãƒ¼ãƒ«ã®é™ç•Œã€ã§ã¯ãªãã€**ã€Œæ”»æ’ƒè€…ã«ã‚ˆã‚‹é«˜åº¦ãªéš è”½å·¥ä½œã€**ã®çµæœã§ã™ã€‚\n")
        f.write("Ghost Detection (USNã‚¸ãƒ£ãƒ¼ãƒŠãƒ«) ã«ã‚ˆã‚Šãƒ•ã‚¡ã‚¤ãƒ«ã®ã€Œå­˜åœ¨ã—ã¦ã„ãŸäº‹å®Ÿã€ã®ã¿ã‚’ç¢ºèªã§ãã¦ã„ã¾ã™ã€‚\n\n")

    def _extract_dual_run_count(self, ioc, dfs):
        ua_count = "N/A"
        pf_count = "N/A"
        text_sources = [ioc.get("Value", ""), ioc.get("Summary", ""), ioc.get("Action", ""), ioc.get("Target_Path", "")]
        for text in text_sources:
            if not text: continue
            match = re.search(r"\(Run:\s*(\d+)\)", str(text), re.IGNORECASE)
            if match: ua_count = match.group(1); break
        
        target_name = ioc.get("Value", "").lower().strip()
        if target_name and dfs and dfs.get('Prefetch') is not None:
            target_base = os.path.basename(target_name.replace("\\", "/")).split(" ")[0]
            df = dfs['Prefetch']
            try:
                cols = {c.lower(): c for c in df.columns}
                exec_col = next((cols[c] for c in cols if "executable" in c), None)
                run_col = next((cols[c] for c in cols if "run" in c and "count" in c), None)
                if exec_col and run_col:
                    hits = df.filter(pl.col(exec_col).str.to_lowercase().str.contains(target_base, literal=True))
                    if hits.height > 0:
                        max_run = hits.select(pl.col(run_col).cast(pl.Int64, strict=False)).max().item()
                        if max_run is not None: pf_count = str(max_run)
            except: pass
        if ua_count == "N/A" and target_name and dfs and dfs.get('UserAssist') is not None:
            target_base = os.path.basename(target_name.replace("\\", "/")).split(" ")[0]
            df = dfs['UserAssist']
            try:
                cols = {c.lower(): c for c in df.columns}
                name_col = next((cols[c] for c in cols if "valuename" in c or "program" in c or "value" in c), None)
                run_col = next((cols[c] for c in cols if "run" in c and "count" in c), None)
                if not run_col: run_col = next((cols[c] for c in cols if "count" in c and "account" not in c), None)
                if name_col and run_col:
                    hits = df.filter(pl.col(name_col).str.to_lowercase().str.contains(target_base, literal=True))
                    if hits.height > 0:
                        max_run = hits.select(pl.col(run_col).cast(pl.Int64, strict=False)).max().item()
                        if max_run is not None: ua_count = str(max_run)
            except: pass
        return f"UA: {ua_count} | PF: {pf_count}"

    def _write_timeline_visual(self, f, phases, analyzer, enricher):
        t = self.txt
        f.write(f"## {t['h1_time']}\n(Detailed Timeline)\n\n")
        for idx, phase in enumerate(phases):
            if not phase: continue
            if isinstance(phase[0], dict) and 'Time' in phase[0]:
                date_str = str(phase[0]['Time']).split('T')[0]
            else: date_str = "Unknown"
            f.write(f"### ğŸ“… Phase {idx+1} ({date_str})\n")
            f.write(f"| Time (UTC) | Category | Event Summary | Source |\n|---|---|---|---|\n") 
            for ev in phase:
                summary = ev['Summary']
                time_display = str(ev.get('Time','')).replace('T', ' ').split('.')[0]
                cat_name = t['cats'].get(ev.get('Category'), ev.get('Category'))
                row_str = f"| {time_display} | {cat_name} | **{summary}** | {ev['Source']} |"
                f.write(f"{row_str}\n")
            f.write("\n")

    def _write_detection_statistics(self, f, medium_events, analyzer, dfs):
        t = self.txt
        f.write(f"## {t['h1_stats']}\n")
        
        # [Fix Issue #1] Correct Stats Presentation
        filtered_count = sum(analyzer.noise_stats.values()) if hasattr(analyzer, "noise_stats") else 0
        critical_count = len(analyzer.visual_iocs)
        total_events = analyzer.total_events_analyzed if hasattr(analyzer, "total_events_analyzed") else (filtered_count + critical_count + len(medium_events))
        if total_events == 0: total_events = 1 
        
        f.write("### ğŸ“Š Overall Analysis Summary\n")
        f.write("| Category | Count | Note |\n|---|---|---|\n")
        f.write(f"| **Total Events Analyzed** | **{total_events}** | After filtering |\n")
        
        crit_pct = (critical_count / total_events) * 100
        f.write(f"| Critical Detections | {critical_count} | {crit_pct:.2f}% of analyzed |\n")
        f.write(f"| Filtered Out (Noise) | {filtered_count} | Removed before analysis |\n\n")
        
        f.write("### ğŸ¯ Critical Detection Breakdown\n")
        f.write("| Type | Count | Max Score | Impact |\n|---|---|---|---|\n")
        type_counts = {}
        for ioc in analyzer.visual_iocs:
            typ = ioc.get("Type", "Unknown")
            if "PHISHING" in typ: typ = "PHISHING / LNK"
            elif "TIMESTOMP" in typ: typ = "TIMESTOMP"
            elif "ANTI" in typ: typ = "ANTI_FORENSICS"
            elif "MASQUERADE" in typ: typ = "MASQUERADE"
            type_counts[typ] = type_counts.get(typ, 0) + 1
        for typ, count in sorted(type_counts.items(), key=lambda x: x[1], reverse=True):
            score = 300 if "ANTI" in typ or "MASQ" in typ else 250
            impact = "Evidence destruction" if "ANTI" in typ else ("Initial access" if "PHISH" in typ else "Evasion")
            f.write(f"| **{typ}** | **{count}** | {score} | {impact} |\n")
        f.write("\n")
        
        # [Fix Issue #2] Medium Events Breakdown
        f.write("### âš ï¸ Medium Confidence Events\n")
        if medium_events:
            f.write(f"**Total Count:** {len(medium_events)} ä»¶ (Timeline CSVå‚ç…§)\n")
            
            # Category Breakdown
            med_counts = {}
            for ev in medium_events:
                cat = ev.get('Category', 'Other')
                med_counts[cat] = med_counts.get(cat, 0) + 1
            
            f.write(f"**ä¸»ãªã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒ:**\n")
            for cat, count in sorted(med_counts.items(), key=lambda x: x[1], reverse=True)[:5]:
                f.write(f"- {cat}: {count}ä»¶\n")
            
            f.write("\n**ä»£è¡¨çš„ãªã‚¤ãƒ™ãƒ³ãƒˆ (Top 5):**\n")
            f.write("| Time | Summary |\n|---|---|\n")
            for ev in medium_events[:5]:
                t_str = str(ev.get('Time','')).replace('T',' ')[:19]
                sum_str = str(ev.get('Summary', ''))[:80] + "..."
                f.write(f"| {t_str} | {sum_str} |\n")
            f.write("\n")
            
        f.write("### ğŸ“‰ Filtered Noise Statistics\n")
        f.write("| Filter Reason | Count |\n|---|---|\n")
        if hasattr(analyzer, "noise_stats") and analyzer.noise_stats:
            for reason, count in sorted(analyzer.noise_stats.items(), key=lambda x: x[1], reverse=True):
                f.write(f"| {reason} | {count} |\n")
        else: f.write("| No noise filtered | 0 |\n")
        f.write("\n")

    def _write_recommendations(self, f, analyzer):
        t = self.txt
        f.write(f"## {t['h1_rec']}\n")
        f.write("æœ¬ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆã«ãŠã‘ã‚‹ãƒ•ã‚©ãƒ¬ãƒ³ã‚¸ãƒƒã‚¯èª¿æŸ»çµæœã«åŸºã¥ãã€ä»¥ä¸‹ã®æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ææ¡ˆã—ã¾ã™ã€‚\n\n")
        
        # Determine Priority based on findings
        has_phishing = any("PHISHING" in str(ioc.get("Type", "")) for ioc in analyzer.visual_iocs)
        has_masquerade = any("MASQUERADE" in str(ioc.get("Type", "")) for ioc in analyzer.visual_iocs)
        has_anti = any("ANTI" in str(ioc.get("Type", "")) for ioc in analyzer.visual_iocs)

        f.write("### ğŸ“‹ Recommended Actions\n")
        f.write("| Priority | Action | Timeline | Reason |\n|---|---|---|---|\n")
        
        if has_anti or has_phishing:
             f.write("| ğŸ”¥ **P0** | **Event Log (4688) Command Line Recovery** | **Immediate** | LNKå¼•æ•°ãŒãƒ¯ã‚¤ãƒ”ãƒ³ã‚°ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€ã‚¤ãƒ™ãƒ³ãƒˆãƒ­ã‚°ãŒå”¯ä¸€ã®å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰ç‰¹å®šæºã§ã™ã€‚ |\n")
        
        if has_masquerade:
             f.write("| ğŸ”¥ **P0** | **Analyze Suspicious Chrome Extension (.crx)** | 24 Hours | æ°¸ç¶šåŒ–ãƒãƒƒã‚¯ãƒ‰ã‚¢ã¨ã—ã¦æ©Ÿèƒ½ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒé«˜ã„ãŸã‚ã€ãƒªãƒãƒ¼ã‚¹ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ãŒå¿…è¦ã§ã™ã€‚ |\n")
        
        f.write("| ğŸ”¥ **P0** | **Network Log Analysis (C2 Identification)** | 24 Hours | å¤–éƒ¨é€šä¿¡å…ˆIPã‚’ç‰¹å®šã—ã€ãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ«ã§ãƒ–ãƒ­ãƒƒã‚¯ã—ã¦ãã ã•ã„ã€‚ |\n")
        f.write("| ğŸŸ¡ P1 | **Lateral Movement Check** | 1 Week | åŒä¸€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å†…ã®ä»–ç«¯æœ«ã¸ã®æ¨ªå±•é–‹ã‚’èª¿æŸ»ã—ã¦ãã ã•ã„ã€‚ |\\n")
        f.write("| ğŸŸ¡ P1 | **Credential Reset** | Immediate | ä¾µå®³ã•ã‚ŒãŸç«¯æœ«ã§ä½¿ç”¨ã•ã‚ŒãŸå…¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãƒªã‚»ãƒƒãƒˆã‚’æ¨å¥¨ã—ã¾ã™ã€‚ |\\n\\n")

    # ==========================================
    # [NEW] Plutos Section Methods (v3.0 Critical Integration)
    # ==========================================
    def _write_plutos_section(self, f, dfs):
        """PlutosGateã®çµæœã‚’ãƒ¬ãƒãƒ¼ãƒˆã«æç”» - å…¨ã‚½ãƒ¼ã‚¹çµ±åˆç‰ˆ"""
        f.write("\n## ğŸŒ 5. é‡è¦ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŠã‚ˆã³æŒã¡å‡ºã—ç—•è·¡ (Critical Network & Exfiltration)\n")
        f.write("PlutosGateã‚¨ãƒ³ã‚¸ãƒ³ã«ã‚ˆã‚Šæ¤œå‡ºã•ã‚ŒãŸã€**ãƒ‡ãƒ¼ã‚¿ã®æŒã¡å‡ºã—**ã€**ãƒ¡ãƒ¼ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ä¸æ­£ã‚³ãƒ”ãƒ¼**ã€ãŠã‚ˆã³**é«˜ãƒªã‚¹ã‚¯ãªå¤–éƒ¨é€šä¿¡**ã®ç—•è·¡ã€‚\n\n")

        # 1. Critical Table (SRUM + Exfil + Emailçµ±åˆ)
        f.write("### ğŸš¨ 5.1 æ¤œå‡ºã•ã‚ŒãŸé‡å¤§ãªè„…å¨ (Critical Threats Detected)\n")
        critical_table = self._generate_critical_threats_table(dfs)
        f.write(critical_table + "\n\n")

        # 2. ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ (Mermaid)
        net_map = self._generate_critical_network_map(dfs)
        if net_map:
            f.write("### ğŸ—ºï¸ 5.2 ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç›¸é–¢å›³ (Critical Activity Map)\n")
            f.write(net_map + "\n\n")
            f.write("> **Note:** èµ¤è‰²ã¯å¤–éƒ¨ã¸ã®æŒã¡å‡ºã—ã‚„C2é€šä¿¡ã€ã‚ªãƒ¬ãƒ³ã‚¸è‰²ã¯å†…éƒ¨ã¸ã®æ¨ªå±•é–‹ã‚’ç¤ºå”†ã—ã¾ã™ã€‚\n\n")
        else:
            f.write("â€» è¦–è¦šåŒ–å¯èƒ½ãªãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒˆãƒãƒ­ã‚¸ãƒ¼ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚\n\n")
        
        f.write("---\n")

    def _generate_critical_network_map(self, dfs):
        """Plutosã®SRUM/EVTXãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã€è„…å¨åº¦ã®é«˜ã„é€šä¿¡ã®ã¿ã‚’æŠ½å‡ºã—ã¦MermaidåŒ–"""
        srum_df = dfs.get("Plutos_Srum")
        net_df = dfs.get("Plutos_Network")
        
        mermaid = ["graph LR", "    H[TARGET HOST]"]
        
        # [FIX] Mermaidæ­£ã—ã„æ§‹æ–‡: classDef ã§ã‚¹ã‚¿ã‚¤ãƒ«å®šç¾©ã€class ã§é©ç”¨
        mermaid.append("    classDef exfil fill:darkred,stroke:red,color:white,stroke-width:2px;")
        mermaid.append("    classDef lateral fill:darkorange,stroke:orange,color:white,stroke-width:2px;")
        mermaid.append("    classDef host fill:darkgreen,stroke:lime,color:white,stroke-width:4px;")
        mermaid.append("    class H host;")
        
        nodes = set()
        edges = []

        # A. SRUMã‹ã‚‰ã®æŒã¡å‡ºã—ãƒãƒ¼ãƒ‰ (Unknown IP -> Cloud Upload)
        if srum_df is not None and srum_df.height > 0:
            try:
                if "Heat_Score" in srum_df.columns:
                    high_heat = srum_df.filter(pl.col("Heat_Score").cast(pl.Int64, strict=False) >= 60)
                    for row in high_heat.iter_rows(named=True):
                        proc = str(row.get("Process", "Unknown")).split("\\")[-1]
                        node_id = "External_Cloud"
                        if node_id not in nodes:
                            mermaid.append(f"    {node_id}([ExternalCloud])")
                            nodes.add(node_id)
                        
                        edge_key = f"{proc}_to_{node_id}"
                        if edge_key not in edges:
                            mermaid.append(f"    H --|{proc}|--> {node_id}")
                            mermaid.append(f"    class {node_id} exfil;")
                            edges.append(edge_key)
            except: pass

        # B. ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è©³ç´°ãƒ­ã‚°ã‹ã‚‰ã®C2/Lateralãƒãƒ¼ãƒ‰
        if net_df is not None and net_df.height > 0:
            try:
                if "Plutos_Verdict" in net_df.columns:
                    critical_net = net_df.filter(
                        pl.col("Plutos_Verdict").str.contains(r"(?i)LATERAL|C2|RDP")
                    ).head(10)
                    
                    for row in critical_net.iter_rows(named=True):
                        remote = row.get("Remote_IP", "Unknown")
                        if remote in ["-", "", "127.0.0.1", "::1", "Unknown"]: continue
                        
                        node_id = remote.replace(".", "_").replace(":", "_")
                        proc = str(row.get("Process", "")).split("\\")[-1]
                        verdict = str(row.get("Plutos_Verdict", ""))
                        
                        if node_id not in nodes:
                            mermaid.append(f"    {node_id}([{remote}])")
                            nodes.add(node_id)
                        
                        if "LATERAL" in verdict:
                            mermaid.append(f"    H ==|{proc}|==> {node_id}")
                            mermaid.append(f"    class {node_id} lateral;")
                        else:
                            mermaid.append(f"    H --|{proc}|--> {node_id}")
                            mermaid.append(f"    class {node_id} exfil;")
            except: pass

        if len(mermaid) <= 5: return ""
        return "```mermaid\n" + "\n".join(mermaid) + "\n```"

    def _generate_critical_threats_table(self, dfs):
        """SRUM, Exfil, Emailã®å…¨ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã€Œè‡´å‘½çš„ã€ãªã‚‚ã®ã ã‘ã‚’çµ±åˆã—ãŸãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ç”Ÿæˆ"""
        rows = []
        
        # 1. SRUM High Heat (é€šä¿¡ãƒãƒ¼ã‚¹ãƒˆ)
        srum_df = dfs.get("Plutos_Srum")
        if srum_df is not None and srum_df.height > 0:
            try:
                if "Heat_Score" in srum_df.columns:
                    df = srum_df.filter(pl.col("Heat_Score").cast(pl.Int64, strict=False) >= 60)
                    for r in df.iter_rows(named=True):
                        ts = str(r.get("Timestamp", "")).split(".")[0]
                        proc = str(r.get("Process", "")).split("\\")[-1]
                        sent_bytes = r.get("BytesSent", 0)
                        sent_mb = int(sent_bytes) // 1024 // 1024 if sent_bytes else 0
                        
                        rows.append({
                            "Time": ts,
                            "Icon": "ğŸ“¤",
                            "Verdict": f"**{r.get('Plutos_Verdict', 'HIGH_HEAT')}**",
                            "Details": f"Proc: {proc}<br>Sent: {sent_mb} MB",
                            "Ref": "See: Plutos_Report_srum.csv"
                        })
            except: pass

        # 2. Exfil Correlation (æŒã¡å‡ºã—ç¢ºå®š)
        exfil_df = dfs.get("Plutos_Exfil")
        if exfil_df is not None and exfil_df.height > 0:
            try:
                for r in exfil_df.iter_rows(named=True):
                    ts = str(r.get("Timestamp", "")).split(".")[0]
                    fname = r.get("FileName", "Unknown")
                    url = str(r.get("URL", ""))[:30] + "..." if r.get("URL") else ""
                    
                    rows.append({
                        "Time": ts,
                        "Icon": "ğŸš¨",
                        "Verdict": "**EXFIL_CORRELATION**",
                        "Details": f"File: **{fname}**<br>URL: {url}",
                        "Ref": "See: Plutos_Report_exfil_correlation.csv"
                    })
            except: pass

        # 3. Email Hunter (ãƒ‘ã‚¹å˜ä½é›†ç´„)
        email_df = dfs.get("Plutos_Email")
        if email_df is not None and email_df.height > 0:
            try:
                # ãƒ‘ã‚¹ï¼ˆå ´æ‰€ï¼‰ã”ã¨ã«é›†ç´„
                if "Path" in email_df.columns:
                    grouped = email_df.group_by("Path").agg([
                        pl.count("Artifact").alias("Count"),
                        pl.min("Timestamp").alias("Start"),
                        pl.max("Timestamp").alias("End"),
                        pl.first("Verdict").alias("Verdict_Sample")
                    ])

                    for r in grouped.iter_rows(named=True):
                        start = str(r["Start"]).split(".")[0]
                        end = str(r["End"]).split(".")[0]
                        count = r["Count"]
                        path = r["Path"]
                        verdict = str(r["Verdict_Sample"] or "")
                        
                        # æ™‚é–“è¡¨è¨˜ã®èª¿æ•´ (å˜ç™ºãªã‚‰Startã®ã¿)
                        time_str = start if start == end else f"{start} - {end}"
                        
                        icon = "ğŸ“¦"
                        if "Dropbox" in str(path) or "Removable" in str(path):
                            icon = "ğŸ’€"
                            verdict += " (CLOUD/USB)"

                        rows.append({
                            "Time": time_str,
                            "Icon": icon,
                            "Verdict": f"**{verdict}** (Aggregated)",
                            "Details": f"Found **{count}** emails/artifacts<br>Location: {path}",
                            "Ref": "Details in: Plutos_Report_email_hunt.csv"
                        })
                else:
                    # Pathã‚«ãƒ©ãƒ ãŒãªã„å ´åˆã¯å¾“æ¥é€šã‚Šå€‹åˆ¥è¡¨ç¤º
                    for r in email_df.iter_rows(named=True):
                        ts = str(r.get("Timestamp", "")).split(".")[0]
                        artifact = r.get("Artifact", "")
                        path = str(r.get("Path", ""))
                        
                        icon = "ğŸ“¦"
                        verdict = str(r.get("Verdict", ""))
                        if "Dropbox" in path or "Removable" in path:
                            icon = "ğŸ’€"
                            verdict += " (CLOUD/USB)"
                        
                        rows.append({
                            "Time": ts,
                            "Icon": icon,
                            "Verdict": f"**{verdict}**",
                            "Details": f"Artifact: {artifact}<br>Path: {path}",
                            "Ref": "DATA_THEFT"
                        })
            except: pass

        # 4. Legacy Plutos_Main fallback
        main_df = dfs.get("Plutos_Main")
        if main_df is not None and main_df.height > 0 and not rows:
            try:
                for r in main_df.iter_rows(named=True):
                    ts = str(r.get("Timestamp", "")).split(".")[0]
                    verdict = r.get("Plutos_Verdict", "")
                    proc = str(r.get("Process", "")).split("\\")[-1] if r.get("Process") else ""
                    
                    icon = "âš ï¸"
                    if "EXFIL" in str(verdict): icon = "ğŸ“¤"
                    elif "LATERAL" in str(verdict): icon = "ğŸ¦€"
                    
                    rows.append({
                        "Time": ts,
                        "Icon": icon,
                        "Verdict": f"**{verdict}**",
                        "Details": f"Proc: {proc}",
                        "Ref": r.get("Tags", "")
                    })
            except: pass

        if not rows: return "ä¸å¯©ãªãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ´»å‹•ã‚„æ¨ªå±•é–‹ã®ç—•è·¡ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚\n"

        # æ™‚é–“é †ã«ã‚½ãƒ¼ãƒˆã—ã¦MarkdownåŒ–
        rows.sort(key=lambda x: x["Time"])
        
        md = "| Time / Period | Verdict | Summary | Reference |\n|---|---|---|---|\n"
        for row in rows:
            ref = row.get('Ref', row.get('Tags', ''))
            md += f"| {row['Time']} | {row['Icon']} {row['Verdict']} | {row['Details']} | {ref} |\n"
            
        return md

    def _write_ioc_appendix_unified(self, f, analyzer):
        t = self.txt
        f.write(f"## {t['h1_app']}\n(Full IOC List)\n")
        f.write("æœ¬èª¿æŸ»ã§ç¢ºèªã•ã‚ŒãŸã™ã¹ã¦ã®ä¾µå®³æŒ‡æ¨™ï¼ˆIOCï¼‰ã®ä¸€è¦§ã§ã™ã€‚\n\n")
        if analyzer.visual_iocs:
            f.write("### ğŸ“‚ File IOCs (Malicious/Suspicious Files)\n")
            f.write("| File Name | Path | Source | Note |\n|---|---|---|---|\n")
            seen = set()
            sorted_iocs = sorted(analyzer.visual_iocs, key=lambda x: 0 if "CRITICAL" in x.get("Reason", "").upper() else 1)
            for ioc in sorted_iocs:
                val = ioc['Value']
                path = ioc.get('Path', '-')
                if self._is_visual_noise(val): continue
                key = f"{val}|{path}"
                if key in seen: continue
                seen.add(key)
                reason = ioc.get("Reason", "Unknown")
                f.write(f"| `{val}` | `{path}` | {ioc['Type']} ({reason}) | {ioc.get('Time', 'N/A')} |\n")
            f.write("\n")
        if hasattr(analyzer, "infra_ips_found") and analyzer.infra_ips_found:
            f.write("### ğŸŒ Network IOCs (Suspicious Connections)\n")
            f.write("| Remote IP | Context |\n|---|---|\n")
            for ip in analyzer.infra_ips_found:
                 f.write(f"| `{ip}` | Detected in Event Logs |\n")
            f.write("\n")

    def export_pivot_config(self, pivot_seeds, path, primary_user):
        if not pivot_seeds: return
        config = {
            "Case_Context": {
                "Hostname": self.hostname,
                "Primary_User": primary_user,
                "Generated_At": datetime.now().isoformat()
            },
            "Deep_Dive_Targets": pivot_seeds[:20]
        }
        try:
            with open(path, "w", encoding="utf-8") as f:
                json.dump(config, f, indent=2, ensure_ascii=False)
            print(f"    -> [Lachesis] Pivot Config generated: {path}")
        except Exception as e:
            print(f"    [!] Failed to export Pivot Config: {e}")

    def export_json_grimoire(self, analysis_result, analyzer, json_path, primary_user):
        serializable_events = []
        for ev in analysis_result.get("events", []):
            serializable_events.append({
                "Time": str(ev.get('Time')),
                "User": ev.get('User'),
                "Category": ev.get('Category'),
                "Summary": ev.get('Summary'),
                "Source": ev.get('Source'),
                "Criticality": ev.get('Criticality', 0)
            })
        ips = list(analyzer.infra_ips_found) if hasattr(analyzer, "infra_ips_found") else []
        iocs = {"File": analyzer.visual_iocs, "Network": ips, "Cmd": []}
        grimoire_data = {
            "Metadata": {
                "Host": self.hostname, 
                "Case": "Investigation", 
                "Primary_User": primary_user, 
                "Generated_At": datetime.now().isoformat()
            },
            "Verdict": {
                "Flags": list(analysis_result.get("verdict_flags", [])), 
                "Lateral_Summary": analysis_result.get("lateral_summary", "")
            },
            "Timeline": serializable_events,
            "IOCs": iocs
        }
        try:
            with open(json_path, "w", encoding="utf-8") as f:
                json.dump(grimoire_data, f, indent=2, ensure_ascii=False)
            print(f"    -> [Chimera Ready] JSON Grimoire saved: {json_path}")
        except Exception as e:
            print(f"    [!] Failed to export JSON Grimoire: {e}")